{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coupled generative adversarial networks\n",
    "\n",
    "Ref.: LIU, Ming-Yu; TUZEL, Oncel. Coupled generative adversarial networks. In: Advances in neural information processing systems. 2016. p. 469-477."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generators(img_shape, latent_dim):\n",
    "    \"\"\" structure is hard-coded\n",
    "    \"\"\"\n",
    "    \n",
    "    # Shared weights between generators\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "    feature_repr = model(noise)\n",
    "\n",
    "    # Generator 1\n",
    "    g1 = Dense(1024)(feature_repr)\n",
    "    g1 = LeakyReLU(alpha=0.2)(g1)\n",
    "    g1 = BatchNormalization(momentum=0.8)(g1)\n",
    "    g1 = Dense(np.prod(img_shape), activation='tanh')(g1)\n",
    "    img1 = Reshape(img_shape)(g1)\n",
    "\n",
    "    # Generator 2\n",
    "    g2 = Dense(1024)(feature_repr)\n",
    "    g2 = LeakyReLU(alpha=0.2)(g2)\n",
    "    g2 = BatchNormalization(momentum=0.8)(g2)\n",
    "    g2 = Dense(np.prod(img_shape), activation='tanh')(g2)\n",
    "    img2 = Reshape(img_shape)(g2)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return Model(noise, img1), Model(noise, img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminators(img_shape):\n",
    "\n",
    "    img1 = Input(shape=img_shape)\n",
    "    img2 = Input(shape=img_shape)\n",
    "\n",
    "    # Shared discriminator layers\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=img_shape))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    img1_embedding = model(img1)\n",
    "    img2_embedding = model(img2)\n",
    "\n",
    "    # Discriminator 1\n",
    "    validity1 = Dense(1, activation='sigmoid')(img1_embedding)\n",
    "    # Discriminator 2\n",
    "    validity2 = Dense(1, activation='sigmoid')(img2_embedding)\n",
    "\n",
    "    return Model(img1, validity1), Model(img2, validity2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(epoch, g1, g2):\n",
    "    r, c = 4, 4\n",
    "    noise = np.random.normal(0, 1, (r * int(c/2), 100))\n",
    "    gen_imgs1 = g1.predict(noise)\n",
    "    gen_imgs2 = g2.predict(noise)\n",
    "\n",
    "    gen_imgs = np.concatenate([gen_imgs1, gen_imgs2])\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g1, g2, d1, d2, combined,\n",
    "          epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "    # Load the dataset\n",
    "    (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "    # Rescale -1 to 1\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "    # Images in domain A and B (rotated)\n",
    "    X1 = X_train[:int(X_train.shape[0]/2)]\n",
    "    X2 = X_train[int(X_train.shape[0]/2):]\n",
    "    X2 = scipy.ndimage.interpolation.rotate(X2, 90, axes=(1, 2))\n",
    "\n",
    "    # Adversarial ground truths\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # ----------------------\n",
    "        #  Train Discriminators\n",
    "        # ----------------------\n",
    "\n",
    "        # Select a random batch of images\n",
    "        idx = np.random.randint(0, X1.shape[0], batch_size)\n",
    "        imgs1 = X1[idx]\n",
    "        imgs2 = X2[idx]\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "        # Generate a batch of new images\n",
    "        gen_imgs1 = g1.predict(noise)\n",
    "        gen_imgs2 = g2.predict(noise)\n",
    "\n",
    "        # Train the discriminators\n",
    "        d1_loss_real = d1.train_on_batch(imgs1, valid)\n",
    "        d2_loss_real = d2.train_on_batch(imgs2, valid)\n",
    "        d1_loss_fake = d1.train_on_batch(gen_imgs1, fake)\n",
    "        d2_loss_fake = d2.train_on_batch(gen_imgs2, fake)\n",
    "        d1_loss = 0.5 * np.add(d1_loss_real, d1_loss_fake)\n",
    "        d2_loss = 0.5 * np.add(d2_loss_real, d2_loss_fake)\n",
    "\n",
    "\n",
    "        # ------------------\n",
    "        #  Train Generators\n",
    "        # ------------------\n",
    "\n",
    "        g_loss = combined.train_on_batch(noise, [valid, valid])\n",
    "\n",
    "        # Plot the progress\n",
    "        print (\"%d [D1 loss: %f, acc.: %.2f%%] [D2 loss: %f, acc.: %.2f%%] [G loss: %f]\" \\\n",
    "            % (epoch, d1_loss[0], 100*d1_loss[1], d2_loss[0], 100*d2_loss[1], g_loss[0]))\n",
    "\n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % sample_interval == 0:\n",
    "            sample_images(epoch, g1, g2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# create optimizer\n",
    "optimizer = Adam(0.0002, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Build and compile both discriminator\n",
    "d1, d2 = build_discriminators(img_shape)\n",
    "\n",
    "d1.compile(loss='binary_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "d2.compile(loss='binary_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "=================================================================\n",
      "Total params: 160,512\n",
      "Trainable params: 158,976\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build both generator\n",
    "g1, g2 = build_generators(img_shape, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generator takes noise as input and generated imgs\n",
    "z = Input(shape=(latent_dim,))\n",
    "img1 = g1(z)\n",
    "img2 = g2(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the combined model we will only train the generators\n",
    "d1.trainable = False\n",
    "d2.trainable = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The valid takes generated images as input and determines validity\n",
    "valid1 = d1(img1)\n",
    "valid2 = d2(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The combined model  (stacked generators and discriminators)\n",
    "# Trains generators to fool discriminators\n",
    "combined = Model(z, [valid1, valid2])\n",
    "combined.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\n",
    "                            optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D1 loss: 0.918094, acc.: 46.88%] [D2 loss: 0.604709, acc.: 62.50%] [G loss: 1.501780]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D1 loss: 0.397216, acc.: 96.88%] [D2 loss: 0.343117, acc.: 89.06%] [G loss: 1.536288]\n",
      "2 [D1 loss: 0.325359, acc.: 92.19%] [D2 loss: 0.328741, acc.: 82.81%] [G loss: 1.660351]\n",
      "3 [D1 loss: 0.303079, acc.: 92.19%] [D2 loss: 0.316144, acc.: 85.94%] [G loss: 1.851377]\n",
      "4 [D1 loss: 0.281995, acc.: 95.31%] [D2 loss: 0.295434, acc.: 93.75%] [G loss: 2.076351]\n",
      "5 [D1 loss: 0.265806, acc.: 98.44%] [D2 loss: 0.301856, acc.: 87.50%] [G loss: 2.408001]\n",
      "6 [D1 loss: 0.222210, acc.: 98.44%] [D2 loss: 0.226912, acc.: 98.44%] [G loss: 2.985777]\n",
      "7 [D1 loss: 0.170685, acc.: 98.44%] [D2 loss: 0.209495, acc.: 98.44%] [G loss: 3.375984]\n",
      "8 [D1 loss: 0.156022, acc.: 100.00%] [D2 loss: 0.151447, acc.: 100.00%] [G loss: 3.787433]\n",
      "9 [D1 loss: 0.114053, acc.: 100.00%] [D2 loss: 0.140640, acc.: 100.00%] [G loss: 4.260783]\n",
      "10 [D1 loss: 0.087218, acc.: 100.00%] [D2 loss: 0.098014, acc.: 100.00%] [G loss: 4.671067]\n",
      "11 [D1 loss: 0.081186, acc.: 100.00%] [D2 loss: 0.080641, acc.: 100.00%] [G loss: 4.970495]\n",
      "12 [D1 loss: 0.062215, acc.: 100.00%] [D2 loss: 0.070752, acc.: 100.00%] [G loss: 5.382321]\n",
      "13 [D1 loss: 0.067984, acc.: 100.00%] [D2 loss: 0.047519, acc.: 100.00%] [G loss: 5.657890]\n",
      "14 [D1 loss: 0.045988, acc.: 100.00%] [D2 loss: 0.045126, acc.: 100.00%] [G loss: 5.978209]\n",
      "15 [D1 loss: 0.044462, acc.: 100.00%] [D2 loss: 0.054308, acc.: 100.00%] [G loss: 6.014473]\n",
      "16 [D1 loss: 0.046461, acc.: 100.00%] [D2 loss: 0.036769, acc.: 100.00%] [G loss: 6.275349]\n",
      "17 [D1 loss: 0.031117, acc.: 100.00%] [D2 loss: 0.032698, acc.: 100.00%] [G loss: 6.631253]\n",
      "18 [D1 loss: 0.026952, acc.: 100.00%] [D2 loss: 0.025498, acc.: 100.00%] [G loss: 6.872656]\n",
      "19 [D1 loss: 0.025429, acc.: 100.00%] [D2 loss: 0.030777, acc.: 100.00%] [G loss: 6.991004]\n",
      "20 [D1 loss: 0.023750, acc.: 100.00%] [D2 loss: 0.026285, acc.: 100.00%] [G loss: 7.215564]\n",
      "21 [D1 loss: 0.031014, acc.: 100.00%] [D2 loss: 0.021767, acc.: 100.00%] [G loss: 7.187027]\n",
      "22 [D1 loss: 0.024157, acc.: 100.00%] [D2 loss: 0.024447, acc.: 100.00%] [G loss: 7.501064]\n",
      "23 [D1 loss: 0.022447, acc.: 100.00%] [D2 loss: 0.020613, acc.: 100.00%] [G loss: 7.513603]\n",
      "24 [D1 loss: 0.024307, acc.: 100.00%] [D2 loss: 0.019775, acc.: 100.00%] [G loss: 7.542162]\n",
      "25 [D1 loss: 0.019084, acc.: 100.00%] [D2 loss: 0.026610, acc.: 100.00%] [G loss: 7.826455]\n",
      "26 [D1 loss: 0.015316, acc.: 100.00%] [D2 loss: 0.017811, acc.: 100.00%] [G loss: 8.015889]\n",
      "27 [D1 loss: 0.019909, acc.: 100.00%] [D2 loss: 0.013340, acc.: 100.00%] [G loss: 8.154172]\n",
      "28 [D1 loss: 0.019560, acc.: 100.00%] [D2 loss: 0.016232, acc.: 100.00%] [G loss: 8.180109]\n",
      "29 [D1 loss: 0.014579, acc.: 100.00%] [D2 loss: 0.013527, acc.: 100.00%] [G loss: 8.263393]\n",
      "30 [D1 loss: 0.018976, acc.: 100.00%] [D2 loss: 0.016790, acc.: 100.00%] [G loss: 8.192527]\n",
      "31 [D1 loss: 0.012333, acc.: 100.00%] [D2 loss: 0.014089, acc.: 100.00%] [G loss: 8.461609]\n",
      "32 [D1 loss: 0.012286, acc.: 100.00%] [D2 loss: 0.012480, acc.: 100.00%] [G loss: 8.406826]\n",
      "33 [D1 loss: 0.017773, acc.: 100.00%] [D2 loss: 0.012546, acc.: 100.00%] [G loss: 8.326774]\n",
      "34 [D1 loss: 0.013238, acc.: 100.00%] [D2 loss: 0.012249, acc.: 100.00%] [G loss: 8.650517]\n",
      "35 [D1 loss: 0.015592, acc.: 100.00%] [D2 loss: 0.013909, acc.: 100.00%] [G loss: 8.602131]\n",
      "36 [D1 loss: 0.011077, acc.: 100.00%] [D2 loss: 0.009692, acc.: 100.00%] [G loss: 8.847874]\n",
      "37 [D1 loss: 0.016208, acc.: 100.00%] [D2 loss: 0.013479, acc.: 100.00%] [G loss: 8.981708]\n",
      "38 [D1 loss: 0.013632, acc.: 100.00%] [D2 loss: 0.009915, acc.: 100.00%] [G loss: 8.734911]\n",
      "39 [D1 loss: 0.013825, acc.: 100.00%] [D2 loss: 0.010078, acc.: 100.00%] [G loss: 9.101345]\n",
      "40 [D1 loss: 0.009739, acc.: 100.00%] [D2 loss: 0.008633, acc.: 100.00%] [G loss: 8.974422]\n",
      "41 [D1 loss: 0.010585, acc.: 100.00%] [D2 loss: 0.012131, acc.: 100.00%] [G loss: 8.795090]\n",
      "42 [D1 loss: 0.017812, acc.: 100.00%] [D2 loss: 0.012191, acc.: 100.00%] [G loss: 9.231505]\n",
      "43 [D1 loss: 0.009873, acc.: 100.00%] [D2 loss: 0.006597, acc.: 100.00%] [G loss: 9.085876]\n",
      "44 [D1 loss: 0.011326, acc.: 100.00%] [D2 loss: 0.015688, acc.: 100.00%] [G loss: 9.310567]\n",
      "45 [D1 loss: 0.011556, acc.: 100.00%] [D2 loss: 0.010556, acc.: 100.00%] [G loss: 9.427855]\n",
      "46 [D1 loss: 0.010332, acc.: 100.00%] [D2 loss: 0.007794, acc.: 100.00%] [G loss: 9.279123]\n",
      "47 [D1 loss: 0.008889, acc.: 100.00%] [D2 loss: 0.008713, acc.: 100.00%] [G loss: 9.236053]\n",
      "48 [D1 loss: 0.010824, acc.: 100.00%] [D2 loss: 0.010208, acc.: 100.00%] [G loss: 9.525726]\n",
      "49 [D1 loss: 0.009734, acc.: 100.00%] [D2 loss: 0.010381, acc.: 100.00%] [G loss: 9.518319]\n",
      "50 [D1 loss: 0.012925, acc.: 100.00%] [D2 loss: 0.017539, acc.: 100.00%] [G loss: 9.565113]\n",
      "51 [D1 loss: 0.008232, acc.: 100.00%] [D2 loss: 0.009528, acc.: 100.00%] [G loss: 9.814000]\n",
      "52 [D1 loss: 0.010237, acc.: 100.00%] [D2 loss: 0.007771, acc.: 100.00%] [G loss: 9.779810]\n",
      "53 [D1 loss: 0.010675, acc.: 100.00%] [D2 loss: 0.008478, acc.: 100.00%] [G loss: 9.845515]\n",
      "54 [D1 loss: 0.011542, acc.: 100.00%] [D2 loss: 0.009675, acc.: 100.00%] [G loss: 9.978231]\n",
      "55 [D1 loss: 0.008592, acc.: 100.00%] [D2 loss: 0.005612, acc.: 100.00%] [G loss: 10.100908]\n",
      "56 [D1 loss: 0.009362, acc.: 100.00%] [D2 loss: 0.008053, acc.: 100.00%] [G loss: 9.868807]\n",
      "57 [D1 loss: 0.006066, acc.: 100.00%] [D2 loss: 0.007448, acc.: 100.00%] [G loss: 9.715421]\n",
      "58 [D1 loss: 0.014242, acc.: 100.00%] [D2 loss: 0.012946, acc.: 100.00%] [G loss: 9.894043]\n",
      "59 [D1 loss: 0.009367, acc.: 100.00%] [D2 loss: 0.010225, acc.: 100.00%] [G loss: 10.242558]\n",
      "60 [D1 loss: 0.009153, acc.: 100.00%] [D2 loss: 0.008551, acc.: 100.00%] [G loss: 10.360216]\n",
      "61 [D1 loss: 0.008396, acc.: 100.00%] [D2 loss: 0.005226, acc.: 100.00%] [G loss: 10.296978]\n",
      "62 [D1 loss: 0.009594, acc.: 100.00%] [D2 loss: 0.011648, acc.: 100.00%] [G loss: 10.290740]\n",
      "63 [D1 loss: 0.012231, acc.: 100.00%] [D2 loss: 0.006065, acc.: 100.00%] [G loss: 10.317942]\n",
      "64 [D1 loss: 0.010115, acc.: 100.00%] [D2 loss: 0.005681, acc.: 100.00%] [G loss: 10.446983]\n",
      "65 [D1 loss: 0.013243, acc.: 100.00%] [D2 loss: 0.010487, acc.: 100.00%] [G loss: 10.428156]\n",
      "66 [D1 loss: 0.008579, acc.: 100.00%] [D2 loss: 0.008966, acc.: 100.00%] [G loss: 10.524879]\n",
      "67 [D1 loss: 0.013706, acc.: 100.00%] [D2 loss: 0.010077, acc.: 100.00%] [G loss: 10.725704]\n",
      "68 [D1 loss: 0.008962, acc.: 100.00%] [D2 loss: 0.012405, acc.: 100.00%] [G loss: 10.855502]\n",
      "69 [D1 loss: 0.012587, acc.: 100.00%] [D2 loss: 0.010877, acc.: 100.00%] [G loss: 10.861853]\n",
      "70 [D1 loss: 0.008806, acc.: 100.00%] [D2 loss: 0.009306, acc.: 100.00%] [G loss: 10.753357]\n",
      "71 [D1 loss: 0.005894, acc.: 100.00%] [D2 loss: 0.006618, acc.: 100.00%] [G loss: 10.546234]\n",
      "72 [D1 loss: 0.007588, acc.: 100.00%] [D2 loss: 0.003895, acc.: 100.00%] [G loss: 10.668112]\n",
      "73 [D1 loss: 0.007746, acc.: 100.00%] [D2 loss: 0.011690, acc.: 100.00%] [G loss: 10.683832]\n",
      "74 [D1 loss: 0.021489, acc.: 100.00%] [D2 loss: 0.006072, acc.: 100.00%] [G loss: 11.132383]\n",
      "75 [D1 loss: 0.013315, acc.: 100.00%] [D2 loss: 0.018392, acc.: 100.00%] [G loss: 11.133550]\n",
      "76 [D1 loss: 0.010534, acc.: 100.00%] [D2 loss: 0.010487, acc.: 100.00%] [G loss: 11.119258]\n",
      "77 [D1 loss: 0.007446, acc.: 100.00%] [D2 loss: 0.011822, acc.: 100.00%] [G loss: 11.006229]\n",
      "78 [D1 loss: 0.014346, acc.: 100.00%] [D2 loss: 0.006104, acc.: 100.00%] [G loss: 11.226717]\n",
      "79 [D1 loss: 0.011685, acc.: 100.00%] [D2 loss: 0.010187, acc.: 100.00%] [G loss: 11.107662]\n",
      "80 [D1 loss: 0.018433, acc.: 100.00%] [D2 loss: 0.018244, acc.: 100.00%] [G loss: 11.977915]\n",
      "81 [D1 loss: 0.064751, acc.: 100.00%] [D2 loss: 0.015766, acc.: 100.00%] [G loss: 10.764589]\n",
      "82 [D1 loss: 0.034295, acc.: 98.44%] [D2 loss: 0.013147, acc.: 100.00%] [G loss: 11.840839]\n",
      "83 [D1 loss: 0.068891, acc.: 96.88%] [D2 loss: 0.043981, acc.: 100.00%] [G loss: 10.180635]\n",
      "84 [D1 loss: 0.007594, acc.: 100.00%] [D2 loss: 0.041341, acc.: 100.00%] [G loss: 11.783529]\n",
      "85 [D1 loss: 0.006821, acc.: 100.00%] [D2 loss: 0.011440, acc.: 100.00%] [G loss: 12.448145]\n",
      "86 [D1 loss: 0.039941, acc.: 100.00%] [D2 loss: 0.022529, acc.: 100.00%] [G loss: 12.202701]\n",
      "87 [D1 loss: 0.019214, acc.: 100.00%] [D2 loss: 0.017222, acc.: 100.00%] [G loss: 12.202387]\n",
      "88 [D1 loss: 0.026807, acc.: 100.00%] [D2 loss: 0.013699, acc.: 100.00%] [G loss: 12.264225]\n",
      "89 [D1 loss: 0.049399, acc.: 100.00%] [D2 loss: 0.020487, acc.: 100.00%] [G loss: 13.519096]\n",
      "90 [D1 loss: 0.270576, acc.: 87.50%] [D2 loss: 0.059855, acc.: 96.88%] [G loss: 11.014143]\n",
      "91 [D1 loss: 0.027811, acc.: 98.44%] [D2 loss: 0.041062, acc.: 96.88%] [G loss: 13.995524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 [D1 loss: 0.114201, acc.: 95.31%] [D2 loss: 0.035227, acc.: 98.44%] [G loss: 11.568077]\n",
      "93 [D1 loss: 0.170189, acc.: 90.62%] [D2 loss: 0.030626, acc.: 100.00%] [G loss: 13.491845]\n",
      "94 [D1 loss: 0.792813, acc.: 75.00%] [D2 loss: 1.587731, acc.: 32.81%] [G loss: 7.175697]\n",
      "95 [D1 loss: 0.266301, acc.: 87.50%] [D2 loss: 0.334540, acc.: 85.94%] [G loss: 7.555627]\n",
      "96 [D1 loss: 0.155245, acc.: 89.06%] [D2 loss: 0.284816, acc.: 90.62%] [G loss: 10.394873]\n",
      "97 [D1 loss: 0.035605, acc.: 98.44%] [D2 loss: 0.079459, acc.: 95.31%] [G loss: 10.828533]\n",
      "98 [D1 loss: 0.103069, acc.: 96.88%] [D2 loss: 0.113216, acc.: 95.31%] [G loss: 11.014616]\n",
      "99 [D1 loss: 0.017494, acc.: 100.00%] [D2 loss: 0.117127, acc.: 96.88%] [G loss: 10.881680]\n",
      "100 [D1 loss: 0.045461, acc.: 100.00%] [D2 loss: 0.061805, acc.: 96.88%] [G loss: 10.304309]\n",
      "101 [D1 loss: 0.044672, acc.: 100.00%] [D2 loss: 0.063246, acc.: 96.88%] [G loss: 10.617430]\n",
      "102 [D1 loss: 0.016717, acc.: 100.00%] [D2 loss: 0.036694, acc.: 100.00%] [G loss: 10.344546]\n",
      "103 [D1 loss: 0.058198, acc.: 98.44%] [D2 loss: 0.034943, acc.: 100.00%] [G loss: 9.635706]\n",
      "104 [D1 loss: 0.061051, acc.: 98.44%] [D2 loss: 0.036887, acc.: 98.44%] [G loss: 10.069813]\n",
      "105 [D1 loss: 0.065115, acc.: 100.00%] [D2 loss: 0.072677, acc.: 96.88%] [G loss: 9.678312]\n",
      "106 [D1 loss: 0.072713, acc.: 98.44%] [D2 loss: 0.075706, acc.: 96.88%] [G loss: 9.813836]\n",
      "107 [D1 loss: 0.083478, acc.: 100.00%] [D2 loss: 0.109192, acc.: 95.31%] [G loss: 11.182666]\n",
      "108 [D1 loss: 0.058067, acc.: 98.44%] [D2 loss: 0.073873, acc.: 98.44%] [G loss: 10.210760]\n",
      "109 [D1 loss: 0.111582, acc.: 95.31%] [D2 loss: 0.029543, acc.: 100.00%] [G loss: 9.931827]\n",
      "110 [D1 loss: 0.069546, acc.: 98.44%] [D2 loss: 0.044966, acc.: 100.00%] [G loss: 9.014910]\n",
      "111 [D1 loss: 0.077947, acc.: 96.88%] [D2 loss: 0.067480, acc.: 100.00%] [G loss: 10.173061]\n",
      "112 [D1 loss: 0.238143, acc.: 92.19%] [D2 loss: 0.075485, acc.: 96.88%] [G loss: 9.733020]\n",
      "113 [D1 loss: 0.134775, acc.: 93.75%] [D2 loss: 0.036896, acc.: 100.00%] [G loss: 12.215622]\n",
      "114 [D1 loss: 0.804337, acc.: 64.06%] [D2 loss: 0.724700, acc.: 70.31%] [G loss: 7.087830]\n",
      "115 [D1 loss: 0.250886, acc.: 85.94%] [D2 loss: 0.047957, acc.: 98.44%] [G loss: 9.500420]\n",
      "116 [D1 loss: 0.059237, acc.: 96.88%] [D2 loss: 0.122921, acc.: 98.44%] [G loss: 8.859661]\n",
      "117 [D1 loss: 0.066868, acc.: 95.31%] [D2 loss: 0.057954, acc.: 100.00%] [G loss: 9.776473]\n",
      "118 [D1 loss: 0.034002, acc.: 100.00%] [D2 loss: 0.052732, acc.: 98.44%] [G loss: 10.242784]\n",
      "119 [D1 loss: 0.069586, acc.: 98.44%] [D2 loss: 0.043130, acc.: 100.00%] [G loss: 10.133106]\n",
      "120 [D1 loss: 0.109353, acc.: 93.75%] [D2 loss: 0.035268, acc.: 100.00%] [G loss: 10.000221]\n",
      "121 [D1 loss: 0.059971, acc.: 100.00%] [D2 loss: 0.079425, acc.: 98.44%] [G loss: 9.047077]\n",
      "122 [D1 loss: 0.090404, acc.: 96.88%] [D2 loss: 0.036553, acc.: 100.00%] [G loss: 8.907869]\n",
      "123 [D1 loss: 0.182535, acc.: 93.75%] [D2 loss: 0.039251, acc.: 98.44%] [G loss: 10.547577]\n",
      "124 [D1 loss: 0.480874, acc.: 78.12%] [D2 loss: 0.490581, acc.: 76.56%] [G loss: 8.395981]\n",
      "125 [D1 loss: 0.035371, acc.: 100.00%] [D2 loss: 0.039623, acc.: 100.00%] [G loss: 11.643990]\n",
      "126 [D1 loss: 0.244069, acc.: 89.06%] [D2 loss: 0.111502, acc.: 96.88%] [G loss: 9.670986]\n",
      "127 [D1 loss: 0.168072, acc.: 90.62%] [D2 loss: 0.055038, acc.: 100.00%] [G loss: 11.775828]\n",
      "128 [D1 loss: 0.270059, acc.: 85.94%] [D2 loss: 0.078138, acc.: 100.00%] [G loss: 9.726864]\n",
      "129 [D1 loss: 0.070296, acc.: 98.44%] [D2 loss: 0.093279, acc.: 98.44%] [G loss: 9.468575]\n",
      "130 [D1 loss: 0.358616, acc.: 84.38%] [D2 loss: 0.059421, acc.: 100.00%] [G loss: 9.779611]\n",
      "131 [D1 loss: 0.250698, acc.: 90.62%] [D2 loss: 0.040384, acc.: 100.00%] [G loss: 10.655951]\n",
      "132 [D1 loss: 0.162917, acc.: 93.75%] [D2 loss: 0.061422, acc.: 100.00%] [G loss: 9.631606]\n",
      "133 [D1 loss: 0.234014, acc.: 92.19%] [D2 loss: 0.114948, acc.: 95.31%] [G loss: 10.858472]\n",
      "134 [D1 loss: 0.225960, acc.: 92.19%] [D2 loss: 0.082988, acc.: 96.88%] [G loss: 10.116634]\n",
      "135 [D1 loss: 0.347035, acc.: 78.12%] [D2 loss: 0.084920, acc.: 98.44%] [G loss: 10.539492]\n",
      "136 [D1 loss: 0.568470, acc.: 71.88%] [D2 loss: 0.165004, acc.: 92.19%] [G loss: 11.812198]\n",
      "137 [D1 loss: 0.314229, acc.: 85.94%] [D2 loss: 0.057393, acc.: 100.00%] [G loss: 10.081356]\n",
      "138 [D1 loss: 0.340741, acc.: 85.94%] [D2 loss: 0.112102, acc.: 98.44%] [G loss: 10.431849]\n",
      "139 [D1 loss: 0.217341, acc.: 90.62%] [D2 loss: 0.112171, acc.: 96.88%] [G loss: 9.804931]\n",
      "140 [D1 loss: 0.175964, acc.: 92.19%] [D2 loss: 0.041098, acc.: 100.00%] [G loss: 8.990553]\n",
      "141 [D1 loss: 0.292433, acc.: 89.06%] [D2 loss: 0.054316, acc.: 100.00%] [G loss: 12.167318]\n",
      "142 [D1 loss: 1.584732, acc.: 39.06%] [D2 loss: 1.140020, acc.: 62.50%] [G loss: 4.681394]\n",
      "143 [D1 loss: 0.304928, acc.: 81.25%] [D2 loss: 0.051329, acc.: 100.00%] [G loss: 9.807174]\n",
      "144 [D1 loss: 0.385693, acc.: 78.12%] [D2 loss: 0.173798, acc.: 95.31%] [G loss: 8.080328]\n",
      "145 [D1 loss: 0.231496, acc.: 90.62%] [D2 loss: 0.154095, acc.: 98.44%] [G loss: 7.809097]\n",
      "146 [D1 loss: 0.162235, acc.: 95.31%] [D2 loss: 0.053867, acc.: 100.00%] [G loss: 9.034706]\n",
      "147 [D1 loss: 0.215714, acc.: 93.75%] [D2 loss: 0.092194, acc.: 96.88%] [G loss: 8.000622]\n",
      "148 [D1 loss: 0.247985, acc.: 85.94%] [D2 loss: 0.041944, acc.: 100.00%] [G loss: 8.922901]\n",
      "149 [D1 loss: 0.768091, acc.: 57.81%] [D2 loss: 0.343200, acc.: 82.81%] [G loss: 8.218760]\n",
      "150 [D1 loss: 0.270251, acc.: 84.38%] [D2 loss: 0.099318, acc.: 98.44%] [G loss: 10.475244]\n",
      "151 [D1 loss: 0.502182, acc.: 71.88%] [D2 loss: 0.154688, acc.: 93.75%] [G loss: 9.609267]\n",
      "152 [D1 loss: 0.501307, acc.: 73.44%] [D2 loss: 0.078457, acc.: 100.00%] [G loss: 9.921906]\n",
      "153 [D1 loss: 0.555406, acc.: 70.31%] [D2 loss: 0.087792, acc.: 100.00%] [G loss: 9.563876]\n",
      "154 [D1 loss: 0.742254, acc.: 57.81%] [D2 loss: 0.301671, acc.: 82.81%] [G loss: 10.356218]\n",
      "155 [D1 loss: 0.344841, acc.: 85.94%] [D2 loss: 0.150657, acc.: 95.31%] [G loss: 9.560525]\n",
      "156 [D1 loss: 0.587957, acc.: 68.75%] [D2 loss: 0.141003, acc.: 95.31%] [G loss: 8.193518]\n",
      "157 [D1 loss: 0.469397, acc.: 76.56%] [D2 loss: 0.309209, acc.: 85.94%] [G loss: 7.409065]\n",
      "158 [D1 loss: 0.446957, acc.: 75.00%] [D2 loss: 0.194655, acc.: 93.75%] [G loss: 7.646454]\n",
      "159 [D1 loss: 0.373539, acc.: 84.38%] [D2 loss: 0.178543, acc.: 87.50%] [G loss: 9.831541]\n",
      "160 [D1 loss: 0.952786, acc.: 42.19%] [D2 loss: 0.557327, acc.: 71.88%] [G loss: 7.447556]\n",
      "161 [D1 loss: 0.304656, acc.: 78.12%] [D2 loss: 0.123764, acc.: 98.44%] [G loss: 8.112518]\n",
      "162 [D1 loss: 0.489078, acc.: 68.75%] [D2 loss: 0.299766, acc.: 87.50%] [G loss: 7.344395]\n",
      "163 [D1 loss: 0.381448, acc.: 79.69%] [D2 loss: 0.163595, acc.: 98.44%] [G loss: 7.291742]\n",
      "164 [D1 loss: 0.528859, acc.: 70.31%] [D2 loss: 0.232558, acc.: 85.94%] [G loss: 10.634508]\n",
      "165 [D1 loss: 1.406799, acc.: 29.69%] [D2 loss: 0.768313, acc.: 70.31%] [G loss: 3.126462]\n",
      "166 [D1 loss: 0.494807, acc.: 65.62%] [D2 loss: 0.175768, acc.: 90.62%] [G loss: 8.268384]\n",
      "167 [D1 loss: 0.656003, acc.: 59.38%] [D2 loss: 0.454334, acc.: 81.25%] [G loss: 5.133724]\n",
      "168 [D1 loss: 0.247672, acc.: 87.50%] [D2 loss: 0.110366, acc.: 98.44%] [G loss: 8.459520]\n",
      "169 [D1 loss: 0.428940, acc.: 76.56%] [D2 loss: 0.123772, acc.: 100.00%] [G loss: 7.335787]\n",
      "170 [D1 loss: 0.464797, acc.: 76.56%] [D2 loss: 0.190512, acc.: 95.31%] [G loss: 9.956206]\n",
      "171 [D1 loss: 0.610029, acc.: 70.31%] [D2 loss: 0.234921, acc.: 87.50%] [G loss: 8.987836]\n",
      "172 [D1 loss: 0.666961, acc.: 57.81%] [D2 loss: 0.164567, acc.: 96.88%] [G loss: 8.841230]\n",
      "173 [D1 loss: 0.876991, acc.: 51.56%] [D2 loss: 0.694990, acc.: 64.06%] [G loss: 7.637416]\n",
      "174 [D1 loss: 0.462752, acc.: 75.00%] [D2 loss: 0.285542, acc.: 85.94%] [G loss: 6.535233]\n",
      "175 [D1 loss: 0.680342, acc.: 56.25%] [D2 loss: 0.367997, acc.: 85.94%] [G loss: 6.265008]\n",
      "176 [D1 loss: 0.469923, acc.: 73.44%] [D2 loss: 0.318734, acc.: 84.38%] [G loss: 9.426575]\n",
      "177 [D1 loss: 1.200680, acc.: 18.75%] [D2 loss: 0.582452, acc.: 68.75%] [G loss: 5.941410]\n",
      "178 [D1 loss: 0.470524, acc.: 71.88%] [D2 loss: 0.209240, acc.: 96.88%] [G loss: 7.156192]\n",
      "179 [D1 loss: 0.802341, acc.: 48.44%] [D2 loss: 1.021655, acc.: 48.44%] [G loss: 2.405525]\n",
      "180 [D1 loss: 0.504273, acc.: 57.81%] [D2 loss: 0.585794, acc.: 68.75%] [G loss: 5.572429]\n",
      "181 [D1 loss: 0.499567, acc.: 76.56%] [D2 loss: 0.245986, acc.: 93.75%] [G loss: 6.703455]\n",
      "182 [D1 loss: 0.622829, acc.: 62.50%] [D2 loss: 0.424915, acc.: 75.00%] [G loss: 4.514891]\n",
      "183 [D1 loss: 0.454940, acc.: 75.00%] [D2 loss: 0.252524, acc.: 90.62%] [G loss: 7.941661]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184 [D1 loss: 0.843212, acc.: 45.31%] [D2 loss: 0.382824, acc.: 79.69%] [G loss: 7.665075]\n",
      "185 [D1 loss: 0.735584, acc.: 54.69%] [D2 loss: 0.321174, acc.: 85.94%] [G loss: 6.476449]\n",
      "186 [D1 loss: 0.528376, acc.: 68.75%] [D2 loss: 0.383958, acc.: 82.81%] [G loss: 5.355880]\n",
      "187 [D1 loss: 0.503927, acc.: 73.44%] [D2 loss: 0.537280, acc.: 73.44%] [G loss: 5.317835]\n",
      "188 [D1 loss: 0.448755, acc.: 81.25%] [D2 loss: 0.325535, acc.: 81.25%] [G loss: 6.483703]\n",
      "189 [D1 loss: 0.585494, acc.: 57.81%] [D2 loss: 0.498598, acc.: 73.44%] [G loss: 5.516922]\n",
      "190 [D1 loss: 0.572999, acc.: 60.94%] [D2 loss: 0.433203, acc.: 71.88%] [G loss: 7.529742]\n",
      "191 [D1 loss: 0.709991, acc.: 59.38%] [D2 loss: 0.320691, acc.: 81.25%] [G loss: 6.344965]\n",
      "192 [D1 loss: 0.669646, acc.: 50.00%] [D2 loss: 0.554466, acc.: 70.31%] [G loss: 4.964752]\n",
      "193 [D1 loss: 0.539823, acc.: 62.50%] [D2 loss: 0.342125, acc.: 85.94%] [G loss: 5.697367]\n",
      "194 [D1 loss: 0.684245, acc.: 48.44%] [D2 loss: 0.391266, acc.: 79.69%] [G loss: 6.502151]\n",
      "195 [D1 loss: 0.790347, acc.: 45.31%] [D2 loss: 0.506600, acc.: 73.44%] [G loss: 5.299318]\n",
      "196 [D1 loss: 0.644211, acc.: 53.12%] [D2 loss: 0.644161, acc.: 64.06%] [G loss: 3.396362]\n",
      "197 [D1 loss: 0.483106, acc.: 78.12%] [D2 loss: 0.242490, acc.: 93.75%] [G loss: 6.072543]\n",
      "198 [D1 loss: 0.756357, acc.: 45.31%] [D2 loss: 0.774417, acc.: 54.69%] [G loss: 3.410745]\n",
      "199 [D1 loss: 0.510392, acc.: 70.31%] [D2 loss: 0.338864, acc.: 79.69%] [G loss: 6.128485]\n",
      "200 [D1 loss: 0.735342, acc.: 37.50%] [D2 loss: 0.858407, acc.: 51.56%] [G loss: 2.875436]\n",
      "201 [D1 loss: 0.550000, acc.: 62.50%] [D2 loss: 0.339912, acc.: 79.69%] [G loss: 5.701636]\n",
      "202 [D1 loss: 0.732970, acc.: 43.75%] [D2 loss: 0.532869, acc.: 75.00%] [G loss: 4.314038]\n",
      "203 [D1 loss: 0.606327, acc.: 54.69%] [D2 loss: 0.558388, acc.: 65.62%] [G loss: 3.352872]\n",
      "204 [D1 loss: 0.621081, acc.: 51.56%] [D2 loss: 0.645506, acc.: 60.94%] [G loss: 4.428974]\n",
      "205 [D1 loss: 0.623146, acc.: 50.00%] [D2 loss: 0.659259, acc.: 59.38%] [G loss: 3.062091]\n",
      "206 [D1 loss: 0.621556, acc.: 64.06%] [D2 loss: 0.478303, acc.: 73.44%] [G loss: 3.617344]\n",
      "207 [D1 loss: 0.636723, acc.: 53.12%] [D2 loss: 0.547077, acc.: 71.88%] [G loss: 4.113982]\n",
      "208 [D1 loss: 0.607225, acc.: 56.25%] [D2 loss: 0.691799, acc.: 56.25%] [G loss: 3.418347]\n",
      "209 [D1 loss: 0.603285, acc.: 57.81%] [D2 loss: 0.419205, acc.: 71.88%] [G loss: 4.367795]\n",
      "210 [D1 loss: 0.694955, acc.: 43.75%] [D2 loss: 0.722606, acc.: 53.12%] [G loss: 2.946177]\n",
      "211 [D1 loss: 0.571114, acc.: 59.38%] [D2 loss: 0.518472, acc.: 67.19%] [G loss: 4.151344]\n",
      "212 [D1 loss: 0.756174, acc.: 34.38%] [D2 loss: 0.752129, acc.: 46.88%] [G loss: 2.572879]\n",
      "213 [D1 loss: 0.589531, acc.: 53.12%] [D2 loss: 0.488859, acc.: 68.75%] [G loss: 3.446084]\n",
      "214 [D1 loss: 0.620611, acc.: 48.44%] [D2 loss: 0.761148, acc.: 56.25%] [G loss: 2.528993]\n",
      "215 [D1 loss: 0.623815, acc.: 53.12%] [D2 loss: 0.502084, acc.: 70.31%] [G loss: 3.637863]\n",
      "216 [D1 loss: 0.610531, acc.: 57.81%] [D2 loss: 0.596182, acc.: 65.62%] [G loss: 2.534476]\n",
      "217 [D1 loss: 0.616920, acc.: 56.25%] [D2 loss: 0.369180, acc.: 84.38%] [G loss: 3.905010]\n",
      "218 [D1 loss: 0.700207, acc.: 39.06%] [D2 loss: 0.728905, acc.: 45.31%] [G loss: 2.550955]\n",
      "219 [D1 loss: 0.584594, acc.: 60.94%] [D2 loss: 0.476975, acc.: 73.44%] [G loss: 3.805166]\n",
      "220 [D1 loss: 0.638229, acc.: 54.69%] [D2 loss: 0.873770, acc.: 42.19%] [G loss: 2.031032]\n",
      "221 [D1 loss: 0.639298, acc.: 50.00%] [D2 loss: 0.528168, acc.: 71.88%] [G loss: 2.857511]\n",
      "222 [D1 loss: 0.596288, acc.: 56.25%] [D2 loss: 0.572762, acc.: 73.44%] [G loss: 3.139487]\n",
      "223 [D1 loss: 0.617512, acc.: 57.81%] [D2 loss: 0.464828, acc.: 76.56%] [G loss: 3.703909]\n",
      "224 [D1 loss: 0.708866, acc.: 40.62%] [D2 loss: 0.760292, acc.: 45.31%] [G loss: 2.197854]\n",
      "225 [D1 loss: 0.607791, acc.: 56.25%] [D2 loss: 0.525652, acc.: 65.62%] [G loss: 2.702427]\n",
      "226 [D1 loss: 0.644364, acc.: 50.00%] [D2 loss: 0.700066, acc.: 56.25%] [G loss: 2.239322]\n",
      "227 [D1 loss: 0.655549, acc.: 46.88%] [D2 loss: 0.626680, acc.: 60.94%] [G loss: 2.390410]\n",
      "228 [D1 loss: 0.610335, acc.: 53.12%] [D2 loss: 0.548404, acc.: 65.62%] [G loss: 2.570600]\n",
      "229 [D1 loss: 0.645658, acc.: 48.44%] [D2 loss: 0.752727, acc.: 48.44%] [G loss: 1.947399]\n",
      "230 [D1 loss: 0.627516, acc.: 54.69%] [D2 loss: 0.554180, acc.: 57.81%] [G loss: 2.750284]\n",
      "231 [D1 loss: 0.632752, acc.: 50.00%] [D2 loss: 0.967700, acc.: 23.44%] [G loss: 1.505498]\n",
      "232 [D1 loss: 0.620778, acc.: 56.25%] [D2 loss: 0.526465, acc.: 60.94%] [G loss: 2.179170]\n",
      "233 [D1 loss: 0.626901, acc.: 56.25%] [D2 loss: 0.539926, acc.: 73.44%] [G loss: 2.277934]\n",
      "234 [D1 loss: 0.627834, acc.: 56.25%] [D2 loss: 0.646993, acc.: 60.94%] [G loss: 2.163348]\n",
      "235 [D1 loss: 0.616571, acc.: 56.25%] [D2 loss: 0.666971, acc.: 51.56%] [G loss: 1.946287]\n",
      "236 [D1 loss: 0.631127, acc.: 59.38%] [D2 loss: 0.502977, acc.: 78.12%] [G loss: 3.015172]\n",
      "237 [D1 loss: 0.684665, acc.: 53.12%] [D2 loss: 0.827859, acc.: 35.94%] [G loss: 1.623762]\n",
      "238 [D1 loss: 0.640896, acc.: 53.12%] [D2 loss: 0.622353, acc.: 54.69%] [G loss: 1.883336]\n",
      "239 [D1 loss: 0.635202, acc.: 51.56%] [D2 loss: 0.647257, acc.: 54.69%] [G loss: 1.875538]\n",
      "240 [D1 loss: 0.647100, acc.: 46.88%] [D2 loss: 0.620116, acc.: 56.25%] [G loss: 1.817063]\n",
      "241 [D1 loss: 0.681761, acc.: 50.00%] [D2 loss: 0.645650, acc.: 57.81%] [G loss: 2.056952]\n",
      "242 [D1 loss: 0.650110, acc.: 51.56%] [D2 loss: 0.767369, acc.: 43.75%] [G loss: 1.576470]\n",
      "243 [D1 loss: 0.619662, acc.: 56.25%] [D2 loss: 0.595393, acc.: 56.25%] [G loss: 1.841501]\n",
      "244 [D1 loss: 0.640475, acc.: 54.69%] [D2 loss: 0.706607, acc.: 51.56%] [G loss: 1.792938]\n",
      "245 [D1 loss: 0.652573, acc.: 57.81%] [D2 loss: 0.619343, acc.: 64.06%] [G loss: 2.092191]\n",
      "246 [D1 loss: 0.663375, acc.: 46.88%] [D2 loss: 0.694595, acc.: 53.12%] [G loss: 1.671449]\n",
      "247 [D1 loss: 0.652746, acc.: 51.56%] [D2 loss: 0.648084, acc.: 51.56%] [G loss: 1.795644]\n",
      "248 [D1 loss: 0.607882, acc.: 62.50%] [D2 loss: 0.710929, acc.: 39.06%] [G loss: 1.532419]\n",
      "249 [D1 loss: 0.647349, acc.: 59.38%] [D2 loss: 0.657921, acc.: 48.44%] [G loss: 1.697319]\n",
      "250 [D1 loss: 0.660702, acc.: 56.25%] [D2 loss: 0.688922, acc.: 48.44%] [G loss: 1.658700]\n",
      "251 [D1 loss: 0.603521, acc.: 56.25%] [D2 loss: 0.663535, acc.: 53.12%] [G loss: 1.717029]\n"
     ]
    }
   ],
   "source": [
    "epochs=3000\n",
    "# epochs=30000\n",
    "train(g1, g2, d1, d2, combined,\n",
    "      epochs=epochs, batch_size=32, sample_interval=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
