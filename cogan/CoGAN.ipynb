{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coupled generative adversarial networks\n",
    "\n",
    "Ref.: LIU, Ming-Yu; TUZEL, Oncel. Coupled generative adversarial networks. In: Advances in neural information processing systems. 2016. p. 469-477."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generators(img_shape, latent_dim):\n",
    "    \"\"\" structure is hard-coded\n",
    "    \"\"\"\n",
    "    \n",
    "    # Shared weights between generators\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "    feature_repr = model(noise)\n",
    "\n",
    "    # Generator 1\n",
    "    g1 = Dense(1024)(feature_repr)\n",
    "    g1 = LeakyReLU(alpha=0.2)(g1)\n",
    "    g1 = BatchNormalization(momentum=0.8)(g1)\n",
    "    g1 = Dense(np.prod(img_shape), activation='tanh')(g1)\n",
    "    img1 = Reshape(img_shape)(g1)\n",
    "\n",
    "    # Generator 2\n",
    "    g2 = Dense(1024)(feature_repr)\n",
    "    g2 = LeakyReLU(alpha=0.2)(g2)\n",
    "    g2 = BatchNormalization(momentum=0.8)(g2)\n",
    "    g2 = Dense(np.prod(img_shape), activation='tanh')(g2)\n",
    "    img2 = Reshape(img_shape)(g2)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return Model(noise, img1), Model(noise, img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminators(img_shape):\n",
    "\n",
    "    img1 = Input(shape=img_shape)\n",
    "    img2 = Input(shape=img_shape)\n",
    "\n",
    "    # Shared discriminator layers\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=img_shape))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    img1_embedding = model(img1)\n",
    "    img2_embedding = model(img2)\n",
    "\n",
    "    # Discriminator 1\n",
    "    validity1 = Dense(1, activation='sigmoid')(img1_embedding)\n",
    "    # Discriminator 2\n",
    "    validity2 = Dense(1, activation='sigmoid')(img2_embedding)\n",
    "\n",
    "    return Model(img1, validity1), Model(img2, validity2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(epoch, g1, g2):\n",
    "    r, c = 4, 4\n",
    "    noise = np.random.normal(0, 1, (r * int(c/2), 100))\n",
    "    gen_imgs1 = g1.predict(noise)\n",
    "    gen_imgs2 = g2.predict(noise)\n",
    "\n",
    "    gen_imgs = np.concatenate([gen_imgs1, gen_imgs2])\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g1, g2, d1, d2, combined,\n",
    "          epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "    # Load the dataset\n",
    "    (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "    # Rescale -1 to 1\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "    # Images in domain A and B (rotated)\n",
    "    X1 = X_train[:int(X_train.shape[0]/2)]\n",
    "    X2 = X_train[int(X_train.shape[0]/2):]\n",
    "    X2 = scipy.ndimage.interpolation.rotate(X2, 90, axes=(1, 2))\n",
    "\n",
    "    # Adversarial ground truths\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # ----------------------\n",
    "        #  Train Discriminators\n",
    "        # ----------------------\n",
    "\n",
    "        # Select a random batch of images\n",
    "        idx = np.random.randint(0, X1.shape[0], batch_size)\n",
    "        imgs1 = X1[idx]\n",
    "        imgs2 = X2[idx]\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "        # Generate a batch of new images\n",
    "        gen_imgs1 = g1.predict(noise)\n",
    "        gen_imgs2 = g2.predict(noise)\n",
    "\n",
    "        # Train the discriminators\n",
    "        d1_loss_real = d1.train_on_batch(imgs1, valid)\n",
    "        d2_loss_real = d2.train_on_batch(imgs2, valid)\n",
    "        d1_loss_fake = d1.train_on_batch(gen_imgs1, fake)\n",
    "        d2_loss_fake = d2.train_on_batch(gen_imgs2, fake)\n",
    "        d1_loss = 0.5 * np.add(d1_loss_real, d1_loss_fake)\n",
    "        d2_loss = 0.5 * np.add(d2_loss_real, d2_loss_fake)\n",
    "\n",
    "\n",
    "        # ------------------\n",
    "        #  Train Generators\n",
    "        # ------------------\n",
    "\n",
    "        g_loss = combined.train_on_batch(noise, [valid, valid])\n",
    "\n",
    "        # Plot the progress\n",
    "        print (\"%d [D1 loss: %f, acc.: %.2f%%] [D2 loss: %f, acc.: %.2f%%] [G loss: %f]\" \\\n",
    "            % (epoch, d1_loss[0], 100*d1_loss[1], d2_loss[0], 100*d2_loss[1], g_loss[0]))\n",
    "\n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % sample_interval == 0:\n",
    "            sample_images(epoch, g1, g2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# create optimizer\n",
    "optimizer = Adam(0.0002, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Build and compile both discriminator\n",
    "d1, d2 = build_discriminators(img_shape)\n",
    "\n",
    "d1.compile(loss='binary_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "d2.compile(loss='binary_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "=================================================================\n",
      "Total params: 160,512\n",
      "Trainable params: 158,976\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build both generator\n",
    "g1, g2 = build_generators(img_shape, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generator takes noise as input and generated imgs\n",
    "z = Input(shape=(latent_dim,))\n",
    "img1 = g1(z)\n",
    "img2 = g2(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the combined model we will only train the generators\n",
    "d1.trainable = False\n",
    "d2.trainable = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The valid takes generated images as input and determines validity\n",
    "valid1 = d1(img1)\n",
    "valid2 = d2(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The combined model  (stacked generators and discriminators)\n",
    "# Trains generators to fool discriminators\n",
    "combined = Model(z, [valid1, valid2])\n",
    "combined.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\n",
    "                            optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D1 loss: 0.918094, acc.: 46.88%] [D2 loss: 0.604709, acc.: 62.50%] [G loss: 1.501780]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D1 loss: 0.397216, acc.: 96.88%] [D2 loss: 0.343117, acc.: 89.06%] [G loss: 1.536288]\n",
      "2 [D1 loss: 0.325359, acc.: 92.19%] [D2 loss: 0.328741, acc.: 82.81%] [G loss: 1.660351]\n",
      "3 [D1 loss: 0.303079, acc.: 92.19%] [D2 loss: 0.316144, acc.: 85.94%] [G loss: 1.851377]\n",
      "4 [D1 loss: 0.281995, acc.: 95.31%] [D2 loss: 0.295434, acc.: 93.75%] [G loss: 2.076351]\n",
      "5 [D1 loss: 0.265806, acc.: 98.44%] [D2 loss: 0.301856, acc.: 87.50%] [G loss: 2.408001]\n",
      "6 [D1 loss: 0.222210, acc.: 98.44%] [D2 loss: 0.226912, acc.: 98.44%] [G loss: 2.985777]\n",
      "7 [D1 loss: 0.170685, acc.: 98.44%] [D2 loss: 0.209495, acc.: 98.44%] [G loss: 3.375984]\n",
      "8 [D1 loss: 0.156022, acc.: 100.00%] [D2 loss: 0.151447, acc.: 100.00%] [G loss: 3.787433]\n",
      "9 [D1 loss: 0.114053, acc.: 100.00%] [D2 loss: 0.140640, acc.: 100.00%] [G loss: 4.260783]\n",
      "10 [D1 loss: 0.087218, acc.: 100.00%] [D2 loss: 0.098014, acc.: 100.00%] [G loss: 4.671067]\n",
      "11 [D1 loss: 0.081186, acc.: 100.00%] [D2 loss: 0.080641, acc.: 100.00%] [G loss: 4.970495]\n",
      "12 [D1 loss: 0.062215, acc.: 100.00%] [D2 loss: 0.070752, acc.: 100.00%] [G loss: 5.382321]\n",
      "13 [D1 loss: 0.067984, acc.: 100.00%] [D2 loss: 0.047519, acc.: 100.00%] [G loss: 5.657890]\n",
      "14 [D1 loss: 0.045988, acc.: 100.00%] [D2 loss: 0.045126, acc.: 100.00%] [G loss: 5.978209]\n",
      "15 [D1 loss: 0.044462, acc.: 100.00%] [D2 loss: 0.054308, acc.: 100.00%] [G loss: 6.014473]\n",
      "16 [D1 loss: 0.046461, acc.: 100.00%] [D2 loss: 0.036769, acc.: 100.00%] [G loss: 6.275349]\n",
      "17 [D1 loss: 0.031117, acc.: 100.00%] [D2 loss: 0.032698, acc.: 100.00%] [G loss: 6.631253]\n",
      "18 [D1 loss: 0.026952, acc.: 100.00%] [D2 loss: 0.025498, acc.: 100.00%] [G loss: 6.872656]\n",
      "19 [D1 loss: 0.025429, acc.: 100.00%] [D2 loss: 0.030777, acc.: 100.00%] [G loss: 6.991004]\n",
      "20 [D1 loss: 0.023750, acc.: 100.00%] [D2 loss: 0.026285, acc.: 100.00%] [G loss: 7.215564]\n",
      "21 [D1 loss: 0.031014, acc.: 100.00%] [D2 loss: 0.021767, acc.: 100.00%] [G loss: 7.187027]\n",
      "22 [D1 loss: 0.024157, acc.: 100.00%] [D2 loss: 0.024447, acc.: 100.00%] [G loss: 7.501064]\n",
      "23 [D1 loss: 0.022447, acc.: 100.00%] [D2 loss: 0.020613, acc.: 100.00%] [G loss: 7.513603]\n",
      "24 [D1 loss: 0.024307, acc.: 100.00%] [D2 loss: 0.019775, acc.: 100.00%] [G loss: 7.542162]\n",
      "25 [D1 loss: 0.019084, acc.: 100.00%] [D2 loss: 0.026610, acc.: 100.00%] [G loss: 7.826455]\n",
      "26 [D1 loss: 0.015316, acc.: 100.00%] [D2 loss: 0.017811, acc.: 100.00%] [G loss: 8.015889]\n",
      "27 [D1 loss: 0.019909, acc.: 100.00%] [D2 loss: 0.013340, acc.: 100.00%] [G loss: 8.154172]\n",
      "28 [D1 loss: 0.019560, acc.: 100.00%] [D2 loss: 0.016232, acc.: 100.00%] [G loss: 8.180109]\n",
      "29 [D1 loss: 0.014579, acc.: 100.00%] [D2 loss: 0.013527, acc.: 100.00%] [G loss: 8.263393]\n",
      "30 [D1 loss: 0.018976, acc.: 100.00%] [D2 loss: 0.016790, acc.: 100.00%] [G loss: 8.192527]\n",
      "31 [D1 loss: 0.012333, acc.: 100.00%] [D2 loss: 0.014089, acc.: 100.00%] [G loss: 8.461609]\n",
      "32 [D1 loss: 0.012286, acc.: 100.00%] [D2 loss: 0.012480, acc.: 100.00%] [G loss: 8.406826]\n",
      "33 [D1 loss: 0.017773, acc.: 100.00%] [D2 loss: 0.012546, acc.: 100.00%] [G loss: 8.326774]\n",
      "34 [D1 loss: 0.013238, acc.: 100.00%] [D2 loss: 0.012249, acc.: 100.00%] [G loss: 8.650517]\n",
      "35 [D1 loss: 0.015592, acc.: 100.00%] [D2 loss: 0.013909, acc.: 100.00%] [G loss: 8.602131]\n",
      "36 [D1 loss: 0.011077, acc.: 100.00%] [D2 loss: 0.009692, acc.: 100.00%] [G loss: 8.847874]\n",
      "37 [D1 loss: 0.016208, acc.: 100.00%] [D2 loss: 0.013479, acc.: 100.00%] [G loss: 8.981708]\n",
      "38 [D1 loss: 0.013632, acc.: 100.00%] [D2 loss: 0.009915, acc.: 100.00%] [G loss: 8.734911]\n",
      "39 [D1 loss: 0.013825, acc.: 100.00%] [D2 loss: 0.010078, acc.: 100.00%] [G loss: 9.101345]\n",
      "40 [D1 loss: 0.009739, acc.: 100.00%] [D2 loss: 0.008633, acc.: 100.00%] [G loss: 8.974422]\n",
      "41 [D1 loss: 0.010585, acc.: 100.00%] [D2 loss: 0.012131, acc.: 100.00%] [G loss: 8.795090]\n",
      "42 [D1 loss: 0.017812, acc.: 100.00%] [D2 loss: 0.012191, acc.: 100.00%] [G loss: 9.231505]\n",
      "43 [D1 loss: 0.009873, acc.: 100.00%] [D2 loss: 0.006597, acc.: 100.00%] [G loss: 9.085876]\n",
      "44 [D1 loss: 0.011326, acc.: 100.00%] [D2 loss: 0.015688, acc.: 100.00%] [G loss: 9.310567]\n",
      "45 [D1 loss: 0.011556, acc.: 100.00%] [D2 loss: 0.010556, acc.: 100.00%] [G loss: 9.427855]\n",
      "46 [D1 loss: 0.010332, acc.: 100.00%] [D2 loss: 0.007794, acc.: 100.00%] [G loss: 9.279123]\n",
      "47 [D1 loss: 0.008889, acc.: 100.00%] [D2 loss: 0.008713, acc.: 100.00%] [G loss: 9.236053]\n",
      "48 [D1 loss: 0.010824, acc.: 100.00%] [D2 loss: 0.010208, acc.: 100.00%] [G loss: 9.525726]\n",
      "49 [D1 loss: 0.009734, acc.: 100.00%] [D2 loss: 0.010381, acc.: 100.00%] [G loss: 9.518319]\n",
      "50 [D1 loss: 0.012925, acc.: 100.00%] [D2 loss: 0.017539, acc.: 100.00%] [G loss: 9.565113]\n",
      "51 [D1 loss: 0.008232, acc.: 100.00%] [D2 loss: 0.009528, acc.: 100.00%] [G loss: 9.814000]\n",
      "52 [D1 loss: 0.010237, acc.: 100.00%] [D2 loss: 0.007771, acc.: 100.00%] [G loss: 9.779810]\n",
      "53 [D1 loss: 0.010675, acc.: 100.00%] [D2 loss: 0.008478, acc.: 100.00%] [G loss: 9.845515]\n",
      "54 [D1 loss: 0.011542, acc.: 100.00%] [D2 loss: 0.009675, acc.: 100.00%] [G loss: 9.978231]\n",
      "55 [D1 loss: 0.008592, acc.: 100.00%] [D2 loss: 0.005612, acc.: 100.00%] [G loss: 10.100908]\n",
      "56 [D1 loss: 0.009362, acc.: 100.00%] [D2 loss: 0.008053, acc.: 100.00%] [G loss: 9.868807]\n",
      "57 [D1 loss: 0.006066, acc.: 100.00%] [D2 loss: 0.007448, acc.: 100.00%] [G loss: 9.715421]\n",
      "58 [D1 loss: 0.014242, acc.: 100.00%] [D2 loss: 0.012946, acc.: 100.00%] [G loss: 9.894043]\n",
      "59 [D1 loss: 0.009367, acc.: 100.00%] [D2 loss: 0.010225, acc.: 100.00%] [G loss: 10.242558]\n",
      "60 [D1 loss: 0.009153, acc.: 100.00%] [D2 loss: 0.008551, acc.: 100.00%] [G loss: 10.360216]\n",
      "61 [D1 loss: 0.008396, acc.: 100.00%] [D2 loss: 0.005226, acc.: 100.00%] [G loss: 10.296978]\n",
      "62 [D1 loss: 0.009594, acc.: 100.00%] [D2 loss: 0.011648, acc.: 100.00%] [G loss: 10.290740]\n",
      "63 [D1 loss: 0.012231, acc.: 100.00%] [D2 loss: 0.006065, acc.: 100.00%] [G loss: 10.317942]\n",
      "64 [D1 loss: 0.010115, acc.: 100.00%] [D2 loss: 0.005681, acc.: 100.00%] [G loss: 10.446983]\n",
      "65 [D1 loss: 0.013243, acc.: 100.00%] [D2 loss: 0.010487, acc.: 100.00%] [G loss: 10.428156]\n",
      "66 [D1 loss: 0.008579, acc.: 100.00%] [D2 loss: 0.008966, acc.: 100.00%] [G loss: 10.524879]\n",
      "67 [D1 loss: 0.013706, acc.: 100.00%] [D2 loss: 0.010077, acc.: 100.00%] [G loss: 10.725704]\n",
      "68 [D1 loss: 0.008962, acc.: 100.00%] [D2 loss: 0.012405, acc.: 100.00%] [G loss: 10.855502]\n",
      "69 [D1 loss: 0.012587, acc.: 100.00%] [D2 loss: 0.010877, acc.: 100.00%] [G loss: 10.861853]\n",
      "70 [D1 loss: 0.008806, acc.: 100.00%] [D2 loss: 0.009306, acc.: 100.00%] [G loss: 10.753357]\n",
      "71 [D1 loss: 0.005894, acc.: 100.00%] [D2 loss: 0.006618, acc.: 100.00%] [G loss: 10.546234]\n",
      "72 [D1 loss: 0.007588, acc.: 100.00%] [D2 loss: 0.003895, acc.: 100.00%] [G loss: 10.668112]\n",
      "73 [D1 loss: 0.007746, acc.: 100.00%] [D2 loss: 0.011690, acc.: 100.00%] [G loss: 10.683832]\n",
      "74 [D1 loss: 0.021489, acc.: 100.00%] [D2 loss: 0.006072, acc.: 100.00%] [G loss: 11.132383]\n",
      "75 [D1 loss: 0.013315, acc.: 100.00%] [D2 loss: 0.018392, acc.: 100.00%] [G loss: 11.133550]\n",
      "76 [D1 loss: 0.010534, acc.: 100.00%] [D2 loss: 0.010487, acc.: 100.00%] [G loss: 11.119258]\n",
      "77 [D1 loss: 0.007446, acc.: 100.00%] [D2 loss: 0.011822, acc.: 100.00%] [G loss: 11.006229]\n",
      "78 [D1 loss: 0.014346, acc.: 100.00%] [D2 loss: 0.006104, acc.: 100.00%] [G loss: 11.226717]\n",
      "79 [D1 loss: 0.011685, acc.: 100.00%] [D2 loss: 0.010187, acc.: 100.00%] [G loss: 11.107662]\n",
      "80 [D1 loss: 0.018433, acc.: 100.00%] [D2 loss: 0.018244, acc.: 100.00%] [G loss: 11.977915]\n",
      "81 [D1 loss: 0.064751, acc.: 100.00%] [D2 loss: 0.015766, acc.: 100.00%] [G loss: 10.764589]\n",
      "82 [D1 loss: 0.034295, acc.: 98.44%] [D2 loss: 0.013147, acc.: 100.00%] [G loss: 11.840839]\n",
      "83 [D1 loss: 0.068891, acc.: 96.88%] [D2 loss: 0.043981, acc.: 100.00%] [G loss: 10.180635]\n",
      "84 [D1 loss: 0.007594, acc.: 100.00%] [D2 loss: 0.041341, acc.: 100.00%] [G loss: 11.783529]\n",
      "85 [D1 loss: 0.006821, acc.: 100.00%] [D2 loss: 0.011440, acc.: 100.00%] [G loss: 12.448145]\n",
      "86 [D1 loss: 0.039941, acc.: 100.00%] [D2 loss: 0.022529, acc.: 100.00%] [G loss: 12.202701]\n",
      "87 [D1 loss: 0.019214, acc.: 100.00%] [D2 loss: 0.017222, acc.: 100.00%] [G loss: 12.202387]\n",
      "88 [D1 loss: 0.026807, acc.: 100.00%] [D2 loss: 0.013699, acc.: 100.00%] [G loss: 12.264225]\n",
      "89 [D1 loss: 0.049399, acc.: 100.00%] [D2 loss: 0.020487, acc.: 100.00%] [G loss: 13.519096]\n",
      "90 [D1 loss: 0.270576, acc.: 87.50%] [D2 loss: 0.059855, acc.: 96.88%] [G loss: 11.014143]\n",
      "91 [D1 loss: 0.027811, acc.: 98.44%] [D2 loss: 0.041062, acc.: 96.88%] [G loss: 13.995524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 [D1 loss: 0.114201, acc.: 95.31%] [D2 loss: 0.035227, acc.: 98.44%] [G loss: 11.568077]\n",
      "93 [D1 loss: 0.170189, acc.: 90.62%] [D2 loss: 0.030626, acc.: 100.00%] [G loss: 13.491845]\n",
      "94 [D1 loss: 0.792813, acc.: 75.00%] [D2 loss: 1.587731, acc.: 32.81%] [G loss: 7.175697]\n",
      "95 [D1 loss: 0.266301, acc.: 87.50%] [D2 loss: 0.334540, acc.: 85.94%] [G loss: 7.555627]\n",
      "96 [D1 loss: 0.155245, acc.: 89.06%] [D2 loss: 0.284816, acc.: 90.62%] [G loss: 10.394873]\n",
      "97 [D1 loss: 0.035605, acc.: 98.44%] [D2 loss: 0.079459, acc.: 95.31%] [G loss: 10.828533]\n",
      "98 [D1 loss: 0.103069, acc.: 96.88%] [D2 loss: 0.113216, acc.: 95.31%] [G loss: 11.014616]\n",
      "99 [D1 loss: 0.017494, acc.: 100.00%] [D2 loss: 0.117127, acc.: 96.88%] [G loss: 10.881680]\n",
      "100 [D1 loss: 0.045461, acc.: 100.00%] [D2 loss: 0.061805, acc.: 96.88%] [G loss: 10.304309]\n",
      "101 [D1 loss: 0.044672, acc.: 100.00%] [D2 loss: 0.063246, acc.: 96.88%] [G loss: 10.617430]\n",
      "102 [D1 loss: 0.016717, acc.: 100.00%] [D2 loss: 0.036694, acc.: 100.00%] [G loss: 10.344546]\n",
      "103 [D1 loss: 0.058198, acc.: 98.44%] [D2 loss: 0.034943, acc.: 100.00%] [G loss: 9.635706]\n",
      "104 [D1 loss: 0.061051, acc.: 98.44%] [D2 loss: 0.036887, acc.: 98.44%] [G loss: 10.069813]\n",
      "105 [D1 loss: 0.065115, acc.: 100.00%] [D2 loss: 0.072677, acc.: 96.88%] [G loss: 9.678312]\n",
      "106 [D1 loss: 0.072713, acc.: 98.44%] [D2 loss: 0.075706, acc.: 96.88%] [G loss: 9.813836]\n",
      "107 [D1 loss: 0.083478, acc.: 100.00%] [D2 loss: 0.109192, acc.: 95.31%] [G loss: 11.182666]\n",
      "108 [D1 loss: 0.058067, acc.: 98.44%] [D2 loss: 0.073873, acc.: 98.44%] [G loss: 10.210760]\n",
      "109 [D1 loss: 0.111582, acc.: 95.31%] [D2 loss: 0.029543, acc.: 100.00%] [G loss: 9.931827]\n",
      "110 [D1 loss: 0.069546, acc.: 98.44%] [D2 loss: 0.044966, acc.: 100.00%] [G loss: 9.014910]\n",
      "111 [D1 loss: 0.077947, acc.: 96.88%] [D2 loss: 0.067480, acc.: 100.00%] [G loss: 10.173061]\n",
      "112 [D1 loss: 0.238143, acc.: 92.19%] [D2 loss: 0.075485, acc.: 96.88%] [G loss: 9.733020]\n",
      "113 [D1 loss: 0.134775, acc.: 93.75%] [D2 loss: 0.036896, acc.: 100.00%] [G loss: 12.215622]\n",
      "114 [D1 loss: 0.804337, acc.: 64.06%] [D2 loss: 0.724700, acc.: 70.31%] [G loss: 7.087830]\n",
      "115 [D1 loss: 0.250886, acc.: 85.94%] [D2 loss: 0.047957, acc.: 98.44%] [G loss: 9.500420]\n",
      "116 [D1 loss: 0.059237, acc.: 96.88%] [D2 loss: 0.122921, acc.: 98.44%] [G loss: 8.859661]\n",
      "117 [D1 loss: 0.066868, acc.: 95.31%] [D2 loss: 0.057954, acc.: 100.00%] [G loss: 9.776473]\n",
      "118 [D1 loss: 0.034002, acc.: 100.00%] [D2 loss: 0.052732, acc.: 98.44%] [G loss: 10.242784]\n",
      "119 [D1 loss: 0.069586, acc.: 98.44%] [D2 loss: 0.043130, acc.: 100.00%] [G loss: 10.133106]\n",
      "120 [D1 loss: 0.109353, acc.: 93.75%] [D2 loss: 0.035268, acc.: 100.00%] [G loss: 10.000221]\n",
      "121 [D1 loss: 0.059971, acc.: 100.00%] [D2 loss: 0.079425, acc.: 98.44%] [G loss: 9.047077]\n",
      "122 [D1 loss: 0.090404, acc.: 96.88%] [D2 loss: 0.036553, acc.: 100.00%] [G loss: 8.907869]\n",
      "123 [D1 loss: 0.182535, acc.: 93.75%] [D2 loss: 0.039251, acc.: 98.44%] [G loss: 10.547577]\n",
      "124 [D1 loss: 0.480874, acc.: 78.12%] [D2 loss: 0.490581, acc.: 76.56%] [G loss: 8.395981]\n",
      "125 [D1 loss: 0.035371, acc.: 100.00%] [D2 loss: 0.039623, acc.: 100.00%] [G loss: 11.643990]\n",
      "126 [D1 loss: 0.244069, acc.: 89.06%] [D2 loss: 0.111502, acc.: 96.88%] [G loss: 9.670986]\n",
      "127 [D1 loss: 0.168072, acc.: 90.62%] [D2 loss: 0.055038, acc.: 100.00%] [G loss: 11.775828]\n",
      "128 [D1 loss: 0.270059, acc.: 85.94%] [D2 loss: 0.078138, acc.: 100.00%] [G loss: 9.726864]\n",
      "129 [D1 loss: 0.070296, acc.: 98.44%] [D2 loss: 0.093279, acc.: 98.44%] [G loss: 9.468575]\n",
      "130 [D1 loss: 0.358616, acc.: 84.38%] [D2 loss: 0.059421, acc.: 100.00%] [G loss: 9.779611]\n",
      "131 [D1 loss: 0.250698, acc.: 90.62%] [D2 loss: 0.040384, acc.: 100.00%] [G loss: 10.655951]\n",
      "132 [D1 loss: 0.162917, acc.: 93.75%] [D2 loss: 0.061422, acc.: 100.00%] [G loss: 9.631606]\n",
      "133 [D1 loss: 0.234014, acc.: 92.19%] [D2 loss: 0.114948, acc.: 95.31%] [G loss: 10.858472]\n",
      "134 [D1 loss: 0.225960, acc.: 92.19%] [D2 loss: 0.082988, acc.: 96.88%] [G loss: 10.116634]\n",
      "135 [D1 loss: 0.347035, acc.: 78.12%] [D2 loss: 0.084920, acc.: 98.44%] [G loss: 10.539492]\n",
      "136 [D1 loss: 0.568470, acc.: 71.88%] [D2 loss: 0.165004, acc.: 92.19%] [G loss: 11.812198]\n",
      "137 [D1 loss: 0.314229, acc.: 85.94%] [D2 loss: 0.057393, acc.: 100.00%] [G loss: 10.081356]\n",
      "138 [D1 loss: 0.340741, acc.: 85.94%] [D2 loss: 0.112102, acc.: 98.44%] [G loss: 10.431849]\n",
      "139 [D1 loss: 0.217341, acc.: 90.62%] [D2 loss: 0.112171, acc.: 96.88%] [G loss: 9.804931]\n",
      "140 [D1 loss: 0.175964, acc.: 92.19%] [D2 loss: 0.041098, acc.: 100.00%] [G loss: 8.990553]\n",
      "141 [D1 loss: 0.292433, acc.: 89.06%] [D2 loss: 0.054316, acc.: 100.00%] [G loss: 12.167318]\n",
      "142 [D1 loss: 1.584732, acc.: 39.06%] [D2 loss: 1.140020, acc.: 62.50%] [G loss: 4.681394]\n",
      "143 [D1 loss: 0.304928, acc.: 81.25%] [D2 loss: 0.051329, acc.: 100.00%] [G loss: 9.807174]\n",
      "144 [D1 loss: 0.385693, acc.: 78.12%] [D2 loss: 0.173798, acc.: 95.31%] [G loss: 8.080328]\n",
      "145 [D1 loss: 0.231496, acc.: 90.62%] [D2 loss: 0.154095, acc.: 98.44%] [G loss: 7.809097]\n",
      "146 [D1 loss: 0.162235, acc.: 95.31%] [D2 loss: 0.053867, acc.: 100.00%] [G loss: 9.034706]\n",
      "147 [D1 loss: 0.215714, acc.: 93.75%] [D2 loss: 0.092194, acc.: 96.88%] [G loss: 8.000622]\n",
      "148 [D1 loss: 0.247985, acc.: 85.94%] [D2 loss: 0.041944, acc.: 100.00%] [G loss: 8.922901]\n",
      "149 [D1 loss: 0.768091, acc.: 57.81%] [D2 loss: 0.343200, acc.: 82.81%] [G loss: 8.218760]\n",
      "150 [D1 loss: 0.270251, acc.: 84.38%] [D2 loss: 0.099318, acc.: 98.44%] [G loss: 10.475244]\n",
      "151 [D1 loss: 0.502182, acc.: 71.88%] [D2 loss: 0.154688, acc.: 93.75%] [G loss: 9.609267]\n",
      "152 [D1 loss: 0.501307, acc.: 73.44%] [D2 loss: 0.078457, acc.: 100.00%] [G loss: 9.921906]\n",
      "153 [D1 loss: 0.555406, acc.: 70.31%] [D2 loss: 0.087792, acc.: 100.00%] [G loss: 9.563876]\n",
      "154 [D1 loss: 0.742254, acc.: 57.81%] [D2 loss: 0.301671, acc.: 82.81%] [G loss: 10.356218]\n",
      "155 [D1 loss: 0.344841, acc.: 85.94%] [D2 loss: 0.150657, acc.: 95.31%] [G loss: 9.560525]\n",
      "156 [D1 loss: 0.587957, acc.: 68.75%] [D2 loss: 0.141003, acc.: 95.31%] [G loss: 8.193518]\n",
      "157 [D1 loss: 0.469397, acc.: 76.56%] [D2 loss: 0.309209, acc.: 85.94%] [G loss: 7.409065]\n",
      "158 [D1 loss: 0.446957, acc.: 75.00%] [D2 loss: 0.194655, acc.: 93.75%] [G loss: 7.646454]\n",
      "159 [D1 loss: 0.373539, acc.: 84.38%] [D2 loss: 0.178543, acc.: 87.50%] [G loss: 9.831541]\n",
      "160 [D1 loss: 0.952786, acc.: 42.19%] [D2 loss: 0.557327, acc.: 71.88%] [G loss: 7.447556]\n",
      "161 [D1 loss: 0.304656, acc.: 78.12%] [D2 loss: 0.123764, acc.: 98.44%] [G loss: 8.112518]\n",
      "162 [D1 loss: 0.489078, acc.: 68.75%] [D2 loss: 0.299766, acc.: 87.50%] [G loss: 7.344395]\n",
      "163 [D1 loss: 0.381448, acc.: 79.69%] [D2 loss: 0.163595, acc.: 98.44%] [G loss: 7.291742]\n",
      "164 [D1 loss: 0.528859, acc.: 70.31%] [D2 loss: 0.232558, acc.: 85.94%] [G loss: 10.634508]\n",
      "165 [D1 loss: 1.406799, acc.: 29.69%] [D2 loss: 0.768313, acc.: 70.31%] [G loss: 3.126462]\n",
      "166 [D1 loss: 0.494807, acc.: 65.62%] [D2 loss: 0.175768, acc.: 90.62%] [G loss: 8.268384]\n",
      "167 [D1 loss: 0.656003, acc.: 59.38%] [D2 loss: 0.454334, acc.: 81.25%] [G loss: 5.133724]\n",
      "168 [D1 loss: 0.247672, acc.: 87.50%] [D2 loss: 0.110366, acc.: 98.44%] [G loss: 8.459520]\n",
      "169 [D1 loss: 0.428940, acc.: 76.56%] [D2 loss: 0.123772, acc.: 100.00%] [G loss: 7.335787]\n",
      "170 [D1 loss: 0.464797, acc.: 76.56%] [D2 loss: 0.190512, acc.: 95.31%] [G loss: 9.956206]\n",
      "171 [D1 loss: 0.610029, acc.: 70.31%] [D2 loss: 0.234921, acc.: 87.50%] [G loss: 8.987836]\n",
      "172 [D1 loss: 0.666961, acc.: 57.81%] [D2 loss: 0.164567, acc.: 96.88%] [G loss: 8.841230]\n",
      "173 [D1 loss: 0.876991, acc.: 51.56%] [D2 loss: 0.694990, acc.: 64.06%] [G loss: 7.637416]\n",
      "174 [D1 loss: 0.462752, acc.: 75.00%] [D2 loss: 0.285542, acc.: 85.94%] [G loss: 6.535233]\n",
      "175 [D1 loss: 0.680342, acc.: 56.25%] [D2 loss: 0.367997, acc.: 85.94%] [G loss: 6.265008]\n",
      "176 [D1 loss: 0.469923, acc.: 73.44%] [D2 loss: 0.318734, acc.: 84.38%] [G loss: 9.426575]\n",
      "177 [D1 loss: 1.200680, acc.: 18.75%] [D2 loss: 0.582452, acc.: 68.75%] [G loss: 5.941410]\n",
      "178 [D1 loss: 0.470524, acc.: 71.88%] [D2 loss: 0.209240, acc.: 96.88%] [G loss: 7.156192]\n",
      "179 [D1 loss: 0.802341, acc.: 48.44%] [D2 loss: 1.021655, acc.: 48.44%] [G loss: 2.405525]\n",
      "180 [D1 loss: 0.504273, acc.: 57.81%] [D2 loss: 0.585794, acc.: 68.75%] [G loss: 5.572429]\n",
      "181 [D1 loss: 0.499567, acc.: 76.56%] [D2 loss: 0.245986, acc.: 93.75%] [G loss: 6.703455]\n",
      "182 [D1 loss: 0.622829, acc.: 62.50%] [D2 loss: 0.424915, acc.: 75.00%] [G loss: 4.514891]\n",
      "183 [D1 loss: 0.454940, acc.: 75.00%] [D2 loss: 0.252524, acc.: 90.62%] [G loss: 7.941661]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184 [D1 loss: 0.843212, acc.: 45.31%] [D2 loss: 0.382824, acc.: 79.69%] [G loss: 7.665075]\n",
      "185 [D1 loss: 0.735584, acc.: 54.69%] [D2 loss: 0.321174, acc.: 85.94%] [G loss: 6.476449]\n",
      "186 [D1 loss: 0.528376, acc.: 68.75%] [D2 loss: 0.383958, acc.: 82.81%] [G loss: 5.355880]\n",
      "187 [D1 loss: 0.503927, acc.: 73.44%] [D2 loss: 0.537280, acc.: 73.44%] [G loss: 5.317835]\n",
      "188 [D1 loss: 0.448755, acc.: 81.25%] [D2 loss: 0.325535, acc.: 81.25%] [G loss: 6.483703]\n",
      "189 [D1 loss: 0.585494, acc.: 57.81%] [D2 loss: 0.498598, acc.: 73.44%] [G loss: 5.516922]\n",
      "190 [D1 loss: 0.572999, acc.: 60.94%] [D2 loss: 0.433203, acc.: 71.88%] [G loss: 7.529742]\n",
      "191 [D1 loss: 0.709991, acc.: 59.38%] [D2 loss: 0.320691, acc.: 81.25%] [G loss: 6.344965]\n",
      "192 [D1 loss: 0.669646, acc.: 50.00%] [D2 loss: 0.554466, acc.: 70.31%] [G loss: 4.964752]\n",
      "193 [D1 loss: 0.539823, acc.: 62.50%] [D2 loss: 0.342125, acc.: 85.94%] [G loss: 5.697367]\n",
      "194 [D1 loss: 0.684245, acc.: 48.44%] [D2 loss: 0.391266, acc.: 79.69%] [G loss: 6.502151]\n",
      "195 [D1 loss: 0.790347, acc.: 45.31%] [D2 loss: 0.506600, acc.: 73.44%] [G loss: 5.299318]\n",
      "196 [D1 loss: 0.644211, acc.: 53.12%] [D2 loss: 0.644161, acc.: 64.06%] [G loss: 3.396362]\n",
      "197 [D1 loss: 0.483106, acc.: 78.12%] [D2 loss: 0.242490, acc.: 93.75%] [G loss: 6.072543]\n",
      "198 [D1 loss: 0.756357, acc.: 45.31%] [D2 loss: 0.774417, acc.: 54.69%] [G loss: 3.410745]\n",
      "199 [D1 loss: 0.510392, acc.: 70.31%] [D2 loss: 0.338864, acc.: 79.69%] [G loss: 6.128485]\n",
      "200 [D1 loss: 0.735342, acc.: 37.50%] [D2 loss: 0.858407, acc.: 51.56%] [G loss: 2.875436]\n",
      "201 [D1 loss: 0.550000, acc.: 62.50%] [D2 loss: 0.339912, acc.: 79.69%] [G loss: 5.701636]\n",
      "202 [D1 loss: 0.732970, acc.: 43.75%] [D2 loss: 0.532869, acc.: 75.00%] [G loss: 4.314038]\n",
      "203 [D1 loss: 0.606327, acc.: 54.69%] [D2 loss: 0.558388, acc.: 65.62%] [G loss: 3.352872]\n",
      "204 [D1 loss: 0.621081, acc.: 51.56%] [D2 loss: 0.645506, acc.: 60.94%] [G loss: 4.428974]\n",
      "205 [D1 loss: 0.623146, acc.: 50.00%] [D2 loss: 0.659259, acc.: 59.38%] [G loss: 3.062091]\n",
      "206 [D1 loss: 0.621556, acc.: 64.06%] [D2 loss: 0.478303, acc.: 73.44%] [G loss: 3.617344]\n",
      "207 [D1 loss: 0.636723, acc.: 53.12%] [D2 loss: 0.547077, acc.: 71.88%] [G loss: 4.113982]\n",
      "208 [D1 loss: 0.607225, acc.: 56.25%] [D2 loss: 0.691799, acc.: 56.25%] [G loss: 3.418347]\n",
      "209 [D1 loss: 0.603285, acc.: 57.81%] [D2 loss: 0.419205, acc.: 71.88%] [G loss: 4.367795]\n",
      "210 [D1 loss: 0.694955, acc.: 43.75%] [D2 loss: 0.722606, acc.: 53.12%] [G loss: 2.946177]\n",
      "211 [D1 loss: 0.571114, acc.: 59.38%] [D2 loss: 0.518472, acc.: 67.19%] [G loss: 4.151344]\n",
      "212 [D1 loss: 0.756174, acc.: 34.38%] [D2 loss: 0.752129, acc.: 46.88%] [G loss: 2.572879]\n",
      "213 [D1 loss: 0.589531, acc.: 53.12%] [D2 loss: 0.488859, acc.: 68.75%] [G loss: 3.446084]\n",
      "214 [D1 loss: 0.620611, acc.: 48.44%] [D2 loss: 0.761148, acc.: 56.25%] [G loss: 2.528993]\n",
      "215 [D1 loss: 0.623815, acc.: 53.12%] [D2 loss: 0.502084, acc.: 70.31%] [G loss: 3.637863]\n",
      "216 [D1 loss: 0.610531, acc.: 57.81%] [D2 loss: 0.596182, acc.: 65.62%] [G loss: 2.534476]\n",
      "217 [D1 loss: 0.616920, acc.: 56.25%] [D2 loss: 0.369180, acc.: 84.38%] [G loss: 3.905010]\n",
      "218 [D1 loss: 0.700207, acc.: 39.06%] [D2 loss: 0.728905, acc.: 45.31%] [G loss: 2.550955]\n",
      "219 [D1 loss: 0.584594, acc.: 60.94%] [D2 loss: 0.476975, acc.: 73.44%] [G loss: 3.805166]\n",
      "220 [D1 loss: 0.638229, acc.: 54.69%] [D2 loss: 0.873770, acc.: 42.19%] [G loss: 2.031032]\n",
      "221 [D1 loss: 0.639298, acc.: 50.00%] [D2 loss: 0.528168, acc.: 71.88%] [G loss: 2.857511]\n",
      "222 [D1 loss: 0.596288, acc.: 56.25%] [D2 loss: 0.572762, acc.: 73.44%] [G loss: 3.139487]\n",
      "223 [D1 loss: 0.617512, acc.: 57.81%] [D2 loss: 0.464828, acc.: 76.56%] [G loss: 3.703909]\n",
      "224 [D1 loss: 0.708866, acc.: 40.62%] [D2 loss: 0.760292, acc.: 45.31%] [G loss: 2.197854]\n",
      "225 [D1 loss: 0.607791, acc.: 56.25%] [D2 loss: 0.525652, acc.: 65.62%] [G loss: 2.702427]\n",
      "226 [D1 loss: 0.644364, acc.: 50.00%] [D2 loss: 0.700066, acc.: 56.25%] [G loss: 2.239322]\n",
      "227 [D1 loss: 0.655549, acc.: 46.88%] [D2 loss: 0.626680, acc.: 60.94%] [G loss: 2.390410]\n",
      "228 [D1 loss: 0.610335, acc.: 53.12%] [D2 loss: 0.548404, acc.: 65.62%] [G loss: 2.570600]\n",
      "229 [D1 loss: 0.645658, acc.: 48.44%] [D2 loss: 0.752727, acc.: 48.44%] [G loss: 1.947399]\n",
      "230 [D1 loss: 0.627516, acc.: 54.69%] [D2 loss: 0.554180, acc.: 57.81%] [G loss: 2.750284]\n",
      "231 [D1 loss: 0.632752, acc.: 50.00%] [D2 loss: 0.967700, acc.: 23.44%] [G loss: 1.505498]\n",
      "232 [D1 loss: 0.620778, acc.: 56.25%] [D2 loss: 0.526465, acc.: 60.94%] [G loss: 2.179170]\n",
      "233 [D1 loss: 0.626901, acc.: 56.25%] [D2 loss: 0.539926, acc.: 73.44%] [G loss: 2.277934]\n",
      "234 [D1 loss: 0.627834, acc.: 56.25%] [D2 loss: 0.646993, acc.: 60.94%] [G loss: 2.163348]\n",
      "235 [D1 loss: 0.616571, acc.: 56.25%] [D2 loss: 0.666971, acc.: 51.56%] [G loss: 1.946287]\n",
      "236 [D1 loss: 0.631127, acc.: 59.38%] [D2 loss: 0.502977, acc.: 78.12%] [G loss: 3.015172]\n",
      "237 [D1 loss: 0.684665, acc.: 53.12%] [D2 loss: 0.827859, acc.: 35.94%] [G loss: 1.623762]\n",
      "238 [D1 loss: 0.640896, acc.: 53.12%] [D2 loss: 0.622353, acc.: 54.69%] [G loss: 1.883336]\n",
      "239 [D1 loss: 0.635202, acc.: 51.56%] [D2 loss: 0.647257, acc.: 54.69%] [G loss: 1.875538]\n",
      "240 [D1 loss: 0.647100, acc.: 46.88%] [D2 loss: 0.620116, acc.: 56.25%] [G loss: 1.817063]\n",
      "241 [D1 loss: 0.681761, acc.: 50.00%] [D2 loss: 0.645650, acc.: 57.81%] [G loss: 2.056952]\n",
      "242 [D1 loss: 0.650110, acc.: 51.56%] [D2 loss: 0.767369, acc.: 43.75%] [G loss: 1.576470]\n",
      "243 [D1 loss: 0.619662, acc.: 56.25%] [D2 loss: 0.595393, acc.: 56.25%] [G loss: 1.841501]\n",
      "244 [D1 loss: 0.640475, acc.: 54.69%] [D2 loss: 0.706607, acc.: 51.56%] [G loss: 1.792938]\n",
      "245 [D1 loss: 0.652573, acc.: 57.81%] [D2 loss: 0.619343, acc.: 64.06%] [G loss: 2.092191]\n",
      "246 [D1 loss: 0.663375, acc.: 46.88%] [D2 loss: 0.694595, acc.: 53.12%] [G loss: 1.671449]\n",
      "247 [D1 loss: 0.652746, acc.: 51.56%] [D2 loss: 0.648084, acc.: 51.56%] [G loss: 1.795644]\n",
      "248 [D1 loss: 0.607882, acc.: 62.50%] [D2 loss: 0.710929, acc.: 39.06%] [G loss: 1.532419]\n",
      "249 [D1 loss: 0.647349, acc.: 59.38%] [D2 loss: 0.657921, acc.: 48.44%] [G loss: 1.697319]\n",
      "250 [D1 loss: 0.660702, acc.: 56.25%] [D2 loss: 0.688922, acc.: 48.44%] [G loss: 1.658700]\n",
      "251 [D1 loss: 0.603521, acc.: 56.25%] [D2 loss: 0.663535, acc.: 53.12%] [G loss: 1.717029]\n",
      "252 [D1 loss: 0.603334, acc.: 57.81%] [D2 loss: 0.611652, acc.: 53.12%] [G loss: 1.790545]\n",
      "253 [D1 loss: 0.604668, acc.: 67.19%] [D2 loss: 0.641232, acc.: 56.25%] [G loss: 1.857425]\n",
      "254 [D1 loss: 0.640788, acc.: 62.50%] [D2 loss: 0.631234, acc.: 54.69%] [G loss: 1.832965]\n",
      "255 [D1 loss: 0.623422, acc.: 57.81%] [D2 loss: 0.668759, acc.: 51.56%] [G loss: 1.806195]\n",
      "256 [D1 loss: 0.592445, acc.: 75.00%] [D2 loss: 0.611354, acc.: 59.38%] [G loss: 1.833639]\n",
      "257 [D1 loss: 0.619884, acc.: 64.06%] [D2 loss: 0.625634, acc.: 60.94%] [G loss: 1.845546]\n",
      "258 [D1 loss: 0.627899, acc.: 54.69%] [D2 loss: 0.561069, acc.: 67.19%] [G loss: 2.179926]\n",
      "259 [D1 loss: 0.662532, acc.: 51.56%] [D2 loss: 0.634053, acc.: 54.69%] [G loss: 1.820395]\n",
      "260 [D1 loss: 0.581138, acc.: 67.19%] [D2 loss: 0.580207, acc.: 60.94%] [G loss: 1.715618]\n",
      "261 [D1 loss: 0.643731, acc.: 53.12%] [D2 loss: 0.669350, acc.: 56.25%] [G loss: 1.865708]\n",
      "262 [D1 loss: 0.643692, acc.: 59.38%] [D2 loss: 0.766384, acc.: 32.81%] [G loss: 1.505831]\n",
      "263 [D1 loss: 0.634066, acc.: 59.38%] [D2 loss: 0.641418, acc.: 51.56%] [G loss: 1.717248]\n",
      "264 [D1 loss: 0.668663, acc.: 50.00%] [D2 loss: 0.625778, acc.: 57.81%] [G loss: 1.745202]\n",
      "265 [D1 loss: 0.627845, acc.: 56.25%] [D2 loss: 0.671406, acc.: 48.44%] [G loss: 1.587754]\n",
      "266 [D1 loss: 0.658043, acc.: 50.00%] [D2 loss: 0.670837, acc.: 50.00%] [G loss: 1.537892]\n",
      "267 [D1 loss: 0.677077, acc.: 43.75%] [D2 loss: 0.614991, acc.: 56.25%] [G loss: 1.729782]\n",
      "268 [D1 loss: 0.650328, acc.: 50.00%] [D2 loss: 0.686857, acc.: 43.75%] [G loss: 1.550291]\n",
      "269 [D1 loss: 0.603823, acc.: 59.38%] [D2 loss: 0.648678, acc.: 46.88%] [G loss: 1.521755]\n",
      "270 [D1 loss: 0.651185, acc.: 57.81%] [D2 loss: 0.569100, acc.: 70.31%] [G loss: 1.677994]\n",
      "271 [D1 loss: 0.638459, acc.: 60.94%] [D2 loss: 0.621041, acc.: 65.62%] [G loss: 2.040143]\n",
      "272 [D1 loss: 0.623149, acc.: 64.06%] [D2 loss: 0.705395, acc.: 46.88%] [G loss: 1.648897]\n",
      "273 [D1 loss: 0.657181, acc.: 53.12%] [D2 loss: 0.618099, acc.: 56.25%] [G loss: 1.600250]\n",
      "274 [D1 loss: 0.641991, acc.: 56.25%] [D2 loss: 0.586083, acc.: 67.19%] [G loss: 1.725956]\n",
      "275 [D1 loss: 0.634501, acc.: 64.06%] [D2 loss: 0.730402, acc.: 45.31%] [G loss: 1.587564]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276 [D1 loss: 0.653919, acc.: 54.69%] [D2 loss: 0.646939, acc.: 54.69%] [G loss: 1.610748]\n",
      "277 [D1 loss: 0.654350, acc.: 50.00%] [D2 loss: 0.635938, acc.: 57.81%] [G loss: 1.657691]\n",
      "278 [D1 loss: 0.675288, acc.: 48.44%] [D2 loss: 0.617238, acc.: 64.06%] [G loss: 1.766468]\n",
      "279 [D1 loss: 0.702686, acc.: 48.44%] [D2 loss: 0.693721, acc.: 48.44%] [G loss: 1.683612]\n",
      "280 [D1 loss: 0.633698, acc.: 54.69%] [D2 loss: 0.669446, acc.: 46.88%] [G loss: 1.635872]\n",
      "281 [D1 loss: 0.633999, acc.: 59.38%] [D2 loss: 0.650157, acc.: 51.56%] [G loss: 1.711440]\n",
      "282 [D1 loss: 0.666389, acc.: 51.56%] [D2 loss: 0.651758, acc.: 48.44%] [G loss: 1.724130]\n",
      "283 [D1 loss: 0.638381, acc.: 59.38%] [D2 loss: 0.610584, acc.: 65.62%] [G loss: 1.952689]\n",
      "284 [D1 loss: 0.640811, acc.: 57.81%] [D2 loss: 0.624832, acc.: 60.94%] [G loss: 1.921546]\n",
      "285 [D1 loss: 0.654418, acc.: 56.25%] [D2 loss: 0.640638, acc.: 54.69%] [G loss: 1.744110]\n",
      "286 [D1 loss: 0.634410, acc.: 64.06%] [D2 loss: 0.683082, acc.: 46.88%] [G loss: 1.608051]\n",
      "287 [D1 loss: 0.675934, acc.: 51.56%] [D2 loss: 0.666558, acc.: 46.88%] [G loss: 1.555528]\n",
      "288 [D1 loss: 0.644665, acc.: 50.00%] [D2 loss: 0.669851, acc.: 48.44%] [G loss: 1.529327]\n",
      "289 [D1 loss: 0.622915, acc.: 54.69%] [D2 loss: 0.631430, acc.: 56.25%] [G loss: 1.567123]\n",
      "290 [D1 loss: 0.645209, acc.: 54.69%] [D2 loss: 0.651903, acc.: 54.69%] [G loss: 1.639258]\n",
      "291 [D1 loss: 0.650884, acc.: 56.25%] [D2 loss: 0.647063, acc.: 56.25%] [G loss: 1.821956]\n",
      "292 [D1 loss: 0.697056, acc.: 40.62%] [D2 loss: 0.643943, acc.: 60.94%] [G loss: 1.578770]\n",
      "293 [D1 loss: 0.617111, acc.: 54.69%] [D2 loss: 0.634426, acc.: 46.88%] [G loss: 1.618968]\n",
      "294 [D1 loss: 0.623810, acc.: 62.50%] [D2 loss: 0.582284, acc.: 68.75%] [G loss: 1.776548]\n",
      "295 [D1 loss: 0.626600, acc.: 62.50%] [D2 loss: 0.591096, acc.: 65.62%] [G loss: 1.654134]\n",
      "296 [D1 loss: 0.629274, acc.: 57.81%] [D2 loss: 0.688621, acc.: 46.88%] [G loss: 1.658848]\n",
      "297 [D1 loss: 0.667865, acc.: 57.81%] [D2 loss: 0.661747, acc.: 45.31%] [G loss: 1.628751]\n",
      "298 [D1 loss: 0.605417, acc.: 56.25%] [D2 loss: 0.652768, acc.: 57.81%] [G loss: 1.646566]\n",
      "299 [D1 loss: 0.681059, acc.: 46.88%] [D2 loss: 0.653860, acc.: 59.38%] [G loss: 1.612466]\n",
      "300 [D1 loss: 0.650580, acc.: 48.44%] [D2 loss: 0.711460, acc.: 46.88%] [G loss: 1.517465]\n",
      "301 [D1 loss: 0.641430, acc.: 53.12%] [D2 loss: 0.615256, acc.: 59.38%] [G loss: 1.676337]\n",
      "302 [D1 loss: 0.614677, acc.: 60.94%] [D2 loss: 0.667773, acc.: 48.44%] [G loss: 1.720725]\n",
      "303 [D1 loss: 0.645312, acc.: 54.69%] [D2 loss: 0.655732, acc.: 45.31%] [G loss: 1.656987]\n",
      "304 [D1 loss: 0.609856, acc.: 62.50%] [D2 loss: 0.665014, acc.: 54.69%] [G loss: 1.540175]\n",
      "305 [D1 loss: 0.632050, acc.: 56.25%] [D2 loss: 0.695161, acc.: 48.44%] [G loss: 1.521546]\n",
      "306 [D1 loss: 0.653188, acc.: 48.44%] [D2 loss: 0.681282, acc.: 45.31%] [G loss: 1.644966]\n",
      "307 [D1 loss: 0.644379, acc.: 48.44%] [D2 loss: 0.673809, acc.: 43.75%] [G loss: 1.648963]\n",
      "308 [D1 loss: 0.651902, acc.: 50.00%] [D2 loss: 0.652597, acc.: 53.12%] [G loss: 1.768699]\n",
      "309 [D1 loss: 0.663371, acc.: 45.31%] [D2 loss: 0.684411, acc.: 43.75%] [G loss: 1.572638]\n",
      "310 [D1 loss: 0.644332, acc.: 56.25%] [D2 loss: 0.650080, acc.: 60.94%] [G loss: 1.603687]\n",
      "311 [D1 loss: 0.625927, acc.: 56.25%] [D2 loss: 0.677897, acc.: 53.12%] [G loss: 1.613551]\n",
      "312 [D1 loss: 0.672140, acc.: 51.56%] [D2 loss: 0.645396, acc.: 56.25%] [G loss: 1.618713]\n",
      "313 [D1 loss: 0.681505, acc.: 50.00%] [D2 loss: 0.653951, acc.: 62.50%] [G loss: 1.711290]\n",
      "314 [D1 loss: 0.644321, acc.: 48.44%] [D2 loss: 0.643703, acc.: 57.81%] [G loss: 1.609245]\n",
      "315 [D1 loss: 0.638598, acc.: 50.00%] [D2 loss: 0.672702, acc.: 51.56%] [G loss: 1.509301]\n",
      "316 [D1 loss: 0.657269, acc.: 51.56%] [D2 loss: 0.657951, acc.: 48.44%] [G loss: 1.511078]\n",
      "317 [D1 loss: 0.659612, acc.: 45.31%] [D2 loss: 0.657105, acc.: 48.44%] [G loss: 1.547158]\n",
      "318 [D1 loss: 0.672814, acc.: 50.00%] [D2 loss: 0.651155, acc.: 46.88%] [G loss: 1.657267]\n",
      "319 [D1 loss: 0.679764, acc.: 46.88%] [D2 loss: 0.666207, acc.: 51.56%] [G loss: 1.660400]\n",
      "320 [D1 loss: 0.646907, acc.: 50.00%] [D2 loss: 0.621421, acc.: 64.06%] [G loss: 1.644211]\n",
      "321 [D1 loss: 0.624995, acc.: 64.06%] [D2 loss: 0.655285, acc.: 59.38%] [G loss: 1.635675]\n",
      "322 [D1 loss: 0.632005, acc.: 64.06%] [D2 loss: 0.642733, acc.: 57.81%] [G loss: 1.652616]\n",
      "323 [D1 loss: 0.662444, acc.: 53.12%] [D2 loss: 0.634519, acc.: 60.94%] [G loss: 1.643336]\n",
      "324 [D1 loss: 0.632610, acc.: 54.69%] [D2 loss: 0.644370, acc.: 53.12%] [G loss: 1.745668]\n",
      "325 [D1 loss: 0.626896, acc.: 56.25%] [D2 loss: 0.619285, acc.: 57.81%] [G loss: 1.685247]\n",
      "326 [D1 loss: 0.655604, acc.: 51.56%] [D2 loss: 0.625970, acc.: 57.81%] [G loss: 1.627370]\n",
      "327 [D1 loss: 0.675555, acc.: 51.56%] [D2 loss: 0.624266, acc.: 59.38%] [G loss: 1.577536]\n",
      "328 [D1 loss: 0.665571, acc.: 50.00%] [D2 loss: 0.659171, acc.: 46.88%] [G loss: 1.561805]\n",
      "329 [D1 loss: 0.634299, acc.: 54.69%] [D2 loss: 0.625336, acc.: 51.56%] [G loss: 1.575737]\n",
      "330 [D1 loss: 0.635071, acc.: 53.12%] [D2 loss: 0.584126, acc.: 67.19%] [G loss: 1.686322]\n",
      "331 [D1 loss: 0.643525, acc.: 45.31%] [D2 loss: 0.613583, acc.: 65.62%] [G loss: 1.703545]\n",
      "332 [D1 loss: 0.656960, acc.: 50.00%] [D2 loss: 0.617087, acc.: 65.62%] [G loss: 1.708120]\n",
      "333 [D1 loss: 0.646659, acc.: 46.88%] [D2 loss: 0.634384, acc.: 56.25%] [G loss: 1.683496]\n",
      "334 [D1 loss: 0.656405, acc.: 54.69%] [D2 loss: 0.602825, acc.: 65.62%] [G loss: 1.631056]\n",
      "335 [D1 loss: 0.637977, acc.: 57.81%] [D2 loss: 0.633778, acc.: 65.62%] [G loss: 1.673025]\n",
      "336 [D1 loss: 0.632323, acc.: 60.94%] [D2 loss: 0.596944, acc.: 64.06%] [G loss: 1.715336]\n",
      "337 [D1 loss: 0.670643, acc.: 56.25%] [D2 loss: 0.580938, acc.: 65.62%] [G loss: 1.726281]\n",
      "338 [D1 loss: 0.633289, acc.: 54.69%] [D2 loss: 0.680236, acc.: 43.75%] [G loss: 1.620395]\n",
      "339 [D1 loss: 0.669746, acc.: 50.00%] [D2 loss: 0.628537, acc.: 51.56%] [G loss: 1.580990]\n",
      "340 [D1 loss: 0.637403, acc.: 57.81%] [D2 loss: 0.629770, acc.: 64.06%] [G loss: 1.708350]\n",
      "341 [D1 loss: 0.633641, acc.: 60.94%] [D2 loss: 0.600324, acc.: 73.44%] [G loss: 1.708261]\n",
      "342 [D1 loss: 0.663280, acc.: 54.69%] [D2 loss: 0.679289, acc.: 48.44%] [G loss: 1.601488]\n",
      "343 [D1 loss: 0.651479, acc.: 51.56%] [D2 loss: 0.605883, acc.: 53.12%] [G loss: 1.632185]\n",
      "344 [D1 loss: 0.642570, acc.: 59.38%] [D2 loss: 0.616379, acc.: 59.38%] [G loss: 1.669818]\n",
      "345 [D1 loss: 0.634427, acc.: 48.44%] [D2 loss: 0.586421, acc.: 68.75%] [G loss: 1.697135]\n",
      "346 [D1 loss: 0.644672, acc.: 48.44%] [D2 loss: 0.651283, acc.: 60.94%] [G loss: 1.661925]\n",
      "347 [D1 loss: 0.624618, acc.: 56.25%] [D2 loss: 0.627967, acc.: 56.25%] [G loss: 1.650218]\n",
      "348 [D1 loss: 0.630735, acc.: 57.81%] [D2 loss: 0.605118, acc.: 64.06%] [G loss: 1.688924]\n",
      "349 [D1 loss: 0.613950, acc.: 68.75%] [D2 loss: 0.556382, acc.: 67.19%] [G loss: 1.731420]\n",
      "350 [D1 loss: 0.669790, acc.: 62.50%] [D2 loss: 0.567193, acc.: 68.75%] [G loss: 1.779403]\n",
      "351 [D1 loss: 0.634299, acc.: 65.62%] [D2 loss: 0.606957, acc.: 68.75%] [G loss: 1.714228]\n",
      "352 [D1 loss: 0.665625, acc.: 53.12%] [D2 loss: 0.627211, acc.: 59.38%] [G loss: 1.624874]\n",
      "353 [D1 loss: 0.624297, acc.: 54.69%] [D2 loss: 0.577698, acc.: 76.56%] [G loss: 1.842543]\n",
      "354 [D1 loss: 0.673001, acc.: 53.12%] [D2 loss: 0.587608, acc.: 62.50%] [G loss: 1.749361]\n",
      "355 [D1 loss: 0.620607, acc.: 56.25%] [D2 loss: 0.586911, acc.: 76.56%] [G loss: 1.680981]\n",
      "356 [D1 loss: 0.618332, acc.: 59.38%] [D2 loss: 0.604367, acc.: 60.94%] [G loss: 1.662617]\n",
      "357 [D1 loss: 0.619246, acc.: 62.50%] [D2 loss: 0.627661, acc.: 62.50%] [G loss: 1.694400]\n",
      "358 [D1 loss: 0.668108, acc.: 54.69%] [D2 loss: 0.595020, acc.: 60.94%] [G loss: 1.702283]\n",
      "359 [D1 loss: 0.613509, acc.: 60.94%] [D2 loss: 0.607423, acc.: 68.75%] [G loss: 1.763148]\n",
      "360 [D1 loss: 0.601375, acc.: 62.50%] [D2 loss: 0.579292, acc.: 75.00%] [G loss: 1.824350]\n",
      "361 [D1 loss: 0.657482, acc.: 54.69%] [D2 loss: 0.592336, acc.: 65.62%] [G loss: 1.817550]\n",
      "362 [D1 loss: 0.616827, acc.: 64.06%] [D2 loss: 0.638106, acc.: 50.00%] [G loss: 1.652023]\n",
      "363 [D1 loss: 0.632238, acc.: 67.19%] [D2 loss: 0.598037, acc.: 60.94%] [G loss: 1.739334]\n",
      "364 [D1 loss: 0.626971, acc.: 60.94%] [D2 loss: 0.579611, acc.: 75.00%] [G loss: 1.894066]\n",
      "365 [D1 loss: 0.640540, acc.: 53.12%] [D2 loss: 0.584625, acc.: 62.50%] [G loss: 1.971896]\n",
      "366 [D1 loss: 0.677213, acc.: 46.88%] [D2 loss: 0.639143, acc.: 56.25%] [G loss: 1.634927]\n",
      "367 [D1 loss: 0.628126, acc.: 62.50%] [D2 loss: 0.574412, acc.: 70.31%] [G loss: 1.651569]\n",
      "368 [D1 loss: 0.634495, acc.: 57.81%] [D2 loss: 0.616625, acc.: 68.75%] [G loss: 1.655150]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369 [D1 loss: 0.649106, acc.: 56.25%] [D2 loss: 0.602324, acc.: 76.56%] [G loss: 1.727276]\n",
      "370 [D1 loss: 0.626374, acc.: 59.38%] [D2 loss: 0.581048, acc.: 70.31%] [G loss: 1.684571]\n",
      "371 [D1 loss: 0.646881, acc.: 54.69%] [D2 loss: 0.627415, acc.: 57.81%] [G loss: 1.725569]\n",
      "372 [D1 loss: 0.616269, acc.: 62.50%] [D2 loss: 0.628430, acc.: 46.88%] [G loss: 1.753407]\n",
      "373 [D1 loss: 0.656470, acc.: 57.81%] [D2 loss: 0.603479, acc.: 62.50%] [G loss: 1.670519]\n",
      "374 [D1 loss: 0.627793, acc.: 62.50%] [D2 loss: 0.590040, acc.: 62.50%] [G loss: 1.698863]\n",
      "375 [D1 loss: 0.650976, acc.: 48.44%] [D2 loss: 0.565456, acc.: 76.56%] [G loss: 1.769246]\n",
      "376 [D1 loss: 0.631937, acc.: 53.12%] [D2 loss: 0.592884, acc.: 67.19%] [G loss: 1.732720]\n",
      "377 [D1 loss: 0.675485, acc.: 46.88%] [D2 loss: 0.632708, acc.: 57.81%] [G loss: 1.686882]\n",
      "378 [D1 loss: 0.662807, acc.: 51.56%] [D2 loss: 0.593269, acc.: 76.56%] [G loss: 1.761949]\n",
      "379 [D1 loss: 0.647198, acc.: 60.94%] [D2 loss: 0.591179, acc.: 65.62%] [G loss: 1.911217]\n",
      "380 [D1 loss: 0.635928, acc.: 65.62%] [D2 loss: 0.572010, acc.: 65.62%] [G loss: 1.799946]\n",
      "381 [D1 loss: 0.612650, acc.: 60.94%] [D2 loss: 0.590051, acc.: 62.50%] [G loss: 1.708184]\n",
      "382 [D1 loss: 0.595729, acc.: 71.88%] [D2 loss: 0.631584, acc.: 60.94%] [G loss: 1.701526]\n",
      "383 [D1 loss: 0.632674, acc.: 59.38%] [D2 loss: 0.586599, acc.: 67.19%] [G loss: 1.789368]\n",
      "384 [D1 loss: 0.630092, acc.: 67.19%] [D2 loss: 0.566541, acc.: 76.56%] [G loss: 1.897576]\n",
      "385 [D1 loss: 0.677572, acc.: 48.44%] [D2 loss: 0.560238, acc.: 78.12%] [G loss: 1.930773]\n",
      "386 [D1 loss: 0.660641, acc.: 54.69%] [D2 loss: 0.653194, acc.: 59.38%] [G loss: 1.687017]\n",
      "387 [D1 loss: 0.625342, acc.: 53.12%] [D2 loss: 0.582886, acc.: 65.62%] [G loss: 1.780897]\n",
      "388 [D1 loss: 0.679073, acc.: 53.12%] [D2 loss: 0.564022, acc.: 82.81%] [G loss: 1.902024]\n",
      "389 [D1 loss: 0.661247, acc.: 46.88%] [D2 loss: 0.631332, acc.: 57.81%] [G loss: 1.651463]\n",
      "390 [D1 loss: 0.624597, acc.: 67.19%] [D2 loss: 0.607528, acc.: 56.25%] [G loss: 1.762467]\n",
      "391 [D1 loss: 0.625724, acc.: 62.50%] [D2 loss: 0.610742, acc.: 60.94%] [G loss: 1.905018]\n",
      "392 [D1 loss: 0.667968, acc.: 64.06%] [D2 loss: 0.594521, acc.: 64.06%] [G loss: 2.123639]\n",
      "393 [D1 loss: 0.666930, acc.: 50.00%] [D2 loss: 0.618781, acc.: 70.31%] [G loss: 1.711324]\n",
      "394 [D1 loss: 0.642025, acc.: 53.12%] [D2 loss: 0.576165, acc.: 75.00%] [G loss: 1.760045]\n",
      "395 [D1 loss: 0.599609, acc.: 64.06%] [D2 loss: 0.619085, acc.: 65.62%] [G loss: 1.748322]\n",
      "396 [D1 loss: 0.604427, acc.: 64.06%] [D2 loss: 0.570943, acc.: 68.75%] [G loss: 1.727636]\n",
      "397 [D1 loss: 0.583917, acc.: 64.06%] [D2 loss: 0.591795, acc.: 67.19%] [G loss: 1.768257]\n",
      "398 [D1 loss: 0.646147, acc.: 64.06%] [D2 loss: 0.640400, acc.: 57.81%] [G loss: 1.763251]\n",
      "399 [D1 loss: 0.605385, acc.: 76.56%] [D2 loss: 0.618293, acc.: 64.06%] [G loss: 2.026491]\n",
      "400 [D1 loss: 0.685701, acc.: 64.06%] [D2 loss: 0.602970, acc.: 62.50%] [G loss: 2.141239]\n",
      "401 [D1 loss: 0.635775, acc.: 64.06%] [D2 loss: 0.636950, acc.: 60.94%] [G loss: 1.802845]\n",
      "402 [D1 loss: 0.605650, acc.: 57.81%] [D2 loss: 0.606141, acc.: 56.25%] [G loss: 1.788659]\n",
      "403 [D1 loss: 0.604486, acc.: 68.75%] [D2 loss: 0.597058, acc.: 70.31%] [G loss: 1.738784]\n",
      "404 [D1 loss: 0.626697, acc.: 60.94%] [D2 loss: 0.615252, acc.: 64.06%] [G loss: 1.716941]\n",
      "405 [D1 loss: 0.646581, acc.: 60.94%] [D2 loss: 0.605166, acc.: 71.88%] [G loss: 1.940328]\n",
      "406 [D1 loss: 0.597854, acc.: 62.50%] [D2 loss: 0.614993, acc.: 67.19%] [G loss: 1.947100]\n",
      "407 [D1 loss: 0.621476, acc.: 62.50%] [D2 loss: 0.599311, acc.: 62.50%] [G loss: 1.860230]\n",
      "408 [D1 loss: 0.608338, acc.: 64.06%] [D2 loss: 0.629069, acc.: 67.19%] [G loss: 1.824305]\n",
      "409 [D1 loss: 0.594934, acc.: 68.75%] [D2 loss: 0.600253, acc.: 71.88%] [G loss: 1.811751]\n",
      "410 [D1 loss: 0.614488, acc.: 68.75%] [D2 loss: 0.653128, acc.: 57.81%] [G loss: 1.720952]\n",
      "411 [D1 loss: 0.631395, acc.: 68.75%] [D2 loss: 0.572802, acc.: 67.19%] [G loss: 1.808753]\n",
      "412 [D1 loss: 0.673033, acc.: 59.38%] [D2 loss: 0.621287, acc.: 73.44%] [G loss: 1.819353]\n",
      "413 [D1 loss: 0.603979, acc.: 62.50%] [D2 loss: 0.631658, acc.: 65.62%] [G loss: 1.768702]\n",
      "414 [D1 loss: 0.602341, acc.: 62.50%] [D2 loss: 0.587086, acc.: 71.88%] [G loss: 1.820504]\n",
      "415 [D1 loss: 0.655237, acc.: 56.25%] [D2 loss: 0.582212, acc.: 75.00%] [G loss: 2.242556]\n",
      "416 [D1 loss: 0.699894, acc.: 50.00%] [D2 loss: 0.604670, acc.: 65.62%] [G loss: 1.929850]\n",
      "417 [D1 loss: 0.654352, acc.: 60.94%] [D2 loss: 0.606716, acc.: 75.00%] [G loss: 1.836792]\n",
      "418 [D1 loss: 0.634575, acc.: 64.06%] [D2 loss: 0.602439, acc.: 65.62%] [G loss: 1.666175]\n",
      "419 [D1 loss: 0.673008, acc.: 60.94%] [D2 loss: 0.561934, acc.: 78.12%] [G loss: 1.712559]\n",
      "420 [D1 loss: 0.640579, acc.: 57.81%] [D2 loss: 0.535869, acc.: 90.62%] [G loss: 2.005610]\n",
      "421 [D1 loss: 0.652042, acc.: 62.50%] [D2 loss: 0.649576, acc.: 64.06%] [G loss: 1.911555]\n",
      "422 [D1 loss: 0.578637, acc.: 73.44%] [D2 loss: 0.542496, acc.: 81.25%] [G loss: 1.947889]\n",
      "423 [D1 loss: 0.645115, acc.: 60.94%] [D2 loss: 0.646055, acc.: 59.38%] [G loss: 1.928453]\n",
      "424 [D1 loss: 0.646124, acc.: 54.69%] [D2 loss: 0.601724, acc.: 71.88%] [G loss: 1.892707]\n",
      "425 [D1 loss: 0.621441, acc.: 59.38%] [D2 loss: 0.599752, acc.: 76.56%] [G loss: 1.995369]\n",
      "426 [D1 loss: 0.646419, acc.: 60.94%] [D2 loss: 0.595351, acc.: 70.31%] [G loss: 1.879542]\n",
      "427 [D1 loss: 0.638801, acc.: 56.25%] [D2 loss: 0.561387, acc.: 81.25%] [G loss: 1.824139]\n",
      "428 [D1 loss: 0.636329, acc.: 54.69%] [D2 loss: 0.537990, acc.: 87.50%] [G loss: 2.030807]\n",
      "429 [D1 loss: 0.633612, acc.: 59.38%] [D2 loss: 0.591971, acc.: 78.12%] [G loss: 1.818840]\n",
      "430 [D1 loss: 0.606158, acc.: 62.50%] [D2 loss: 0.628246, acc.: 64.06%] [G loss: 1.691766]\n",
      "431 [D1 loss: 0.620531, acc.: 59.38%] [D2 loss: 0.556304, acc.: 82.81%] [G loss: 1.821148]\n",
      "432 [D1 loss: 0.592262, acc.: 73.44%] [D2 loss: 0.558231, acc.: 78.12%] [G loss: 2.221685]\n",
      "433 [D1 loss: 0.609215, acc.: 59.38%] [D2 loss: 0.535445, acc.: 89.06%] [G loss: 1.945074]\n",
      "434 [D1 loss: 0.616720, acc.: 54.69%] [D2 loss: 0.602936, acc.: 68.75%] [G loss: 1.857608]\n",
      "435 [D1 loss: 0.622706, acc.: 62.50%] [D2 loss: 0.520555, acc.: 84.38%] [G loss: 1.969709]\n",
      "436 [D1 loss: 0.580367, acc.: 78.12%] [D2 loss: 0.546511, acc.: 79.69%] [G loss: 1.960655]\n",
      "437 [D1 loss: 0.581799, acc.: 67.19%] [D2 loss: 0.607287, acc.: 73.44%] [G loss: 1.860911]\n",
      "438 [D1 loss: 0.567016, acc.: 71.88%] [D2 loss: 0.580445, acc.: 73.44%] [G loss: 1.904179]\n",
      "439 [D1 loss: 0.634094, acc.: 59.38%] [D2 loss: 0.596744, acc.: 65.62%] [G loss: 1.813627]\n",
      "440 [D1 loss: 0.560262, acc.: 71.88%] [D2 loss: 0.561404, acc.: 76.56%] [G loss: 2.054775]\n",
      "441 [D1 loss: 0.637866, acc.: 68.75%] [D2 loss: 0.593846, acc.: 78.12%] [G loss: 1.977431]\n",
      "442 [D1 loss: 0.626634, acc.: 62.50%] [D2 loss: 0.544041, acc.: 79.69%] [G loss: 2.102679]\n",
      "443 [D1 loss: 0.609323, acc.: 67.19%] [D2 loss: 0.597539, acc.: 75.00%] [G loss: 1.908360]\n",
      "444 [D1 loss: 0.607794, acc.: 59.38%] [D2 loss: 0.616340, acc.: 64.06%] [G loss: 1.928051]\n",
      "445 [D1 loss: 0.554094, acc.: 76.56%] [D2 loss: 0.587007, acc.: 78.12%] [G loss: 1.898510]\n",
      "446 [D1 loss: 0.566956, acc.: 73.44%] [D2 loss: 0.639037, acc.: 64.06%] [G loss: 1.804828]\n",
      "447 [D1 loss: 0.610097, acc.: 59.38%] [D2 loss: 0.612192, acc.: 57.81%] [G loss: 1.857898]\n",
      "448 [D1 loss: 0.570188, acc.: 76.56%] [D2 loss: 0.580649, acc.: 82.81%] [G loss: 1.998597]\n",
      "449 [D1 loss: 0.577844, acc.: 68.75%] [D2 loss: 0.523542, acc.: 89.06%] [G loss: 2.130044]\n",
      "450 [D1 loss: 0.542657, acc.: 79.69%] [D2 loss: 0.598982, acc.: 68.75%] [G loss: 2.002026]\n",
      "451 [D1 loss: 0.616816, acc.: 57.81%] [D2 loss: 0.576146, acc.: 71.88%] [G loss: 2.088427]\n",
      "452 [D1 loss: 0.536255, acc.: 73.44%] [D2 loss: 0.568798, acc.: 78.12%] [G loss: 2.149671]\n",
      "453 [D1 loss: 0.690538, acc.: 50.00%] [D2 loss: 0.625218, acc.: 57.81%] [G loss: 1.884476]\n",
      "454 [D1 loss: 0.608682, acc.: 73.44%] [D2 loss: 0.595069, acc.: 62.50%] [G loss: 1.861294]\n",
      "455 [D1 loss: 0.605824, acc.: 68.75%] [D2 loss: 0.624776, acc.: 51.56%] [G loss: 1.978897]\n",
      "456 [D1 loss: 0.663223, acc.: 60.94%] [D2 loss: 0.600579, acc.: 70.31%] [G loss: 2.048363]\n",
      "457 [D1 loss: 0.599746, acc.: 67.19%] [D2 loss: 0.576642, acc.: 70.31%] [G loss: 2.128758]\n",
      "458 [D1 loss: 0.598671, acc.: 70.31%] [D2 loss: 0.526266, acc.: 87.50%] [G loss: 2.076225]\n",
      "459 [D1 loss: 0.584583, acc.: 71.88%] [D2 loss: 0.583874, acc.: 67.19%] [G loss: 1.929600]\n",
      "460 [D1 loss: 0.603740, acc.: 62.50%] [D2 loss: 0.553989, acc.: 70.31%] [G loss: 1.861898]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "461 [D1 loss: 0.572991, acc.: 71.88%] [D2 loss: 0.585293, acc.: 71.88%] [G loss: 1.908547]\n",
      "462 [D1 loss: 0.607741, acc.: 65.62%] [D2 loss: 0.583373, acc.: 73.44%] [G loss: 1.926935]\n",
      "463 [D1 loss: 0.608061, acc.: 64.06%] [D2 loss: 0.521038, acc.: 87.50%] [G loss: 1.925344]\n",
      "464 [D1 loss: 0.614720, acc.: 57.81%] [D2 loss: 0.549417, acc.: 75.00%] [G loss: 2.015236]\n",
      "465 [D1 loss: 0.687753, acc.: 43.75%] [D2 loss: 0.564469, acc.: 73.44%] [G loss: 2.209119]\n",
      "466 [D1 loss: 0.629111, acc.: 67.19%] [D2 loss: 0.563425, acc.: 71.88%] [G loss: 2.007701]\n",
      "467 [D1 loss: 0.622495, acc.: 67.19%] [D2 loss: 0.607698, acc.: 71.88%] [G loss: 1.983852]\n",
      "468 [D1 loss: 0.690281, acc.: 57.81%] [D2 loss: 0.554197, acc.: 76.56%] [G loss: 2.049994]\n",
      "469 [D1 loss: 0.656624, acc.: 56.25%] [D2 loss: 0.598251, acc.: 67.19%] [G loss: 1.815611]\n",
      "470 [D1 loss: 0.650643, acc.: 54.69%] [D2 loss: 0.573157, acc.: 76.56%] [G loss: 1.960771]\n",
      "471 [D1 loss: 0.702201, acc.: 48.44%] [D2 loss: 0.610442, acc.: 67.19%] [G loss: 1.942354]\n",
      "472 [D1 loss: 0.676044, acc.: 50.00%] [D2 loss: 0.519676, acc.: 87.50%] [G loss: 2.102271]\n",
      "473 [D1 loss: 0.680476, acc.: 56.25%] [D2 loss: 0.557047, acc.: 84.38%] [G loss: 1.969482]\n",
      "474 [D1 loss: 0.671833, acc.: 54.69%] [D2 loss: 0.567555, acc.: 70.31%] [G loss: 1.942581]\n",
      "475 [D1 loss: 0.648474, acc.: 54.69%] [D2 loss: 0.570885, acc.: 75.00%] [G loss: 1.979790]\n",
      "476 [D1 loss: 0.669071, acc.: 56.25%] [D2 loss: 0.557014, acc.: 75.00%] [G loss: 1.968405]\n",
      "477 [D1 loss: 0.719394, acc.: 45.31%] [D2 loss: 0.632732, acc.: 56.25%] [G loss: 1.984464]\n",
      "478 [D1 loss: 0.644240, acc.: 50.00%] [D2 loss: 0.574431, acc.: 79.69%] [G loss: 1.910609]\n",
      "479 [D1 loss: 0.741725, acc.: 34.38%] [D2 loss: 0.520250, acc.: 78.12%] [G loss: 1.883491]\n",
      "480 [D1 loss: 0.683358, acc.: 48.44%] [D2 loss: 0.550873, acc.: 75.00%] [G loss: 2.025768]\n",
      "481 [D1 loss: 0.663142, acc.: 51.56%] [D2 loss: 0.575653, acc.: 79.69%] [G loss: 2.104641]\n",
      "482 [D1 loss: 0.638246, acc.: 54.69%] [D2 loss: 0.530952, acc.: 82.81%] [G loss: 1.899056]\n",
      "483 [D1 loss: 0.602676, acc.: 68.75%] [D2 loss: 0.552370, acc.: 81.25%] [G loss: 1.812599]\n",
      "484 [D1 loss: 0.645015, acc.: 56.25%] [D2 loss: 0.623264, acc.: 70.31%] [G loss: 1.859109]\n",
      "485 [D1 loss: 0.606873, acc.: 68.75%] [D2 loss: 0.572493, acc.: 76.56%] [G loss: 2.000250]\n",
      "486 [D1 loss: 0.623577, acc.: 67.19%] [D2 loss: 0.565423, acc.: 78.12%] [G loss: 1.936002]\n",
      "487 [D1 loss: 0.582952, acc.: 73.44%] [D2 loss: 0.548729, acc.: 81.25%] [G loss: 1.978286]\n",
      "488 [D1 loss: 0.621987, acc.: 67.19%] [D2 loss: 0.597704, acc.: 75.00%] [G loss: 1.955548]\n",
      "489 [D1 loss: 0.642952, acc.: 60.94%] [D2 loss: 0.584825, acc.: 73.44%] [G loss: 2.064317]\n",
      "490 [D1 loss: 0.612513, acc.: 64.06%] [D2 loss: 0.566585, acc.: 73.44%] [G loss: 2.046805]\n",
      "491 [D1 loss: 0.602906, acc.: 67.19%] [D2 loss: 0.554174, acc.: 79.69%] [G loss: 2.047198]\n",
      "492 [D1 loss: 0.643151, acc.: 50.00%] [D2 loss: 0.584848, acc.: 70.31%] [G loss: 2.019851]\n",
      "493 [D1 loss: 0.612110, acc.: 68.75%] [D2 loss: 0.625608, acc.: 67.19%] [G loss: 1.873633]\n",
      "494 [D1 loss: 0.595735, acc.: 68.75%] [D2 loss: 0.547928, acc.: 73.44%] [G loss: 1.908126]\n",
      "495 [D1 loss: 0.641524, acc.: 59.38%] [D2 loss: 0.612218, acc.: 65.62%] [G loss: 1.922353]\n",
      "496 [D1 loss: 0.672383, acc.: 56.25%] [D2 loss: 0.583719, acc.: 76.56%] [G loss: 2.126178]\n",
      "497 [D1 loss: 0.630339, acc.: 65.62%] [D2 loss: 0.586346, acc.: 75.00%] [G loss: 2.085788]\n",
      "498 [D1 loss: 0.663893, acc.: 53.12%] [D2 loss: 0.578649, acc.: 70.31%] [G loss: 1.847724]\n",
      "499 [D1 loss: 0.646182, acc.: 53.12%] [D2 loss: 0.601181, acc.: 67.19%] [G loss: 1.826822]\n",
      "500 [D1 loss: 0.613468, acc.: 60.94%] [D2 loss: 0.598043, acc.: 75.00%] [G loss: 1.930594]\n",
      "501 [D1 loss: 0.586173, acc.: 67.19%] [D2 loss: 0.513142, acc.: 84.38%] [G loss: 2.082688]\n",
      "502 [D1 loss: 0.675515, acc.: 48.44%] [D2 loss: 0.530571, acc.: 85.94%] [G loss: 1.986096]\n",
      "503 [D1 loss: 0.606557, acc.: 62.50%] [D2 loss: 0.560259, acc.: 76.56%] [G loss: 1.970205]\n",
      "504 [D1 loss: 0.636845, acc.: 56.25%] [D2 loss: 0.533052, acc.: 79.69%] [G loss: 1.899042]\n",
      "505 [D1 loss: 0.669649, acc.: 59.38%] [D2 loss: 0.601874, acc.: 70.31%] [G loss: 1.802593]\n",
      "506 [D1 loss: 0.609175, acc.: 57.81%] [D2 loss: 0.517731, acc.: 79.69%] [G loss: 1.996728]\n",
      "507 [D1 loss: 0.636182, acc.: 56.25%] [D2 loss: 0.576902, acc.: 65.62%] [G loss: 2.010857]\n",
      "508 [D1 loss: 0.635341, acc.: 59.38%] [D2 loss: 0.586088, acc.: 68.75%] [G loss: 1.997739]\n",
      "509 [D1 loss: 0.650986, acc.: 53.12%] [D2 loss: 0.626913, acc.: 62.50%] [G loss: 1.858594]\n",
      "510 [D1 loss: 0.579446, acc.: 73.44%] [D2 loss: 0.600144, acc.: 73.44%] [G loss: 1.896856]\n",
      "511 [D1 loss: 0.649032, acc.: 56.25%] [D2 loss: 0.624958, acc.: 65.62%] [G loss: 1.822407]\n",
      "512 [D1 loss: 0.585211, acc.: 67.19%] [D2 loss: 0.630171, acc.: 54.69%] [G loss: 2.260687]\n",
      "513 [D1 loss: 0.673809, acc.: 59.38%] [D2 loss: 0.585959, acc.: 82.81%] [G loss: 1.951169]\n",
      "514 [D1 loss: 0.637055, acc.: 60.94%] [D2 loss: 0.760148, acc.: 46.88%] [G loss: 1.768517]\n",
      "515 [D1 loss: 0.667807, acc.: 59.38%] [D2 loss: 0.578179, acc.: 73.44%] [G loss: 1.955961]\n",
      "516 [D1 loss: 0.631733, acc.: 59.38%] [D2 loss: 0.608432, acc.: 70.31%] [G loss: 1.995587]\n",
      "517 [D1 loss: 0.638826, acc.: 50.00%] [D2 loss: 0.610572, acc.: 68.75%] [G loss: 1.954459]\n",
      "518 [D1 loss: 0.582486, acc.: 67.19%] [D2 loss: 0.612937, acc.: 68.75%] [G loss: 1.936083]\n",
      "519 [D1 loss: 0.617568, acc.: 70.31%] [D2 loss: 0.604868, acc.: 57.81%] [G loss: 2.061434]\n",
      "520 [D1 loss: 0.593025, acc.: 67.19%] [D2 loss: 0.602996, acc.: 73.44%] [G loss: 1.942732]\n",
      "521 [D1 loss: 0.677629, acc.: 48.44%] [D2 loss: 0.564678, acc.: 68.75%] [G loss: 2.062915]\n",
      "522 [D1 loss: 0.599987, acc.: 67.19%] [D2 loss: 0.572189, acc.: 70.31%] [G loss: 2.197467]\n",
      "523 [D1 loss: 0.657662, acc.: 60.94%] [D2 loss: 0.553242, acc.: 75.00%] [G loss: 1.981528]\n",
      "524 [D1 loss: 0.606579, acc.: 64.06%] [D2 loss: 0.579749, acc.: 62.50%] [G loss: 1.992724]\n",
      "525 [D1 loss: 0.650113, acc.: 60.94%] [D2 loss: 0.559698, acc.: 76.56%] [G loss: 1.961964]\n",
      "526 [D1 loss: 0.681537, acc.: 60.94%] [D2 loss: 0.614147, acc.: 56.25%] [G loss: 1.811993]\n",
      "527 [D1 loss: 0.642458, acc.: 51.56%] [D2 loss: 0.625458, acc.: 59.38%] [G loss: 1.858780]\n",
      "528 [D1 loss: 0.616445, acc.: 67.19%] [D2 loss: 0.596589, acc.: 68.75%] [G loss: 1.891240]\n",
      "529 [D1 loss: 0.632708, acc.: 60.94%] [D2 loss: 0.551026, acc.: 76.56%] [G loss: 1.897476]\n",
      "530 [D1 loss: 0.608548, acc.: 64.06%] [D2 loss: 0.545009, acc.: 73.44%] [G loss: 2.098546]\n",
      "531 [D1 loss: 0.668226, acc.: 53.12%] [D2 loss: 0.639315, acc.: 60.94%] [G loss: 2.071360]\n",
      "532 [D1 loss: 0.629141, acc.: 62.50%] [D2 loss: 0.679995, acc.: 51.56%] [G loss: 1.942242]\n",
      "533 [D1 loss: 0.627543, acc.: 54.69%] [D2 loss: 0.653635, acc.: 60.94%] [G loss: 1.990597]\n",
      "534 [D1 loss: 0.641897, acc.: 60.94%] [D2 loss: 0.607907, acc.: 67.19%] [G loss: 1.999618]\n",
      "535 [D1 loss: 0.628446, acc.: 57.81%] [D2 loss: 0.649501, acc.: 64.06%] [G loss: 2.000783]\n",
      "536 [D1 loss: 0.635919, acc.: 59.38%] [D2 loss: 0.654679, acc.: 57.81%] [G loss: 1.907534]\n",
      "537 [D1 loss: 0.568213, acc.: 70.31%] [D2 loss: 0.627886, acc.: 64.06%] [G loss: 2.091043]\n",
      "538 [D1 loss: 0.650602, acc.: 60.94%] [D2 loss: 0.669274, acc.: 53.12%] [G loss: 1.816711]\n",
      "539 [D1 loss: 0.597021, acc.: 65.62%] [D2 loss: 0.597976, acc.: 70.31%] [G loss: 1.900834]\n",
      "540 [D1 loss: 0.656352, acc.: 60.94%] [D2 loss: 0.578712, acc.: 68.75%] [G loss: 2.024989]\n",
      "541 [D1 loss: 0.672745, acc.: 50.00%] [D2 loss: 0.598675, acc.: 81.25%] [G loss: 1.914466]\n",
      "542 [D1 loss: 0.677366, acc.: 48.44%] [D2 loss: 0.644918, acc.: 60.94%] [G loss: 1.778068]\n",
      "543 [D1 loss: 0.587651, acc.: 70.31%] [D2 loss: 0.613258, acc.: 67.19%] [G loss: 1.827411]\n",
      "544 [D1 loss: 0.654599, acc.: 62.50%] [D2 loss: 0.599123, acc.: 73.44%] [G loss: 2.035581]\n",
      "545 [D1 loss: 0.666085, acc.: 53.12%] [D2 loss: 0.576506, acc.: 76.56%] [G loss: 1.891948]\n",
      "546 [D1 loss: 0.620393, acc.: 68.75%] [D2 loss: 0.553562, acc.: 73.44%] [G loss: 1.955511]\n",
      "547 [D1 loss: 0.592228, acc.: 70.31%] [D2 loss: 0.521693, acc.: 78.12%] [G loss: 2.006108]\n",
      "548 [D1 loss: 0.704279, acc.: 48.44%] [D2 loss: 0.604454, acc.: 68.75%] [G loss: 1.816750]\n",
      "549 [D1 loss: 0.634254, acc.: 62.50%] [D2 loss: 0.578240, acc.: 75.00%] [G loss: 1.959168]\n",
      "550 [D1 loss: 0.624450, acc.: 57.81%] [D2 loss: 0.576186, acc.: 71.88%] [G loss: 1.996776]\n",
      "551 [D1 loss: 0.652064, acc.: 57.81%] [D2 loss: 0.571889, acc.: 79.69%] [G loss: 2.000315]\n",
      "552 [D1 loss: 0.672302, acc.: 51.56%] [D2 loss: 0.543198, acc.: 79.69%] [G loss: 2.042004]\n",
      "553 [D1 loss: 0.686384, acc.: 54.69%] [D2 loss: 0.588465, acc.: 78.12%] [G loss: 1.844763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554 [D1 loss: 0.656677, acc.: 53.12%] [D2 loss: 0.551433, acc.: 79.69%] [G loss: 1.933757]\n",
      "555 [D1 loss: 0.657222, acc.: 56.25%] [D2 loss: 0.592855, acc.: 62.50%] [G loss: 1.855131]\n",
      "556 [D1 loss: 0.671259, acc.: 54.69%] [D2 loss: 0.555087, acc.: 73.44%] [G loss: 1.788247]\n",
      "557 [D1 loss: 0.668951, acc.: 59.38%] [D2 loss: 0.558062, acc.: 71.88%] [G loss: 1.885299]\n",
      "558 [D1 loss: 0.689522, acc.: 56.25%] [D2 loss: 0.614891, acc.: 65.62%] [G loss: 1.859081]\n",
      "559 [D1 loss: 0.635548, acc.: 62.50%] [D2 loss: 0.539409, acc.: 82.81%] [G loss: 1.994881]\n",
      "560 [D1 loss: 0.661093, acc.: 53.12%] [D2 loss: 0.610273, acc.: 65.62%] [G loss: 1.815862]\n",
      "561 [D1 loss: 0.629016, acc.: 60.94%] [D2 loss: 0.568264, acc.: 78.12%] [G loss: 1.848925]\n",
      "562 [D1 loss: 0.618825, acc.: 65.62%] [D2 loss: 0.590310, acc.: 71.88%] [G loss: 1.891282]\n",
      "563 [D1 loss: 0.655436, acc.: 59.38%] [D2 loss: 0.616844, acc.: 68.75%] [G loss: 1.993249]\n",
      "564 [D1 loss: 0.629020, acc.: 65.62%] [D2 loss: 0.552940, acc.: 75.00%] [G loss: 1.990035]\n",
      "565 [D1 loss: 0.659749, acc.: 54.69%] [D2 loss: 0.599906, acc.: 65.62%] [G loss: 1.969101]\n",
      "566 [D1 loss: 0.627884, acc.: 65.62%] [D2 loss: 0.585381, acc.: 70.31%] [G loss: 1.901068]\n",
      "567 [D1 loss: 0.641551, acc.: 57.81%] [D2 loss: 0.628940, acc.: 65.62%] [G loss: 1.768431]\n",
      "568 [D1 loss: 0.631268, acc.: 64.06%] [D2 loss: 0.604116, acc.: 70.31%] [G loss: 1.849283]\n",
      "569 [D1 loss: 0.639988, acc.: 65.62%] [D2 loss: 0.579862, acc.: 79.69%] [G loss: 1.989259]\n",
      "570 [D1 loss: 0.653094, acc.: 62.50%] [D2 loss: 0.598800, acc.: 67.19%] [G loss: 1.986462]\n",
      "571 [D1 loss: 0.681077, acc.: 56.25%] [D2 loss: 0.604205, acc.: 68.75%] [G loss: 1.930102]\n",
      "572 [D1 loss: 0.605310, acc.: 73.44%] [D2 loss: 0.594638, acc.: 73.44%] [G loss: 1.952128]\n",
      "573 [D1 loss: 0.637904, acc.: 62.50%] [D2 loss: 0.616122, acc.: 67.19%] [G loss: 1.822214]\n",
      "574 [D1 loss: 0.602370, acc.: 67.19%] [D2 loss: 0.594519, acc.: 67.19%] [G loss: 1.816543]\n",
      "575 [D1 loss: 0.589647, acc.: 60.94%] [D2 loss: 0.565737, acc.: 71.88%] [G loss: 1.947823]\n",
      "576 [D1 loss: 0.619356, acc.: 65.62%] [D2 loss: 0.613999, acc.: 67.19%] [G loss: 1.995844]\n",
      "577 [D1 loss: 0.639306, acc.: 62.50%] [D2 loss: 0.564763, acc.: 87.50%] [G loss: 1.989476]\n",
      "578 [D1 loss: 0.676305, acc.: 62.50%] [D2 loss: 0.573365, acc.: 78.12%] [G loss: 1.889340]\n",
      "579 [D1 loss: 0.649377, acc.: 62.50%] [D2 loss: 0.556259, acc.: 78.12%] [G loss: 1.867182]\n",
      "580 [D1 loss: 0.663187, acc.: 56.25%] [D2 loss: 0.547263, acc.: 79.69%] [G loss: 1.934047]\n",
      "581 [D1 loss: 0.642227, acc.: 64.06%] [D2 loss: 0.653233, acc.: 59.38%] [G loss: 1.899850]\n",
      "582 [D1 loss: 0.694849, acc.: 54.69%] [D2 loss: 0.566739, acc.: 64.06%] [G loss: 2.050945]\n",
      "583 [D1 loss: 0.662609, acc.: 56.25%] [D2 loss: 0.572462, acc.: 79.69%] [G loss: 1.925592]\n",
      "584 [D1 loss: 0.697081, acc.: 43.75%] [D2 loss: 0.648016, acc.: 60.94%] [G loss: 1.806852]\n",
      "585 [D1 loss: 0.674547, acc.: 48.44%] [D2 loss: 0.634675, acc.: 60.94%] [G loss: 1.824584]\n",
      "586 [D1 loss: 0.624449, acc.: 71.88%] [D2 loss: 0.612662, acc.: 65.62%] [G loss: 2.036734]\n",
      "587 [D1 loss: 0.650249, acc.: 59.38%] [D2 loss: 0.545497, acc.: 78.12%] [G loss: 1.997630]\n",
      "588 [D1 loss: 0.636026, acc.: 60.94%] [D2 loss: 0.643028, acc.: 60.94%] [G loss: 1.837501]\n",
      "589 [D1 loss: 0.640904, acc.: 59.38%] [D2 loss: 0.559455, acc.: 81.25%] [G loss: 1.940405]\n",
      "590 [D1 loss: 0.683450, acc.: 57.81%] [D2 loss: 0.608171, acc.: 68.75%] [G loss: 1.912377]\n",
      "591 [D1 loss: 0.609427, acc.: 64.06%] [D2 loss: 0.608784, acc.: 65.62%] [G loss: 1.896721]\n",
      "592 [D1 loss: 0.637648, acc.: 65.62%] [D2 loss: 0.615452, acc.: 62.50%] [G loss: 1.828321]\n",
      "593 [D1 loss: 0.606694, acc.: 65.62%] [D2 loss: 0.614495, acc.: 70.31%] [G loss: 1.900952]\n",
      "594 [D1 loss: 0.634637, acc.: 64.06%] [D2 loss: 0.541148, acc.: 84.38%] [G loss: 1.964666]\n",
      "595 [D1 loss: 0.638351, acc.: 65.62%] [D2 loss: 0.630329, acc.: 64.06%] [G loss: 1.832799]\n",
      "596 [D1 loss: 0.579374, acc.: 62.50%] [D2 loss: 0.529323, acc.: 79.69%] [G loss: 1.975519]\n",
      "597 [D1 loss: 0.613806, acc.: 71.88%] [D2 loss: 0.608261, acc.: 67.19%] [G loss: 1.840620]\n",
      "598 [D1 loss: 0.641150, acc.: 70.31%] [D2 loss: 0.589823, acc.: 65.62%] [G loss: 1.879738]\n",
      "599 [D1 loss: 0.607708, acc.: 65.62%] [D2 loss: 0.560034, acc.: 71.88%] [G loss: 1.998610]\n",
      "600 [D1 loss: 0.619267, acc.: 71.88%] [D2 loss: 0.633461, acc.: 54.69%] [G loss: 1.853312]\n",
      "601 [D1 loss: 0.625383, acc.: 67.19%] [D2 loss: 0.589291, acc.: 67.19%] [G loss: 1.784479]\n",
      "602 [D1 loss: 0.604211, acc.: 71.88%] [D2 loss: 0.592564, acc.: 70.31%] [G loss: 1.915036]\n",
      "603 [D1 loss: 0.610659, acc.: 70.31%] [D2 loss: 0.503420, acc.: 84.38%] [G loss: 2.152204]\n",
      "604 [D1 loss: 0.634232, acc.: 60.94%] [D2 loss: 0.585665, acc.: 62.50%] [G loss: 2.269944]\n",
      "605 [D1 loss: 0.646519, acc.: 59.38%] [D2 loss: 0.582201, acc.: 71.88%] [G loss: 1.906702]\n",
      "606 [D1 loss: 0.632389, acc.: 59.38%] [D2 loss: 0.616546, acc.: 68.75%] [G loss: 1.782442]\n",
      "607 [D1 loss: 0.623930, acc.: 64.06%] [D2 loss: 0.582659, acc.: 70.31%] [G loss: 1.815150]\n",
      "608 [D1 loss: 0.637669, acc.: 62.50%] [D2 loss: 0.573477, acc.: 70.31%] [G loss: 1.865956]\n",
      "609 [D1 loss: 0.633847, acc.: 62.50%] [D2 loss: 0.570820, acc.: 71.88%] [G loss: 2.021419]\n",
      "610 [D1 loss: 0.618820, acc.: 56.25%] [D2 loss: 0.604435, acc.: 73.44%] [G loss: 2.026386]\n",
      "611 [D1 loss: 0.633760, acc.: 57.81%] [D2 loss: 0.572962, acc.: 81.25%] [G loss: 1.990294]\n",
      "612 [D1 loss: 0.638907, acc.: 54.69%] [D2 loss: 0.592926, acc.: 73.44%] [G loss: 1.798129]\n",
      "613 [D1 loss: 0.657775, acc.: 62.50%] [D2 loss: 0.604696, acc.: 68.75%] [G loss: 1.846942]\n",
      "614 [D1 loss: 0.606857, acc.: 65.62%] [D2 loss: 0.602429, acc.: 75.00%] [G loss: 1.997123]\n",
      "615 [D1 loss: 0.622286, acc.: 65.62%] [D2 loss: 0.570209, acc.: 78.12%] [G loss: 1.948686]\n",
      "616 [D1 loss: 0.671705, acc.: 62.50%] [D2 loss: 0.590224, acc.: 71.88%] [G loss: 1.873863]\n",
      "617 [D1 loss: 0.599746, acc.: 68.75%] [D2 loss: 0.566487, acc.: 81.25%] [G loss: 1.991223]\n",
      "618 [D1 loss: 0.603493, acc.: 70.31%] [D2 loss: 0.592914, acc.: 65.62%] [G loss: 1.973083]\n",
      "619 [D1 loss: 0.593321, acc.: 65.62%] [D2 loss: 0.591219, acc.: 62.50%] [G loss: 1.959965]\n",
      "620 [D1 loss: 0.639046, acc.: 59.38%] [D2 loss: 0.579530, acc.: 76.56%] [G loss: 2.003991]\n",
      "621 [D1 loss: 0.645489, acc.: 59.38%] [D2 loss: 0.537291, acc.: 93.75%] [G loss: 2.121432]\n",
      "622 [D1 loss: 0.641980, acc.: 67.19%] [D2 loss: 0.572545, acc.: 75.00%] [G loss: 1.881357]\n",
      "623 [D1 loss: 0.593891, acc.: 71.88%] [D2 loss: 0.546425, acc.: 79.69%] [G loss: 1.875760]\n",
      "624 [D1 loss: 0.643447, acc.: 70.31%] [D2 loss: 0.615875, acc.: 70.31%] [G loss: 2.036045]\n",
      "625 [D1 loss: 0.647688, acc.: 60.94%] [D2 loss: 0.523790, acc.: 85.94%] [G loss: 2.073424]\n",
      "626 [D1 loss: 0.659831, acc.: 51.56%] [D2 loss: 0.625533, acc.: 67.19%] [G loss: 1.907184]\n",
      "627 [D1 loss: 0.636127, acc.: 67.19%] [D2 loss: 0.615271, acc.: 64.06%] [G loss: 2.060704]\n",
      "628 [D1 loss: 0.591882, acc.: 70.31%] [D2 loss: 0.562324, acc.: 78.12%] [G loss: 2.132390]\n",
      "629 [D1 loss: 0.639504, acc.: 62.50%] [D2 loss: 0.626410, acc.: 73.44%] [G loss: 1.876048]\n",
      "630 [D1 loss: 0.610289, acc.: 64.06%] [D2 loss: 0.556306, acc.: 79.69%] [G loss: 2.064460]\n",
      "631 [D1 loss: 0.624258, acc.: 64.06%] [D2 loss: 0.601028, acc.: 68.75%] [G loss: 2.010828]\n",
      "632 [D1 loss: 0.675997, acc.: 56.25%] [D2 loss: 0.601807, acc.: 64.06%] [G loss: 2.076843]\n",
      "633 [D1 loss: 0.643703, acc.: 71.88%] [D2 loss: 0.569199, acc.: 68.75%] [G loss: 1.931259]\n",
      "634 [D1 loss: 0.687272, acc.: 60.94%] [D2 loss: 0.600736, acc.: 64.06%] [G loss: 1.834144]\n",
      "635 [D1 loss: 0.698609, acc.: 50.00%] [D2 loss: 0.594170, acc.: 71.88%] [G loss: 1.973508]\n",
      "636 [D1 loss: 0.672980, acc.: 50.00%] [D2 loss: 0.579091, acc.: 76.56%] [G loss: 1.938236]\n",
      "637 [D1 loss: 0.620460, acc.: 60.94%] [D2 loss: 0.628005, acc.: 62.50%] [G loss: 1.887890]\n",
      "638 [D1 loss: 0.641892, acc.: 54.69%] [D2 loss: 0.565931, acc.: 76.56%] [G loss: 2.029656]\n",
      "639 [D1 loss: 0.628704, acc.: 68.75%] [D2 loss: 0.631463, acc.: 70.31%] [G loss: 1.849776]\n",
      "640 [D1 loss: 0.664946, acc.: 53.12%] [D2 loss: 0.620735, acc.: 64.06%] [G loss: 1.767051]\n",
      "641 [D1 loss: 0.638588, acc.: 70.31%] [D2 loss: 0.561889, acc.: 73.44%] [G loss: 1.971243]\n",
      "642 [D1 loss: 0.683869, acc.: 54.69%] [D2 loss: 0.584041, acc.: 73.44%] [G loss: 1.859727]\n",
      "643 [D1 loss: 0.674572, acc.: 54.69%] [D2 loss: 0.510701, acc.: 81.25%] [G loss: 2.056680]\n",
      "644 [D1 loss: 0.656261, acc.: 57.81%] [D2 loss: 0.606720, acc.: 65.62%] [G loss: 1.921846]\n",
      "645 [D1 loss: 0.679043, acc.: 46.88%] [D2 loss: 0.600180, acc.: 67.19%] [G loss: 1.948890]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646 [D1 loss: 0.632455, acc.: 60.94%] [D2 loss: 0.544936, acc.: 79.69%] [G loss: 1.965943]\n",
      "647 [D1 loss: 0.642681, acc.: 57.81%] [D2 loss: 0.540561, acc.: 82.81%] [G loss: 1.984786]\n",
      "648 [D1 loss: 0.598486, acc.: 73.44%] [D2 loss: 0.543306, acc.: 81.25%] [G loss: 2.137443]\n",
      "649 [D1 loss: 0.678221, acc.: 59.38%] [D2 loss: 0.603401, acc.: 68.75%] [G loss: 1.920970]\n",
      "650 [D1 loss: 0.619958, acc.: 56.25%] [D2 loss: 0.593940, acc.: 67.19%] [G loss: 1.887103]\n",
      "651 [D1 loss: 0.653784, acc.: 64.06%] [D2 loss: 0.610702, acc.: 68.75%] [G loss: 1.821408]\n",
      "652 [D1 loss: 0.609857, acc.: 71.88%] [D2 loss: 0.600728, acc.: 62.50%] [G loss: 1.837106]\n",
      "653 [D1 loss: 0.626838, acc.: 64.06%] [D2 loss: 0.540287, acc.: 76.56%] [G loss: 1.853718]\n",
      "654 [D1 loss: 0.621068, acc.: 64.06%] [D2 loss: 0.589529, acc.: 70.31%] [G loss: 1.900631]\n",
      "655 [D1 loss: 0.640601, acc.: 59.38%] [D2 loss: 0.608633, acc.: 67.19%] [G loss: 2.034640]\n",
      "656 [D1 loss: 0.601379, acc.: 73.44%] [D2 loss: 0.583598, acc.: 73.44%] [G loss: 2.008693]\n",
      "657 [D1 loss: 0.605041, acc.: 67.19%] [D2 loss: 0.560490, acc.: 75.00%] [G loss: 1.908389]\n",
      "658 [D1 loss: 0.670678, acc.: 59.38%] [D2 loss: 0.561658, acc.: 79.69%] [G loss: 1.970716]\n",
      "659 [D1 loss: 0.638168, acc.: 60.94%] [D2 loss: 0.576439, acc.: 81.25%] [G loss: 2.029127]\n",
      "660 [D1 loss: 0.624551, acc.: 60.94%] [D2 loss: 0.579571, acc.: 67.19%] [G loss: 1.893458]\n",
      "661 [D1 loss: 0.593505, acc.: 70.31%] [D2 loss: 0.561078, acc.: 78.12%] [G loss: 1.904283]\n",
      "662 [D1 loss: 0.637966, acc.: 62.50%] [D2 loss: 0.583466, acc.: 70.31%] [G loss: 1.956290]\n",
      "663 [D1 loss: 0.601107, acc.: 71.88%] [D2 loss: 0.599428, acc.: 64.06%] [G loss: 1.920979]\n",
      "664 [D1 loss: 0.600696, acc.: 75.00%] [D2 loss: 0.580634, acc.: 68.75%] [G loss: 1.870273]\n",
      "665 [D1 loss: 0.616464, acc.: 71.88%] [D2 loss: 0.580307, acc.: 75.00%] [G loss: 1.837093]\n",
      "666 [D1 loss: 0.576604, acc.: 73.44%] [D2 loss: 0.606777, acc.: 71.88%] [G loss: 1.871458]\n",
      "667 [D1 loss: 0.630665, acc.: 62.50%] [D2 loss: 0.583656, acc.: 75.00%] [G loss: 1.962037]\n",
      "668 [D1 loss: 0.640215, acc.: 70.31%] [D2 loss: 0.585001, acc.: 70.31%] [G loss: 1.932662]\n",
      "669 [D1 loss: 0.632667, acc.: 67.19%] [D2 loss: 0.582863, acc.: 75.00%] [G loss: 1.971356]\n",
      "670 [D1 loss: 0.581004, acc.: 81.25%] [D2 loss: 0.596638, acc.: 68.75%] [G loss: 1.982845]\n",
      "671 [D1 loss: 0.596969, acc.: 73.44%] [D2 loss: 0.582025, acc.: 78.12%] [G loss: 1.947588]\n",
      "672 [D1 loss: 0.629716, acc.: 68.75%] [D2 loss: 0.619372, acc.: 62.50%] [G loss: 1.860298]\n",
      "673 [D1 loss: 0.606897, acc.: 65.62%] [D2 loss: 0.641511, acc.: 62.50%] [G loss: 1.819607]\n",
      "674 [D1 loss: 0.632593, acc.: 60.94%] [D2 loss: 0.604321, acc.: 70.31%] [G loss: 1.849660]\n",
      "675 [D1 loss: 0.656334, acc.: 56.25%] [D2 loss: 0.576076, acc.: 84.38%] [G loss: 2.025799]\n",
      "676 [D1 loss: 0.624982, acc.: 60.94%] [D2 loss: 0.603654, acc.: 75.00%] [G loss: 1.958026]\n",
      "677 [D1 loss: 0.611545, acc.: 71.88%] [D2 loss: 0.625928, acc.: 65.62%] [G loss: 1.907000]\n",
      "678 [D1 loss: 0.586149, acc.: 78.12%] [D2 loss: 0.555801, acc.: 79.69%] [G loss: 1.918271]\n",
      "679 [D1 loss: 0.664006, acc.: 59.38%] [D2 loss: 0.601705, acc.: 71.88%] [G loss: 2.025326]\n",
      "680 [D1 loss: 0.651155, acc.: 62.50%] [D2 loss: 0.561720, acc.: 79.69%] [G loss: 2.009704]\n",
      "681 [D1 loss: 0.620432, acc.: 60.94%] [D2 loss: 0.569490, acc.: 79.69%] [G loss: 2.041477]\n",
      "682 [D1 loss: 0.613535, acc.: 70.31%] [D2 loss: 0.520728, acc.: 81.25%] [G loss: 1.968942]\n",
      "683 [D1 loss: 0.650094, acc.: 59.38%] [D2 loss: 0.586319, acc.: 68.75%] [G loss: 2.010141]\n",
      "684 [D1 loss: 0.641229, acc.: 62.50%] [D2 loss: 0.529681, acc.: 82.81%] [G loss: 1.957415]\n",
      "685 [D1 loss: 0.585438, acc.: 71.88%] [D2 loss: 0.558857, acc.: 82.81%] [G loss: 1.975677]\n",
      "686 [D1 loss: 0.616085, acc.: 67.19%] [D2 loss: 0.585531, acc.: 70.31%] [G loss: 1.865064]\n",
      "687 [D1 loss: 0.607246, acc.: 70.31%] [D2 loss: 0.572192, acc.: 79.69%] [G loss: 1.943016]\n",
      "688 [D1 loss: 0.597780, acc.: 71.88%] [D2 loss: 0.568703, acc.: 82.81%] [G loss: 2.023699]\n",
      "689 [D1 loss: 0.670572, acc.: 54.69%] [D2 loss: 0.566613, acc.: 76.56%] [G loss: 2.215507]\n",
      "690 [D1 loss: 0.647455, acc.: 71.88%] [D2 loss: 0.578547, acc.: 71.88%] [G loss: 2.022222]\n",
      "691 [D1 loss: 0.658061, acc.: 51.56%] [D2 loss: 0.632819, acc.: 60.94%] [G loss: 1.854321]\n",
      "692 [D1 loss: 0.639892, acc.: 59.38%] [D2 loss: 0.592302, acc.: 64.06%] [G loss: 2.021557]\n",
      "693 [D1 loss: 0.648729, acc.: 57.81%] [D2 loss: 0.598517, acc.: 68.75%] [G loss: 1.940166]\n",
      "694 [D1 loss: 0.598593, acc.: 76.56%] [D2 loss: 0.558362, acc.: 81.25%] [G loss: 2.037830]\n",
      "695 [D1 loss: 0.641616, acc.: 57.81%] [D2 loss: 0.627005, acc.: 65.62%] [G loss: 2.112571]\n",
      "696 [D1 loss: 0.669267, acc.: 50.00%] [D2 loss: 0.563857, acc.: 79.69%] [G loss: 2.176769]\n",
      "697 [D1 loss: 0.640383, acc.: 64.06%] [D2 loss: 0.580045, acc.: 75.00%] [G loss: 2.164877]\n",
      "698 [D1 loss: 0.660907, acc.: 53.12%] [D2 loss: 0.635534, acc.: 65.62%] [G loss: 2.114859]\n",
      "699 [D1 loss: 0.612806, acc.: 65.62%] [D2 loss: 0.621891, acc.: 67.19%] [G loss: 1.920854]\n",
      "700 [D1 loss: 0.618088, acc.: 62.50%] [D2 loss: 0.597723, acc.: 71.88%] [G loss: 1.895851]\n",
      "701 [D1 loss: 0.672309, acc.: 60.94%] [D2 loss: 0.569427, acc.: 71.88%] [G loss: 2.032705]\n",
      "702 [D1 loss: 0.673054, acc.: 56.25%] [D2 loss: 0.583130, acc.: 78.12%] [G loss: 2.028746]\n",
      "703 [D1 loss: 0.690110, acc.: 51.56%] [D2 loss: 0.588454, acc.: 76.56%] [G loss: 2.107887]\n",
      "704 [D1 loss: 0.653675, acc.: 56.25%] [D2 loss: 0.592967, acc.: 71.88%] [G loss: 2.221279]\n",
      "705 [D1 loss: 0.649891, acc.: 56.25%] [D2 loss: 0.613460, acc.: 65.62%] [G loss: 1.902595]\n",
      "706 [D1 loss: 0.583077, acc.: 68.75%] [D2 loss: 0.585636, acc.: 73.44%] [G loss: 2.007211]\n",
      "707 [D1 loss: 0.670768, acc.: 53.12%] [D2 loss: 0.610226, acc.: 60.94%] [G loss: 1.922130]\n",
      "708 [D1 loss: 0.610789, acc.: 64.06%] [D2 loss: 0.547590, acc.: 82.81%] [G loss: 2.026495]\n",
      "709 [D1 loss: 0.591871, acc.: 67.19%] [D2 loss: 0.528867, acc.: 90.62%] [G loss: 2.020702]\n",
      "710 [D1 loss: 0.611612, acc.: 65.62%] [D2 loss: 0.602239, acc.: 68.75%] [G loss: 2.026818]\n",
      "711 [D1 loss: 0.579589, acc.: 60.94%] [D2 loss: 0.628306, acc.: 65.62%] [G loss: 2.186144]\n",
      "712 [D1 loss: 0.643512, acc.: 64.06%] [D2 loss: 0.571277, acc.: 75.00%] [G loss: 2.054855]\n",
      "713 [D1 loss: 0.614855, acc.: 67.19%] [D2 loss: 0.518380, acc.: 84.38%] [G loss: 2.109935]\n",
      "714 [D1 loss: 0.588244, acc.: 68.75%] [D2 loss: 0.599280, acc.: 70.31%] [G loss: 1.944333]\n",
      "715 [D1 loss: 0.622123, acc.: 68.75%] [D2 loss: 0.637246, acc.: 57.81%] [G loss: 1.818413]\n",
      "716 [D1 loss: 0.585684, acc.: 68.75%] [D2 loss: 0.562249, acc.: 73.44%] [G loss: 1.913190]\n",
      "717 [D1 loss: 0.639796, acc.: 59.38%] [D2 loss: 0.590973, acc.: 67.19%] [G loss: 1.897229]\n",
      "718 [D1 loss: 0.583508, acc.: 60.94%] [D2 loss: 0.639904, acc.: 59.38%] [G loss: 2.017157]\n",
      "719 [D1 loss: 0.642672, acc.: 59.38%] [D2 loss: 0.575532, acc.: 67.19%] [G loss: 2.230415]\n",
      "720 [D1 loss: 0.549825, acc.: 84.38%] [D2 loss: 0.598677, acc.: 62.50%] [G loss: 2.279849]\n",
      "721 [D1 loss: 0.655315, acc.: 67.19%] [D2 loss: 0.606327, acc.: 67.19%] [G loss: 2.018703]\n",
      "722 [D1 loss: 0.624109, acc.: 65.62%] [D2 loss: 0.621080, acc.: 64.06%] [G loss: 1.920843]\n",
      "723 [D1 loss: 0.596351, acc.: 64.06%] [D2 loss: 0.635288, acc.: 57.81%] [G loss: 1.933957]\n",
      "724 [D1 loss: 0.622021, acc.: 60.94%] [D2 loss: 0.587834, acc.: 76.56%] [G loss: 2.072978]\n",
      "725 [D1 loss: 0.631005, acc.: 64.06%] [D2 loss: 0.557224, acc.: 81.25%] [G loss: 2.071390]\n",
      "726 [D1 loss: 0.597832, acc.: 67.19%] [D2 loss: 0.562483, acc.: 79.69%] [G loss: 1.960862]\n",
      "727 [D1 loss: 0.618109, acc.: 65.62%] [D2 loss: 0.559881, acc.: 76.56%] [G loss: 2.010353]\n",
      "728 [D1 loss: 0.556390, acc.: 78.12%] [D2 loss: 0.548024, acc.: 73.44%] [G loss: 1.991304]\n",
      "729 [D1 loss: 0.588101, acc.: 67.19%] [D2 loss: 0.565360, acc.: 79.69%] [G loss: 2.023610]\n",
      "730 [D1 loss: 0.537583, acc.: 84.38%] [D2 loss: 0.541873, acc.: 76.56%] [G loss: 2.059408]\n",
      "731 [D1 loss: 0.612304, acc.: 56.25%] [D2 loss: 0.542872, acc.: 76.56%] [G loss: 2.038984]\n",
      "732 [D1 loss: 0.585572, acc.: 75.00%] [D2 loss: 0.581237, acc.: 70.31%] [G loss: 2.194630]\n",
      "733 [D1 loss: 0.585250, acc.: 73.44%] [D2 loss: 0.573539, acc.: 79.69%] [G loss: 2.187225]\n",
      "734 [D1 loss: 0.589608, acc.: 70.31%] [D2 loss: 0.662452, acc.: 54.69%] [G loss: 1.956987]\n",
      "735 [D1 loss: 0.587199, acc.: 68.75%] [D2 loss: 0.580297, acc.: 65.62%] [G loss: 1.959643]\n",
      "736 [D1 loss: 0.600971, acc.: 68.75%] [D2 loss: 0.572666, acc.: 79.69%] [G loss: 2.025130]\n",
      "737 [D1 loss: 0.583763, acc.: 76.56%] [D2 loss: 0.601876, acc.: 79.69%] [G loss: 1.984782]\n",
      "738 [D1 loss: 0.592228, acc.: 71.88%] [D2 loss: 0.583876, acc.: 73.44%] [G loss: 1.944657]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "739 [D1 loss: 0.568002, acc.: 76.56%] [D2 loss: 0.609512, acc.: 68.75%] [G loss: 2.022719]\n",
      "740 [D1 loss: 0.676395, acc.: 60.94%] [D2 loss: 0.569555, acc.: 73.44%] [G loss: 1.986013]\n",
      "741 [D1 loss: 0.600103, acc.: 78.12%] [D2 loss: 0.589144, acc.: 73.44%] [G loss: 2.057251]\n",
      "742 [D1 loss: 0.617674, acc.: 60.94%] [D2 loss: 0.597452, acc.: 71.88%] [G loss: 2.007678]\n",
      "743 [D1 loss: 0.612027, acc.: 71.88%] [D2 loss: 0.636504, acc.: 67.19%] [G loss: 2.058329]\n",
      "744 [D1 loss: 0.591829, acc.: 65.62%] [D2 loss: 0.532425, acc.: 79.69%] [G loss: 2.085129]\n",
      "745 [D1 loss: 0.616142, acc.: 59.38%] [D2 loss: 0.646717, acc.: 57.81%] [G loss: 1.862557]\n",
      "746 [D1 loss: 0.652308, acc.: 59.38%] [D2 loss: 0.609483, acc.: 68.75%] [G loss: 1.946368]\n",
      "747 [D1 loss: 0.620820, acc.: 62.50%] [D2 loss: 0.645504, acc.: 62.50%] [G loss: 1.970520]\n",
      "748 [D1 loss: 0.653098, acc.: 60.94%] [D2 loss: 0.598462, acc.: 70.31%] [G loss: 2.110260]\n",
      "749 [D1 loss: 0.660742, acc.: 50.00%] [D2 loss: 0.647441, acc.: 54.69%] [G loss: 1.988805]\n",
      "750 [D1 loss: 0.637294, acc.: 57.81%] [D2 loss: 0.649264, acc.: 57.81%] [G loss: 1.973138]\n",
      "751 [D1 loss: 0.605934, acc.: 64.06%] [D2 loss: 0.673026, acc.: 54.69%] [G loss: 2.352722]\n",
      "752 [D1 loss: 0.649442, acc.: 59.38%] [D2 loss: 0.652591, acc.: 59.38%] [G loss: 2.063364]\n",
      "753 [D1 loss: 0.673382, acc.: 57.81%] [D2 loss: 0.600016, acc.: 59.38%] [G loss: 2.010445]\n",
      "754 [D1 loss: 0.630058, acc.: 64.06%] [D2 loss: 0.604781, acc.: 62.50%] [G loss: 2.123637]\n",
      "755 [D1 loss: 0.601213, acc.: 70.31%] [D2 loss: 0.591201, acc.: 70.31%] [G loss: 1.922610]\n",
      "756 [D1 loss: 0.613808, acc.: 68.75%] [D2 loss: 0.604624, acc.: 68.75%] [G loss: 1.921448]\n",
      "757 [D1 loss: 0.597667, acc.: 64.06%] [D2 loss: 0.601491, acc.: 65.62%] [G loss: 1.966004]\n",
      "758 [D1 loss: 0.622550, acc.: 56.25%] [D2 loss: 0.563215, acc.: 75.00%] [G loss: 2.004324]\n",
      "759 [D1 loss: 0.634222, acc.: 56.25%] [D2 loss: 0.540719, acc.: 82.81%] [G loss: 2.162310]\n",
      "760 [D1 loss: 0.597564, acc.: 68.75%] [D2 loss: 0.626946, acc.: 65.62%] [G loss: 2.097295]\n",
      "761 [D1 loss: 0.589481, acc.: 73.44%] [D2 loss: 0.609263, acc.: 60.94%] [G loss: 1.951585]\n",
      "762 [D1 loss: 0.623685, acc.: 64.06%] [D2 loss: 0.581380, acc.: 70.31%] [G loss: 2.060350]\n",
      "763 [D1 loss: 0.585460, acc.: 67.19%] [D2 loss: 0.612695, acc.: 65.62%] [G loss: 2.216062]\n",
      "764 [D1 loss: 0.637598, acc.: 60.94%] [D2 loss: 0.593573, acc.: 75.00%] [G loss: 2.048747]\n",
      "765 [D1 loss: 0.632988, acc.: 64.06%] [D2 loss: 0.609133, acc.: 64.06%] [G loss: 1.955309]\n",
      "766 [D1 loss: 0.617616, acc.: 71.88%] [D2 loss: 0.594826, acc.: 65.62%] [G loss: 1.978494]\n",
      "767 [D1 loss: 0.620586, acc.: 65.62%] [D2 loss: 0.524752, acc.: 84.38%] [G loss: 2.062409]\n",
      "768 [D1 loss: 0.661983, acc.: 59.38%] [D2 loss: 0.623342, acc.: 59.38%] [G loss: 2.275888]\n",
      "769 [D1 loss: 0.660883, acc.: 64.06%] [D2 loss: 0.581817, acc.: 75.00%] [G loss: 2.138784]\n",
      "770 [D1 loss: 0.616065, acc.: 67.19%] [D2 loss: 0.580782, acc.: 70.31%] [G loss: 2.187195]\n",
      "771 [D1 loss: 0.600752, acc.: 75.00%] [D2 loss: 0.552583, acc.: 73.44%] [G loss: 2.121817]\n",
      "772 [D1 loss: 0.601120, acc.: 64.06%] [D2 loss: 0.547731, acc.: 71.88%] [G loss: 2.131577]\n",
      "773 [D1 loss: 0.603938, acc.: 68.75%] [D2 loss: 0.600444, acc.: 70.31%] [G loss: 2.083889]\n",
      "774 [D1 loss: 0.622985, acc.: 68.75%] [D2 loss: 0.637809, acc.: 59.38%] [G loss: 1.840737]\n",
      "775 [D1 loss: 0.605565, acc.: 70.31%] [D2 loss: 0.625323, acc.: 65.62%] [G loss: 1.949179]\n",
      "776 [D1 loss: 0.632537, acc.: 67.19%] [D2 loss: 0.531161, acc.: 82.81%] [G loss: 2.200442]\n",
      "777 [D1 loss: 0.674026, acc.: 51.56%] [D2 loss: 0.562166, acc.: 75.00%] [G loss: 2.012422]\n",
      "778 [D1 loss: 0.605263, acc.: 65.62%] [D2 loss: 0.590516, acc.: 70.31%] [G loss: 1.881207]\n",
      "779 [D1 loss: 0.601021, acc.: 73.44%] [D2 loss: 0.543123, acc.: 71.88%] [G loss: 2.064277]\n",
      "780 [D1 loss: 0.646117, acc.: 56.25%] [D2 loss: 0.568642, acc.: 78.12%] [G loss: 2.074869]\n",
      "781 [D1 loss: 0.558856, acc.: 73.44%] [D2 loss: 0.592224, acc.: 75.00%] [G loss: 2.040954]\n",
      "782 [D1 loss: 0.590634, acc.: 76.56%] [D2 loss: 0.655309, acc.: 59.38%] [G loss: 1.998917]\n",
      "783 [D1 loss: 0.587116, acc.: 70.31%] [D2 loss: 0.554034, acc.: 78.12%] [G loss: 2.067967]\n",
      "784 [D1 loss: 0.613497, acc.: 65.62%] [D2 loss: 0.586926, acc.: 73.44%] [G loss: 2.024924]\n",
      "785 [D1 loss: 0.585673, acc.: 76.56%] [D2 loss: 0.599925, acc.: 70.31%] [G loss: 1.878955]\n",
      "786 [D1 loss: 0.596927, acc.: 68.75%] [D2 loss: 0.632328, acc.: 64.06%] [G loss: 1.874344]\n",
      "787 [D1 loss: 0.640352, acc.: 64.06%] [D2 loss: 0.563530, acc.: 79.69%] [G loss: 1.989007]\n",
      "788 [D1 loss: 0.595766, acc.: 70.31%] [D2 loss: 0.591826, acc.: 73.44%] [G loss: 1.926550]\n",
      "789 [D1 loss: 0.608991, acc.: 70.31%] [D2 loss: 0.536119, acc.: 78.12%] [G loss: 2.126019]\n",
      "790 [D1 loss: 0.579309, acc.: 79.69%] [D2 loss: 0.575726, acc.: 78.12%] [G loss: 2.109857]\n",
      "791 [D1 loss: 0.583310, acc.: 81.25%] [D2 loss: 0.561663, acc.: 81.25%] [G loss: 2.070934]\n",
      "792 [D1 loss: 0.584543, acc.: 82.81%] [D2 loss: 0.546781, acc.: 78.12%] [G loss: 2.166513]\n",
      "793 [D1 loss: 0.582922, acc.: 75.00%] [D2 loss: 0.599532, acc.: 73.44%] [G loss: 2.063299]\n",
      "794 [D1 loss: 0.566824, acc.: 81.25%] [D2 loss: 0.568090, acc.: 76.56%] [G loss: 1.933568]\n",
      "795 [D1 loss: 0.608112, acc.: 76.56%] [D2 loss: 0.589839, acc.: 71.88%] [G loss: 1.906257]\n",
      "796 [D1 loss: 0.632251, acc.: 62.50%] [D2 loss: 0.542742, acc.: 81.25%] [G loss: 1.904677]\n",
      "797 [D1 loss: 0.564676, acc.: 76.56%] [D2 loss: 0.554412, acc.: 81.25%] [G loss: 1.943604]\n",
      "798 [D1 loss: 0.583541, acc.: 75.00%] [D2 loss: 0.546543, acc.: 82.81%] [G loss: 1.921886]\n",
      "799 [D1 loss: 0.569506, acc.: 81.25%] [D2 loss: 0.564798, acc.: 73.44%] [G loss: 2.043144]\n",
      "800 [D1 loss: 0.593418, acc.: 68.75%] [D2 loss: 0.581732, acc.: 76.56%] [G loss: 2.012343]\n",
      "801 [D1 loss: 0.607458, acc.: 76.56%] [D2 loss: 0.540530, acc.: 79.69%] [G loss: 1.850523]\n",
      "802 [D1 loss: 0.552044, acc.: 81.25%] [D2 loss: 0.527645, acc.: 82.81%] [G loss: 2.083876]\n",
      "803 [D1 loss: 0.563369, acc.: 78.12%] [D2 loss: 0.524209, acc.: 85.94%] [G loss: 2.004523]\n",
      "804 [D1 loss: 0.616745, acc.: 68.75%] [D2 loss: 0.572066, acc.: 71.88%] [G loss: 2.013441]\n",
      "805 [D1 loss: 0.601663, acc.: 70.31%] [D2 loss: 0.577936, acc.: 67.19%] [G loss: 2.072155]\n",
      "806 [D1 loss: 0.569862, acc.: 76.56%] [D2 loss: 0.548055, acc.: 84.38%] [G loss: 2.135553]\n",
      "807 [D1 loss: 0.618716, acc.: 64.06%] [D2 loss: 0.646668, acc.: 57.81%] [G loss: 2.078980]\n",
      "808 [D1 loss: 0.551504, acc.: 79.69%] [D2 loss: 0.602783, acc.: 67.19%] [G loss: 1.987370]\n",
      "809 [D1 loss: 0.590833, acc.: 71.88%] [D2 loss: 0.648527, acc.: 62.50%] [G loss: 1.898107]\n",
      "810 [D1 loss: 0.586868, acc.: 68.75%] [D2 loss: 0.595818, acc.: 65.62%] [G loss: 1.995182]\n",
      "811 [D1 loss: 0.584621, acc.: 67.19%] [D2 loss: 0.569303, acc.: 73.44%] [G loss: 2.066300]\n",
      "812 [D1 loss: 0.553050, acc.: 75.00%] [D2 loss: 0.603455, acc.: 62.50%] [G loss: 2.091852]\n",
      "813 [D1 loss: 0.589871, acc.: 68.75%] [D2 loss: 0.636515, acc.: 60.94%] [G loss: 1.911642]\n",
      "814 [D1 loss: 0.523819, acc.: 81.25%] [D2 loss: 0.597599, acc.: 67.19%] [G loss: 2.004453]\n",
      "815 [D1 loss: 0.564503, acc.: 81.25%] [D2 loss: 0.585732, acc.: 68.75%] [G loss: 1.978462]\n",
      "816 [D1 loss: 0.567198, acc.: 75.00%] [D2 loss: 0.626655, acc.: 62.50%] [G loss: 1.994876]\n",
      "817 [D1 loss: 0.554521, acc.: 78.12%] [D2 loss: 0.604192, acc.: 70.31%] [G loss: 2.133556]\n",
      "818 [D1 loss: 0.637528, acc.: 57.81%] [D2 loss: 0.569144, acc.: 81.25%] [G loss: 2.176718]\n",
      "819 [D1 loss: 0.596266, acc.: 73.44%] [D2 loss: 0.593795, acc.: 64.06%] [G loss: 2.056625]\n",
      "820 [D1 loss: 0.582043, acc.: 71.88%] [D2 loss: 0.593210, acc.: 71.88%] [G loss: 2.025293]\n",
      "821 [D1 loss: 0.585223, acc.: 71.88%] [D2 loss: 0.564503, acc.: 78.12%] [G loss: 2.064969]\n",
      "822 [D1 loss: 0.553714, acc.: 79.69%] [D2 loss: 0.599782, acc.: 68.75%] [G loss: 2.035518]\n",
      "823 [D1 loss: 0.562077, acc.: 78.12%] [D2 loss: 0.562875, acc.: 75.00%] [G loss: 2.004371]\n",
      "824 [D1 loss: 0.563854, acc.: 76.56%] [D2 loss: 0.539906, acc.: 81.25%] [G loss: 2.088034]\n",
      "825 [D1 loss: 0.546013, acc.: 78.12%] [D2 loss: 0.502380, acc.: 90.62%] [G loss: 2.170506]\n",
      "826 [D1 loss: 0.629031, acc.: 65.62%] [D2 loss: 0.620048, acc.: 64.06%] [G loss: 1.991298]\n",
      "827 [D1 loss: 0.581607, acc.: 70.31%] [D2 loss: 0.605845, acc.: 67.19%] [G loss: 2.024374]\n",
      "828 [D1 loss: 0.563519, acc.: 82.81%] [D2 loss: 0.526996, acc.: 82.81%] [G loss: 2.065583]\n",
      "829 [D1 loss: 0.584905, acc.: 70.31%] [D2 loss: 0.484466, acc.: 92.19%] [G loss: 2.131747]\n",
      "830 [D1 loss: 0.572133, acc.: 75.00%] [D2 loss: 0.567710, acc.: 76.56%] [G loss: 2.082944]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "831 [D1 loss: 0.541850, acc.: 78.12%] [D2 loss: 0.527481, acc.: 78.12%] [G loss: 2.170136]\n",
      "832 [D1 loss: 0.549221, acc.: 85.94%] [D2 loss: 0.514486, acc.: 84.38%] [G loss: 2.127663]\n",
      "833 [D1 loss: 0.596434, acc.: 68.75%] [D2 loss: 0.549695, acc.: 84.38%] [G loss: 1.967735]\n",
      "834 [D1 loss: 0.555623, acc.: 73.44%] [D2 loss: 0.533949, acc.: 81.25%] [G loss: 2.122163]\n",
      "835 [D1 loss: 0.552028, acc.: 81.25%] [D2 loss: 0.557519, acc.: 73.44%] [G loss: 2.063494]\n",
      "836 [D1 loss: 0.548270, acc.: 78.12%] [D2 loss: 0.547294, acc.: 75.00%] [G loss: 2.098363]\n",
      "837 [D1 loss: 0.633860, acc.: 64.06%] [D2 loss: 0.544058, acc.: 75.00%] [G loss: 2.076979]\n",
      "838 [D1 loss: 0.589020, acc.: 71.88%] [D2 loss: 0.549479, acc.: 78.12%] [G loss: 2.194468]\n",
      "839 [D1 loss: 0.598377, acc.: 60.94%] [D2 loss: 0.594772, acc.: 65.62%] [G loss: 2.171523]\n"
     ]
    }
   ],
   "source": [
    "epochs=3000\n",
    "# epochs=30000\n",
    "train(g1, g2, d1, d2, combined,\n",
    "      epochs=epochs, batch_size=32, sample_interval=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
