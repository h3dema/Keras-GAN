{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional GAN\n",
    "\n",
    "Ref.: DONAHUE, Jeff; KRÄHENBÜHL, Philipp; DARRELL, Trevor.  \n",
    "      Adversarial feature learning. arXiv preprint arXiv:1605.09782, 2016.  \n",
    "      https://arxiv.org/abs/1605.09782"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(latent_dim, img_shape):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(512, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
    "    model.add(Reshape(img_shape))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    z = Input(shape=(latent_dim,))\n",
    "    gen_img = model(z)\n",
    "\n",
    "    return Model(z, gen_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(latent_dim, img_shape):\n",
    "\n",
    "    z = Input(shape=(latent_dim, ))\n",
    "    img = Input(shape=img_shape)\n",
    "    d_in = concatenate([z, Flatten()(img)])\n",
    "\n",
    "    model = Dense(1024)(d_in)\n",
    "    model = LeakyReLU(alpha=0.2)(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "    model = Dense(1024)(model)\n",
    "    model = LeakyReLU(alpha=0.2)(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "    model = Dense(1024)(model)\n",
    "    model = LeakyReLU(alpha=0.2)(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "    validity = Dense(1, activation=\"sigmoid\")(model)\n",
    "\n",
    "    return Model([z, img], validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(latent_dim, img_shape):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Flatten(input_shape=img_shape))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(latent_dim))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "    z = model(img)\n",
    "\n",
    "    return Model(img, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_sample_interval(G, latent_dim, epoch):\n",
    "    r, c = 5, 5\n",
    "    z = np.random.normal(size=(25, latent_dim))\n",
    "    gen_imgs = G.predict(z)\n",
    "\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(G, D, encoder, bigan_generator, \n",
    "          latent_dim, \n",
    "          epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "    # Load the dataset\n",
    "    (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "    # Rescale -1 to 1\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "    # Adversarial ground truths\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        # Sample noise and generate img\n",
    "        z = np.random.normal(size=(batch_size, latent_dim))\n",
    "        imgs_ = G.predict(z)\n",
    "\n",
    "        # Select a random batch of images and encode\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        imgs = X_train[idx]\n",
    "        z_ = encoder.predict(imgs)\n",
    "\n",
    "        # Train the discriminator (img -> z is valid, z -> img is fake)\n",
    "        d_loss_real = D.train_on_batch([z_, imgs], valid)\n",
    "        d_loss_fake = D.train_on_batch([z, imgs_], fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "\n",
    "        # Train the generator (z -> img is valid and img -> z is is invalid)\n",
    "        g_loss = bigan_generator.train_on_batch([z, imgs], [valid, fake])\n",
    "\n",
    "        # Plot the progress\n",
    "        print (\"%d [D loss: %f, acc: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0]))\n",
    "\n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % sample_interval == 0  or epoch == epochs - 1:\n",
    "            do_sample_interval(G, latent_dim, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# create optimizer\n",
    "optimizer = Adam(0.0002, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Build and compile the discriminator\n",
    "D = build_discriminator(latent_dim, img_shape)\n",
    "D.compile(loss=['binary_crossentropy'],\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 512)               51712     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               402192    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 720,656\n",
      "Trainable params: 718,608\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the generator\n",
    "G = build_generator(latent_dim, img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               51300     \n",
      "=================================================================\n",
      "Total params: 719,972\n",
      "Trainable params: 717,924\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the encoder\n",
    "encoder = build_encoder(latent_dim, img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The part of the bigan that trains the discriminator and encoder\n",
    "D.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate image from sampled noise\n",
    "z = Input(shape=(latent_dim, ))\n",
    "img_ = G(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode image\n",
    "img = Input(shape=img_shape)\n",
    "z_ = encoder(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent -> img is fake, and img -> latent is valid\n",
    "fake = D([z, img_])\n",
    "valid = D([z_, img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up and compile the combined model\n",
    "# Trains generator to fool the discriminator\n",
    "bigan_generator = Model([z, img], [fake, valid])\n",
    "bigan_generator.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\n",
    "    optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.266661, acc: 90.62%] [G loss: 5.525970]\n",
      "1 [D loss: 0.159406, acc: 100.00%] [G loss: 5.670177]\n",
      "2 [D loss: 0.110983, acc: 98.44%] [G loss: 6.560831]\n",
      "3 [D loss: 0.070067, acc: 100.00%] [G loss: 7.139620]\n",
      "4 [D loss: 0.056305, acc: 100.00%] [G loss: 8.750211]\n",
      "5 [D loss: 0.043628, acc: 100.00%] [G loss: 9.320330]\n",
      "6 [D loss: 0.033754, acc: 100.00%] [G loss: 10.177339]\n",
      "7 [D loss: 0.022928, acc: 100.00%] [G loss: 10.352518]\n",
      "8 [D loss: 0.012991, acc: 100.00%] [G loss: 10.128178]\n",
      "9 [D loss: 0.015250, acc: 100.00%] [G loss: 11.003234]\n",
      "10 [D loss: 0.023476, acc: 100.00%] [G loss: 11.932797]\n",
      "11 [D loss: 0.009466, acc: 100.00%] [G loss: 12.238003]\n",
      "12 [D loss: 0.008053, acc: 100.00%] [G loss: 12.639989]\n",
      "13 [D loss: 0.018690, acc: 98.44%] [G loss: 13.858693]\n",
      "14 [D loss: 0.009253, acc: 100.00%] [G loss: 13.544466]\n",
      "15 [D loss: 0.011066, acc: 100.00%] [G loss: 13.547765]\n",
      "16 [D loss: 0.011858, acc: 100.00%] [G loss: 13.203274]\n",
      "17 [D loss: 0.003389, acc: 100.00%] [G loss: 13.329329]\n",
      "18 [D loss: 0.027912, acc: 98.44%] [G loss: 14.933147]\n",
      "19 [D loss: 0.006500, acc: 100.00%] [G loss: 15.263142]\n",
      "20 [D loss: 0.004505, acc: 100.00%] [G loss: 15.180937]\n",
      "21 [D loss: 0.009092, acc: 100.00%] [G loss: 15.376012]\n",
      "22 [D loss: 0.009921, acc: 100.00%] [G loss: 16.546253]\n",
      "23 [D loss: 0.006004, acc: 100.00%] [G loss: 15.836638]\n",
      "24 [D loss: 0.060187, acc: 98.44%] [G loss: 19.538635]\n",
      "25 [D loss: 0.008221, acc: 100.00%] [G loss: 18.733860]\n",
      "26 [D loss: 0.008812, acc: 100.00%] [G loss: 19.743858]\n",
      "27 [D loss: 0.008333, acc: 100.00%] [G loss: 20.376621]\n",
      "28 [D loss: 0.032851, acc: 98.44%] [G loss: 23.211636]\n",
      "29 [D loss: 0.004601, acc: 100.00%] [G loss: 24.630346]\n",
      "30 [D loss: 0.007492, acc: 100.00%] [G loss: 22.554138]\n",
      "31 [D loss: 0.049916, acc: 98.44%] [G loss: 26.175133]\n",
      "32 [D loss: 0.022498, acc: 100.00%] [G loss: 27.992802]\n",
      "33 [D loss: 0.003228, acc: 100.00%] [G loss: 26.077610]\n",
      "34 [D loss: 0.004716, acc: 100.00%] [G loss: 25.697849]\n",
      "35 [D loss: 0.009858, acc: 100.00%] [G loss: 25.441673]\n",
      "36 [D loss: 0.008389, acc: 100.00%] [G loss: 24.828188]\n",
      "37 [D loss: 0.004333, acc: 100.00%] [G loss: 25.514313]\n",
      "38 [D loss: 0.002369, acc: 100.00%] [G loss: 25.045937]\n",
      "39 [D loss: 0.006103, acc: 100.00%] [G loss: 24.574017]\n",
      "40 [D loss: 0.132137, acc: 96.88%] [G loss: 34.893387]\n",
      "41 [D loss: 0.031665, acc: 98.44%] [G loss: 34.177483]\n",
      "42 [D loss: 0.013645, acc: 100.00%] [G loss: 32.511719]\n",
      "43 [D loss: 0.009835, acc: 100.00%] [G loss: 32.358681]\n",
      "44 [D loss: 0.092756, acc: 98.44%] [G loss: 37.562897]\n",
      "45 [D loss: 0.007694, acc: 100.00%] [G loss: 37.725010]\n",
      "46 [D loss: 0.005892, acc: 100.00%] [G loss: 34.764503]\n",
      "47 [D loss: 0.013089, acc: 100.00%] [G loss: 36.275597]\n",
      "48 [D loss: 0.006252, acc: 100.00%] [G loss: 35.595860]\n",
      "49 [D loss: 0.011700, acc: 100.00%] [G loss: 31.494267]\n",
      "50 [D loss: 0.069959, acc: 98.44%] [G loss: 38.211578]\n",
      "51 [D loss: 0.006145, acc: 100.00%] [G loss: 37.173523]\n",
      "52 [D loss: 0.108707, acc: 96.88%] [G loss: 43.211498]\n",
      "53 [D loss: 0.029145, acc: 98.44%] [G loss: 39.747234]\n",
      "54 [D loss: 0.001647, acc: 100.00%] [G loss: 38.549438]\n",
      "55 [D loss: 0.028315, acc: 96.88%] [G loss: 37.546089]\n",
      "56 [D loss: 0.002875, acc: 100.00%] [G loss: 36.130898]\n",
      "57 [D loss: 0.227655, acc: 96.88%] [G loss: 36.174141]\n",
      "58 [D loss: 0.001888, acc: 100.00%] [G loss: 37.762054]\n",
      "59 [D loss: 0.658855, acc: 92.19%] [G loss: 56.142628]\n",
      "60 [D loss: 0.505179, acc: 84.38%] [G loss: 54.656448]\n",
      "61 [D loss: 0.128940, acc: 95.31%] [G loss: 44.513695]\n",
      "62 [D loss: 0.642010, acc: 87.50%] [G loss: 63.443794]\n",
      "63 [D loss: 0.078521, acc: 96.88%] [G loss: 60.533966]\n",
      "64 [D loss: 0.284916, acc: 93.75%] [G loss: 60.920586]\n",
      "65 [D loss: 0.117190, acc: 93.75%] [G loss: 55.014370]\n",
      "66 [D loss: 0.033658, acc: 98.44%] [G loss: 47.531586]\n",
      "67 [D loss: 1.495348, acc: 89.06%] [G loss: 64.395164]\n",
      "68 [D loss: 0.355082, acc: 92.19%] [G loss: 55.579647]\n",
      "69 [D loss: 0.208584, acc: 93.75%] [G loss: 54.371906]\n",
      "70 [D loss: 0.109134, acc: 95.31%] [G loss: 45.313988]\n",
      "71 [D loss: 0.118532, acc: 95.31%] [G loss: 48.211281]\n",
      "72 [D loss: 0.179130, acc: 90.62%] [G loss: 36.511234]\n",
      "73 [D loss: 0.509260, acc: 90.62%] [G loss: 36.368008]\n",
      "74 [D loss: 1.406782, acc: 84.38%] [G loss: 54.139946]\n",
      "75 [D loss: 0.710099, acc: 87.50%] [G loss: 53.476879]\n",
      "76 [D loss: 0.237101, acc: 93.75%] [G loss: 40.874371]\n",
      "77 [D loss: 0.570113, acc: 89.06%] [G loss: 35.905266]\n",
      "78 [D loss: 0.288154, acc: 92.19%] [G loss: 37.745537]\n",
      "79 [D loss: 0.422200, acc: 93.75%] [G loss: 47.673115]\n",
      "80 [D loss: 0.392610, acc: 93.75%] [G loss: 35.475681]\n",
      "81 [D loss: 0.215062, acc: 92.19%] [G loss: 24.857851]\n",
      "82 [D loss: 1.111985, acc: 81.25%] [G loss: 50.357826]\n",
      "83 [D loss: 0.298064, acc: 87.50%] [G loss: 44.390179]\n",
      "84 [D loss: 0.284814, acc: 90.62%] [G loss: 34.201366]\n",
      "85 [D loss: 0.441628, acc: 87.50%] [G loss: 32.450191]\n",
      "86 [D loss: 1.036876, acc: 79.69%] [G loss: 43.156227]\n",
      "87 [D loss: 0.387449, acc: 92.19%] [G loss: 40.760685]\n",
      "88 [D loss: 0.093763, acc: 96.88%] [G loss: 30.279446]\n",
      "89 [D loss: 0.068674, acc: 95.31%] [G loss: 29.646606]\n",
      "90 [D loss: 0.274535, acc: 87.50%] [G loss: 23.746845]\n",
      "91 [D loss: 1.070296, acc: 79.69%] [G loss: 35.289059]\n",
      "92 [D loss: 0.702899, acc: 76.56%] [G loss: 33.190231]\n",
      "93 [D loss: 0.639341, acc: 96.88%] [G loss: 28.912998]\n",
      "94 [D loss: 0.346182, acc: 92.19%] [G loss: 29.618618]\n",
      "95 [D loss: 0.288781, acc: 92.19%] [G loss: 31.737082]\n",
      "96 [D loss: 0.269033, acc: 90.62%] [G loss: 26.255436]\n",
      "97 [D loss: 0.108725, acc: 93.75%] [G loss: 18.181934]\n",
      "98 [D loss: 0.882261, acc: 81.25%] [G loss: 30.612967]\n",
      "99 [D loss: 0.299261, acc: 89.06%] [G loss: 24.884239]\n",
      "100 [D loss: 0.186343, acc: 95.31%] [G loss: 25.459604]\n",
      "101 [D loss: 0.570930, acc: 79.69%] [G loss: 27.651619]\n",
      "102 [D loss: 0.512542, acc: 82.81%] [G loss: 22.998993]\n",
      "103 [D loss: 0.082750, acc: 95.31%] [G loss: 20.565239]\n",
      "104 [D loss: 0.320385, acc: 87.50%] [G loss: 24.793041]\n",
      "105 [D loss: 0.237851, acc: 90.62%] [G loss: 20.951900]\n",
      "106 [D loss: 0.209868, acc: 87.50%] [G loss: 24.600445]\n",
      "107 [D loss: 0.084924, acc: 96.88%] [G loss: 23.855373]\n",
      "108 [D loss: 0.566109, acc: 82.81%] [G loss: 22.895960]\n",
      "109 [D loss: 0.160450, acc: 90.62%] [G loss: 20.146893]\n",
      "110 [D loss: 0.202020, acc: 90.62%] [G loss: 18.417301]\n",
      "111 [D loss: 0.184488, acc: 89.06%] [G loss: 18.374268]\n",
      "112 [D loss: 0.101428, acc: 95.31%] [G loss: 18.310953]\n",
      "113 [D loss: 0.060126, acc: 95.31%] [G loss: 18.722536]\n",
      "114 [D loss: 0.075082, acc: 96.88%] [G loss: 14.391109]\n",
      "115 [D loss: 0.449277, acc: 87.50%] [G loss: 17.290648]\n",
      "116 [D loss: 0.886885, acc: 75.00%] [G loss: 20.125591]\n",
      "117 [D loss: 0.214816, acc: 87.50%] [G loss: 17.476589]\n",
      "118 [D loss: 0.103372, acc: 96.88%] [G loss: 15.941668]\n",
      "119 [D loss: 0.252548, acc: 92.19%] [G loss: 17.274883]\n",
      "120 [D loss: 0.103292, acc: 95.31%] [G loss: 15.783428]\n",
      "121 [D loss: 0.242794, acc: 92.19%] [G loss: 17.200314]\n",
      "122 [D loss: 0.551330, acc: 81.25%] [G loss: 17.311047]\n",
      "123 [D loss: 0.120713, acc: 95.31%] [G loss: 12.155771]\n",
      "124 [D loss: 0.639810, acc: 78.12%] [G loss: 24.102793]\n",
      "125 [D loss: 0.560601, acc: 79.69%] [G loss: 17.158916]\n",
      "126 [D loss: 0.404429, acc: 92.19%] [G loss: 13.416829]\n",
      "127 [D loss: 0.904075, acc: 73.44%] [G loss: 19.345793]\n",
      "128 [D loss: 0.419834, acc: 87.50%] [G loss: 14.931042]\n",
      "129 [D loss: 0.421246, acc: 90.62%] [G loss: 16.364838]\n",
      "130 [D loss: 0.183784, acc: 92.19%] [G loss: 14.073920]\n",
      "131 [D loss: 0.550726, acc: 82.81%] [G loss: 15.118105]\n",
      "132 [D loss: 0.242578, acc: 92.19%] [G loss: 13.055098]\n",
      "133 [D loss: 0.220521, acc: 89.06%] [G loss: 10.551792]\n",
      "134 [D loss: 0.330207, acc: 90.62%] [G loss: 13.360210]\n",
      "135 [D loss: 0.319280, acc: 85.94%] [G loss: 11.405841]\n",
      "136 [D loss: 0.545435, acc: 71.88%] [G loss: 12.453643]\n",
      "137 [D loss: 0.534479, acc: 76.56%] [G loss: 13.693468]\n",
      "138 [D loss: 0.294618, acc: 90.62%] [G loss: 10.583169]\n",
      "139 [D loss: 0.364007, acc: 84.38%] [G loss: 13.877802]\n",
      "140 [D loss: 0.372349, acc: 87.50%] [G loss: 11.667500]\n",
      "141 [D loss: 0.437637, acc: 78.12%] [G loss: 11.307655]\n",
      "142 [D loss: 0.363934, acc: 81.25%] [G loss: 11.709321]\n",
      "143 [D loss: 0.272978, acc: 85.94%] [G loss: 11.783680]\n",
      "144 [D loss: 0.312143, acc: 84.38%] [G loss: 12.845135]\n",
      "145 [D loss: 0.174166, acc: 92.19%] [G loss: 9.594530]\n",
      "146 [D loss: 0.392601, acc: 78.12%] [G loss: 9.614336]\n",
      "147 [D loss: 0.318134, acc: 92.19%] [G loss: 11.277413]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148 [D loss: 0.192493, acc: 90.62%] [G loss: 10.716168]\n",
      "149 [D loss: 0.335824, acc: 84.38%] [G loss: 8.934477]\n",
      "150 [D loss: 0.347498, acc: 79.69%] [G loss: 9.117983]\n",
      "151 [D loss: 0.400675, acc: 84.38%] [G loss: 10.401075]\n",
      "152 [D loss: 0.547935, acc: 82.81%] [G loss: 10.115701]\n",
      "153 [D loss: 0.266802, acc: 79.69%] [G loss: 8.113564]\n",
      "154 [D loss: 0.147362, acc: 96.88%] [G loss: 7.416132]\n",
      "155 [D loss: 0.517420, acc: 75.00%] [G loss: 11.461792]\n",
      "156 [D loss: 0.139759, acc: 95.31%] [G loss: 8.939787]\n",
      "157 [D loss: 0.143692, acc: 96.88%] [G loss: 7.900506]\n",
      "158 [D loss: 0.407594, acc: 81.25%] [G loss: 9.772662]\n",
      "159 [D loss: 0.255236, acc: 90.62%] [G loss: 10.083895]\n",
      "160 [D loss: 0.134348, acc: 93.75%] [G loss: 7.129897]\n",
      "161 [D loss: 0.795474, acc: 64.06%] [G loss: 13.799948]\n",
      "162 [D loss: 0.452014, acc: 75.00%] [G loss: 9.877586]\n",
      "163 [D loss: 0.173852, acc: 92.19%] [G loss: 6.716862]\n",
      "164 [D loss: 0.893863, acc: 60.94%] [G loss: 13.561461]\n",
      "165 [D loss: 0.393881, acc: 82.81%] [G loss: 10.650093]\n",
      "166 [D loss: 0.251793, acc: 87.50%] [G loss: 7.019027]\n",
      "167 [D loss: 0.204362, acc: 90.62%] [G loss: 8.251581]\n",
      "168 [D loss: 0.312616, acc: 84.38%] [G loss: 11.368608]\n",
      "169 [D loss: 0.330494, acc: 84.38%] [G loss: 9.386288]\n",
      "170 [D loss: 0.212989, acc: 90.62%] [G loss: 8.613989]\n",
      "171 [D loss: 0.087303, acc: 98.44%] [G loss: 8.248060]\n",
      "172 [D loss: 0.367758, acc: 79.69%] [G loss: 8.613275]\n",
      "173 [D loss: 0.190111, acc: 92.19%] [G loss: 7.803091]\n",
      "174 [D loss: 0.203529, acc: 92.19%] [G loss: 7.773525]\n",
      "175 [D loss: 0.527220, acc: 75.00%] [G loss: 9.683983]\n",
      "176 [D loss: 0.131672, acc: 90.62%] [G loss: 7.442362]\n",
      "177 [D loss: 0.622962, acc: 73.44%] [G loss: 10.299928]\n",
      "178 [D loss: 0.213551, acc: 90.62%] [G loss: 7.590581]\n",
      "179 [D loss: 0.195618, acc: 92.19%] [G loss: 7.144861]\n",
      "180 [D loss: 0.263096, acc: 89.06%] [G loss: 8.447338]\n",
      "181 [D loss: 0.161603, acc: 95.31%] [G loss: 6.958690]\n",
      "182 [D loss: 0.245253, acc: 90.62%] [G loss: 7.208441]\n",
      "183 [D loss: 0.196552, acc: 90.62%] [G loss: 8.576478]\n",
      "184 [D loss: 0.346254, acc: 85.94%] [G loss: 9.392225]\n",
      "185 [D loss: 0.153097, acc: 92.19%] [G loss: 7.656558]\n",
      "186 [D loss: 0.133419, acc: 93.75%] [G loss: 6.401545]\n",
      "187 [D loss: 0.458762, acc: 76.56%] [G loss: 8.084414]\n",
      "188 [D loss: 0.170447, acc: 90.62%] [G loss: 7.680923]\n",
      "189 [D loss: 0.257158, acc: 87.50%] [G loss: 5.912830]\n",
      "190 [D loss: 0.220886, acc: 90.62%] [G loss: 6.896755]\n",
      "191 [D loss: 0.646999, acc: 79.69%] [G loss: 8.867694]\n",
      "192 [D loss: 0.222428, acc: 92.19%] [G loss: 5.878359]\n",
      "193 [D loss: 0.454770, acc: 76.56%] [G loss: 8.704510]\n",
      "194 [D loss: 0.188602, acc: 89.06%] [G loss: 6.523884]\n",
      "195 [D loss: 0.268205, acc: 93.75%] [G loss: 6.989804]\n",
      "196 [D loss: 0.497363, acc: 73.44%] [G loss: 8.572032]\n",
      "197 [D loss: 0.247237, acc: 92.19%] [G loss: 7.493543]\n",
      "198 [D loss: 0.238526, acc: 87.50%] [G loss: 6.246345]\n",
      "199 [D loss: 0.305791, acc: 87.50%] [G loss: 9.090533]\n",
      "200 [D loss: 0.533683, acc: 73.44%] [G loss: 6.112353]\n",
      "201 [D loss: 0.232321, acc: 90.62%] [G loss: 5.681418]\n",
      "202 [D loss: 0.607843, acc: 81.25%] [G loss: 9.617590]\n",
      "203 [D loss: 0.240538, acc: 89.06%] [G loss: 7.684880]\n",
      "204 [D loss: 0.193434, acc: 90.62%] [G loss: 5.594592]\n",
      "205 [D loss: 0.876139, acc: 59.38%] [G loss: 9.720694]\n",
      "206 [D loss: 0.299319, acc: 84.38%] [G loss: 7.787537]\n",
      "207 [D loss: 0.245547, acc: 89.06%] [G loss: 5.692403]\n",
      "208 [D loss: 0.277375, acc: 89.06%] [G loss: 7.446507]\n",
      "209 [D loss: 0.288417, acc: 81.25%] [G loss: 6.763962]\n",
      "210 [D loss: 0.125684, acc: 98.44%] [G loss: 5.546705]\n",
      "211 [D loss: 0.299360, acc: 81.25%] [G loss: 6.410317]\n",
      "212 [D loss: 0.512380, acc: 75.00%] [G loss: 7.148014]\n",
      "213 [D loss: 0.213876, acc: 95.31%] [G loss: 5.427860]\n",
      "214 [D loss: 0.337287, acc: 84.38%] [G loss: 7.019974]\n",
      "215 [D loss: 0.211297, acc: 92.19%] [G loss: 5.990750]\n",
      "216 [D loss: 0.132603, acc: 96.88%] [G loss: 5.777753]\n",
      "217 [D loss: 0.473206, acc: 79.69%] [G loss: 7.272632]\n",
      "218 [D loss: 0.362048, acc: 82.81%] [G loss: 5.920479]\n",
      "219 [D loss: 0.176673, acc: 93.75%] [G loss: 5.573359]\n",
      "220 [D loss: 0.389819, acc: 82.81%] [G loss: 6.651592]\n",
      "221 [D loss: 0.475153, acc: 76.56%] [G loss: 6.177925]\n",
      "222 [D loss: 0.451276, acc: 79.69%] [G loss: 6.429581]\n",
      "223 [D loss: 0.492674, acc: 73.44%] [G loss: 6.049444]\n",
      "224 [D loss: 0.233658, acc: 92.19%] [G loss: 4.710912]\n",
      "225 [D loss: 0.823724, acc: 60.94%] [G loss: 6.560664]\n",
      "226 [D loss: 0.223641, acc: 93.75%] [G loss: 5.246829]\n",
      "227 [D loss: 0.719346, acc: 56.25%] [G loss: 8.472485]\n",
      "228 [D loss: 0.270249, acc: 81.25%] [G loss: 6.153099]\n",
      "229 [D loss: 0.558407, acc: 68.75%] [G loss: 7.206966]\n",
      "230 [D loss: 0.280525, acc: 89.06%] [G loss: 6.202509]\n",
      "231 [D loss: 0.507247, acc: 76.56%] [G loss: 6.365226]\n",
      "232 [D loss: 0.373727, acc: 79.69%] [G loss: 5.064276]\n",
      "233 [D loss: 0.272615, acc: 85.94%] [G loss: 6.556263]\n",
      "234 [D loss: 0.348476, acc: 78.12%] [G loss: 5.739587]\n",
      "235 [D loss: 0.273032, acc: 90.62%] [G loss: 5.838003]\n",
      "236 [D loss: 0.276307, acc: 89.06%] [G loss: 5.628574]\n",
      "237 [D loss: 0.202848, acc: 90.62%] [G loss: 5.529809]\n",
      "238 [D loss: 0.153638, acc: 98.44%] [G loss: 5.306683]\n",
      "239 [D loss: 0.468946, acc: 76.56%] [G loss: 5.383147]\n",
      "240 [D loss: 0.300489, acc: 89.06%] [G loss: 5.568239]\n",
      "241 [D loss: 0.150909, acc: 96.88%] [G loss: 5.567753]\n",
      "242 [D loss: 0.514999, acc: 70.31%] [G loss: 6.940009]\n",
      "243 [D loss: 0.246253, acc: 90.62%] [G loss: 5.068728]\n",
      "244 [D loss: 0.427957, acc: 82.81%] [G loss: 6.459219]\n",
      "245 [D loss: 0.267149, acc: 89.06%] [G loss: 4.979908]\n",
      "246 [D loss: 0.231595, acc: 92.19%] [G loss: 4.685530]\n",
      "247 [D loss: 1.017348, acc: 54.69%] [G loss: 8.297363]\n",
      "248 [D loss: 0.359988, acc: 78.12%] [G loss: 6.031828]\n",
      "249 [D loss: 0.328173, acc: 89.06%] [G loss: 5.281974]\n",
      "250 [D loss: 0.455723, acc: 84.38%] [G loss: 5.030743]\n",
      "251 [D loss: 0.566361, acc: 65.62%] [G loss: 6.776219]\n",
      "252 [D loss: 0.166774, acc: 96.88%] [G loss: 5.734794]\n",
      "253 [D loss: 0.459234, acc: 81.25%] [G loss: 5.277222]\n",
      "254 [D loss: 0.187266, acc: 93.75%] [G loss: 5.474733]\n",
      "255 [D loss: 0.238864, acc: 95.31%] [G loss: 5.070107]\n",
      "256 [D loss: 0.355568, acc: 89.06%] [G loss: 5.287704]\n",
      "257 [D loss: 0.204166, acc: 92.19%] [G loss: 5.543039]\n",
      "258 [D loss: 0.250856, acc: 93.75%] [G loss: 4.466712]\n",
      "259 [D loss: 0.357369, acc: 89.06%] [G loss: 5.797194]\n",
      "260 [D loss: 0.296715, acc: 87.50%] [G loss: 5.593492]\n",
      "261 [D loss: 0.203298, acc: 92.19%] [G loss: 5.797652]\n",
      "262 [D loss: 0.271971, acc: 89.06%] [G loss: 5.422319]\n",
      "263 [D loss: 0.326747, acc: 89.06%] [G loss: 4.996693]\n",
      "264 [D loss: 0.325257, acc: 82.81%] [G loss: 5.130174]\n",
      "265 [D loss: 0.288867, acc: 87.50%] [G loss: 4.835329]\n",
      "266 [D loss: 0.430462, acc: 78.12%] [G loss: 6.332243]\n",
      "267 [D loss: 0.192381, acc: 93.75%] [G loss: 4.489937]\n",
      "268 [D loss: 0.461721, acc: 78.12%] [G loss: 5.320770]\n",
      "269 [D loss: 0.247154, acc: 92.19%] [G loss: 5.502927]\n",
      "270 [D loss: 0.138737, acc: 98.44%] [G loss: 4.793842]\n",
      "271 [D loss: 0.494164, acc: 73.44%] [G loss: 5.630387]\n",
      "272 [D loss: 0.302705, acc: 85.94%] [G loss: 5.347654]\n",
      "273 [D loss: 0.509824, acc: 75.00%] [G loss: 5.204143]\n",
      "274 [D loss: 0.301267, acc: 89.06%] [G loss: 5.804125]\n",
      "275 [D loss: 0.424766, acc: 79.69%] [G loss: 5.304081]\n",
      "276 [D loss: 0.376248, acc: 78.12%] [G loss: 4.907053]\n",
      "277 [D loss: 0.337298, acc: 89.06%] [G loss: 5.591874]\n",
      "278 [D loss: 0.190662, acc: 95.31%] [G loss: 5.232522]\n",
      "279 [D loss: 0.463084, acc: 75.00%] [G loss: 4.809528]\n",
      "280 [D loss: 0.293251, acc: 84.38%] [G loss: 5.832361]\n",
      "281 [D loss: 0.417109, acc: 81.25%] [G loss: 5.508187]\n",
      "282 [D loss: 0.418154, acc: 81.25%] [G loss: 5.372011]\n",
      "283 [D loss: 0.237891, acc: 92.19%] [G loss: 5.112346]\n",
      "284 [D loss: 0.654682, acc: 65.62%] [G loss: 6.231267]\n",
      "285 [D loss: 0.121239, acc: 100.00%] [G loss: 5.486441]\n",
      "286 [D loss: 0.445137, acc: 73.44%] [G loss: 5.798561]\n",
      "287 [D loss: 0.248420, acc: 90.62%] [G loss: 5.764985]\n",
      "288 [D loss: 0.435484, acc: 81.25%] [G loss: 6.011545]\n",
      "289 [D loss: 0.168719, acc: 96.88%] [G loss: 4.786128]\n",
      "290 [D loss: 0.450764, acc: 79.69%] [G loss: 5.347807]\n",
      "291 [D loss: 0.276852, acc: 90.62%] [G loss: 5.063235]\n",
      "292 [D loss: 0.352171, acc: 89.06%] [G loss: 5.477726]\n",
      "293 [D loss: 0.334687, acc: 82.81%] [G loss: 5.010237]\n",
      "294 [D loss: 0.474819, acc: 73.44%] [G loss: 5.417974]\n",
      "295 [D loss: 0.239736, acc: 92.19%] [G loss: 4.355790]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 [D loss: 0.541749, acc: 70.31%] [G loss: 6.204755]\n",
      "297 [D loss: 0.231997, acc: 87.50%] [G loss: 4.724100]\n",
      "298 [D loss: 0.442047, acc: 71.88%] [G loss: 6.234987]\n",
      "299 [D loss: 0.212319, acc: 96.88%] [G loss: 4.316307]\n",
      "300 [D loss: 0.424617, acc: 79.69%] [G loss: 6.014455]\n",
      "301 [D loss: 0.306377, acc: 85.94%] [G loss: 5.439369]\n",
      "302 [D loss: 0.386179, acc: 84.38%] [G loss: 5.444127]\n",
      "303 [D loss: 0.491338, acc: 73.44%] [G loss: 5.815137]\n",
      "304 [D loss: 0.218338, acc: 92.19%] [G loss: 4.782976]\n",
      "305 [D loss: 0.811746, acc: 48.44%] [G loss: 5.988901]\n",
      "306 [D loss: 0.451243, acc: 73.44%] [G loss: 4.309905]\n",
      "307 [D loss: 0.279376, acc: 84.38%] [G loss: 4.237712]\n",
      "308 [D loss: 0.497834, acc: 70.31%] [G loss: 5.571743]\n",
      "309 [D loss: 0.283938, acc: 89.06%] [G loss: 6.079249]\n",
      "310 [D loss: 0.190767, acc: 93.75%] [G loss: 4.653200]\n",
      "311 [D loss: 0.280417, acc: 85.94%] [G loss: 4.972998]\n",
      "312 [D loss: 0.347642, acc: 87.50%] [G loss: 4.327041]\n",
      "313 [D loss: 0.227946, acc: 93.75%] [G loss: 4.803055]\n",
      "314 [D loss: 0.316207, acc: 87.50%] [G loss: 6.110671]\n",
      "315 [D loss: 0.228468, acc: 92.19%] [G loss: 5.687135]\n",
      "316 [D loss: 0.183497, acc: 95.31%] [G loss: 5.205120]\n",
      "317 [D loss: 0.294677, acc: 89.06%] [G loss: 5.102903]\n",
      "318 [D loss: 0.239399, acc: 89.06%] [G loss: 4.875107]\n",
      "319 [D loss: 0.317248, acc: 82.81%] [G loss: 5.344274]\n",
      "320 [D loss: 0.312163, acc: 87.50%] [G loss: 5.095485]\n",
      "321 [D loss: 0.724351, acc: 62.50%] [G loss: 4.685511]\n",
      "322 [D loss: 0.285367, acc: 92.19%] [G loss: 4.887185]\n",
      "323 [D loss: 0.202893, acc: 93.75%] [G loss: 4.511666]\n",
      "324 [D loss: 0.340256, acc: 82.81%] [G loss: 4.874045]\n",
      "325 [D loss: 0.296585, acc: 87.50%] [G loss: 4.793186]\n",
      "326 [D loss: 0.292301, acc: 90.62%] [G loss: 4.396924]\n",
      "327 [D loss: 0.250201, acc: 93.75%] [G loss: 5.154839]\n",
      "328 [D loss: 0.409799, acc: 78.12%] [G loss: 5.924462]\n",
      "329 [D loss: 0.140891, acc: 98.44%] [G loss: 4.543662]\n",
      "330 [D loss: 0.451667, acc: 76.56%] [G loss: 6.039727]\n",
      "331 [D loss: 0.190822, acc: 95.31%] [G loss: 5.070210]\n",
      "332 [D loss: 0.508491, acc: 76.56%] [G loss: 5.580395]\n",
      "333 [D loss: 0.250084, acc: 92.19%] [G loss: 4.564064]\n",
      "334 [D loss: 0.337449, acc: 87.50%] [G loss: 5.315755]\n",
      "335 [D loss: 0.367371, acc: 85.94%] [G loss: 4.681600]\n",
      "336 [D loss: 0.275719, acc: 93.75%] [G loss: 5.136520]\n",
      "337 [D loss: 0.427477, acc: 79.69%] [G loss: 5.407282]\n",
      "338 [D loss: 0.196059, acc: 92.19%] [G loss: 5.401384]\n",
      "339 [D loss: 0.214043, acc: 92.19%] [G loss: 4.356627]\n",
      "340 [D loss: 0.555389, acc: 73.44%] [G loss: 6.500819]\n",
      "341 [D loss: 0.313753, acc: 85.94%] [G loss: 4.348800]\n",
      "342 [D loss: 0.635266, acc: 65.62%] [G loss: 4.922196]\n",
      "343 [D loss: 0.276573, acc: 93.75%] [G loss: 4.307042]\n",
      "344 [D loss: 0.328802, acc: 84.38%] [G loss: 5.263166]\n",
      "345 [D loss: 0.257195, acc: 93.75%] [G loss: 4.637923]\n",
      "346 [D loss: 0.269159, acc: 89.06%] [G loss: 4.676557]\n",
      "347 [D loss: 0.287148, acc: 92.19%] [G loss: 4.347125]\n",
      "348 [D loss: 0.447150, acc: 76.56%] [G loss: 5.170703]\n",
      "349 [D loss: 0.308665, acc: 87.50%] [G loss: 4.354690]\n",
      "350 [D loss: 0.595508, acc: 71.88%] [G loss: 4.679543]\n",
      "351 [D loss: 0.158396, acc: 95.31%] [G loss: 4.766473]\n",
      "352 [D loss: 0.767723, acc: 51.56%] [G loss: 6.553728]\n",
      "353 [D loss: 0.187794, acc: 92.19%] [G loss: 4.415759]\n",
      "354 [D loss: 0.420793, acc: 78.12%] [G loss: 4.930081]\n",
      "355 [D loss: 0.203213, acc: 93.75%] [G loss: 4.833702]\n",
      "356 [D loss: 0.665248, acc: 62.50%] [G loss: 4.623984]\n",
      "357 [D loss: 0.252922, acc: 93.75%] [G loss: 4.292264]\n",
      "358 [D loss: 0.341217, acc: 87.50%] [G loss: 4.780173]\n",
      "359 [D loss: 0.294129, acc: 89.06%] [G loss: 4.810354]\n",
      "360 [D loss: 0.352736, acc: 85.94%] [G loss: 4.585761]\n",
      "361 [D loss: 0.418935, acc: 78.12%] [G loss: 4.602148]\n",
      "362 [D loss: 0.405586, acc: 81.25%] [G loss: 4.349812]\n",
      "363 [D loss: 0.231766, acc: 95.31%] [G loss: 4.466345]\n",
      "364 [D loss: 0.419406, acc: 82.81%] [G loss: 5.661412]\n",
      "365 [D loss: 0.502933, acc: 67.19%] [G loss: 4.920559]\n",
      "366 [D loss: 0.303414, acc: 90.62%] [G loss: 4.323008]\n",
      "367 [D loss: 0.292121, acc: 92.19%] [G loss: 5.296160]\n",
      "368 [D loss: 0.269111, acc: 89.06%] [G loss: 4.881079]\n",
      "369 [D loss: 0.268307, acc: 95.31%] [G loss: 4.676459]\n",
      "370 [D loss: 0.370681, acc: 81.25%] [G loss: 4.503631]\n",
      "371 [D loss: 0.339914, acc: 87.50%] [G loss: 5.440238]\n",
      "372 [D loss: 0.324501, acc: 89.06%] [G loss: 4.587314]\n",
      "373 [D loss: 0.300694, acc: 90.62%] [G loss: 4.757143]\n",
      "374 [D loss: 0.391501, acc: 82.81%] [G loss: 4.667600]\n",
      "375 [D loss: 0.218519, acc: 93.75%] [G loss: 4.096844]\n",
      "376 [D loss: 0.533665, acc: 68.75%] [G loss: 4.920888]\n",
      "377 [D loss: 0.342088, acc: 85.94%] [G loss: 4.339546]\n",
      "378 [D loss: 0.396731, acc: 78.12%] [G loss: 5.404801]\n",
      "379 [D loss: 0.340963, acc: 82.81%] [G loss: 4.074161]\n",
      "380 [D loss: 0.417098, acc: 82.81%] [G loss: 4.762108]\n",
      "381 [D loss: 0.338894, acc: 93.75%] [G loss: 5.083388]\n",
      "382 [D loss: 0.340046, acc: 87.50%] [G loss: 4.352923]\n",
      "383 [D loss: 0.310570, acc: 87.50%] [G loss: 5.312482]\n",
      "384 [D loss: 0.257894, acc: 92.19%] [G loss: 4.550050]\n",
      "385 [D loss: 0.360237, acc: 82.81%] [G loss: 5.498880]\n",
      "386 [D loss: 0.273515, acc: 92.19%] [G loss: 4.674519]\n",
      "387 [D loss: 0.408072, acc: 85.94%] [G loss: 4.572326]\n",
      "388 [D loss: 0.289453, acc: 87.50%] [G loss: 5.188270]\n",
      "389 [D loss: 0.280071, acc: 92.19%] [G loss: 5.315428]\n",
      "390 [D loss: 0.354775, acc: 85.94%] [G loss: 5.139144]\n",
      "391 [D loss: 0.262651, acc: 92.19%] [G loss: 4.557137]\n",
      "392 [D loss: 0.424872, acc: 82.81%] [G loss: 4.975739]\n",
      "393 [D loss: 0.247958, acc: 90.62%] [G loss: 5.028363]\n",
      "394 [D loss: 0.376930, acc: 84.38%] [G loss: 4.818051]\n",
      "395 [D loss: 0.245765, acc: 90.62%] [G loss: 4.893225]\n",
      "396 [D loss: 0.293723, acc: 85.94%] [G loss: 5.300123]\n",
      "397 [D loss: 0.291348, acc: 89.06%] [G loss: 4.691448]\n",
      "398 [D loss: 0.205251, acc: 100.00%] [G loss: 4.708896]\n",
      "399 [D loss: 0.442463, acc: 81.25%] [G loss: 5.649610]\n",
      "400 [D loss: 0.367729, acc: 81.25%] [G loss: 5.064534]\n",
      "401 [D loss: 0.336735, acc: 85.94%] [G loss: 4.889850]\n",
      "402 [D loss: 0.243476, acc: 95.31%] [G loss: 5.311263]\n",
      "403 [D loss: 0.303632, acc: 84.38%] [G loss: 5.330416]\n",
      "404 [D loss: 0.415871, acc: 82.81%] [G loss: 4.745676]\n",
      "405 [D loss: 0.282010, acc: 90.62%] [G loss: 3.806024]\n",
      "406 [D loss: 0.362208, acc: 79.69%] [G loss: 5.621626]\n",
      "407 [D loss: 0.357966, acc: 89.06%] [G loss: 4.582274]\n",
      "408 [D loss: 0.361838, acc: 84.38%] [G loss: 4.679811]\n",
      "409 [D loss: 0.255970, acc: 92.19%] [G loss: 4.886321]\n",
      "410 [D loss: 0.359288, acc: 87.50%] [G loss: 5.085092]\n",
      "411 [D loss: 0.274817, acc: 89.06%] [G loss: 4.927701]\n",
      "412 [D loss: 0.363642, acc: 87.50%] [G loss: 5.070443]\n",
      "413 [D loss: 0.388699, acc: 79.69%] [G loss: 5.332702]\n",
      "414 [D loss: 0.362702, acc: 81.25%] [G loss: 5.207873]\n",
      "415 [D loss: 0.264199, acc: 92.19%] [G loss: 5.133165]\n",
      "416 [D loss: 0.463170, acc: 71.88%] [G loss: 5.035549]\n",
      "417 [D loss: 0.213486, acc: 92.19%] [G loss: 4.474153]\n",
      "418 [D loss: 0.601162, acc: 68.75%] [G loss: 5.422649]\n",
      "419 [D loss: 0.183333, acc: 98.44%] [G loss: 4.911469]\n",
      "420 [D loss: 0.333101, acc: 84.38%] [G loss: 5.292139]\n",
      "421 [D loss: 0.274842, acc: 92.19%] [G loss: 4.533302]\n",
      "422 [D loss: 0.463082, acc: 79.69%] [G loss: 4.981266]\n",
      "423 [D loss: 0.225668, acc: 96.88%] [G loss: 4.315116]\n",
      "424 [D loss: 0.475634, acc: 76.56%] [G loss: 4.673900]\n",
      "425 [D loss: 0.294499, acc: 84.38%] [G loss: 5.134404]\n",
      "426 [D loss: 0.291172, acc: 90.62%] [G loss: 4.907540]\n",
      "427 [D loss: 0.268233, acc: 92.19%] [G loss: 5.408792]\n",
      "428 [D loss: 0.534287, acc: 73.44%] [G loss: 4.947457]\n",
      "429 [D loss: 0.323597, acc: 87.50%] [G loss: 4.463066]\n",
      "430 [D loss: 0.377735, acc: 82.81%] [G loss: 5.090845]\n",
      "431 [D loss: 0.304145, acc: 87.50%] [G loss: 4.949233]\n",
      "432 [D loss: 0.579919, acc: 68.75%] [G loss: 6.053296]\n",
      "433 [D loss: 0.231317, acc: 93.75%] [G loss: 4.449539]\n",
      "434 [D loss: 0.425221, acc: 82.81%] [G loss: 6.038424]\n",
      "435 [D loss: 0.252801, acc: 89.06%] [G loss: 4.535143]\n",
      "436 [D loss: 0.471846, acc: 79.69%] [G loss: 5.903247]\n",
      "437 [D loss: 0.331779, acc: 85.94%] [G loss: 4.425870]\n",
      "438 [D loss: 0.453546, acc: 79.69%] [G loss: 5.892734]\n",
      "439 [D loss: 0.278816, acc: 87.50%] [G loss: 4.608983]\n",
      "440 [D loss: 0.342466, acc: 87.50%] [G loss: 4.731088]\n",
      "441 [D loss: 0.354510, acc: 85.94%] [G loss: 4.398427]\n",
      "442 [D loss: 0.260649, acc: 90.62%] [G loss: 4.960119]\n",
      "443 [D loss: 0.480725, acc: 81.25%] [G loss: 5.246041]\n",
      "444 [D loss: 0.332562, acc: 87.50%] [G loss: 4.771300]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445 [D loss: 0.334358, acc: 90.62%] [G loss: 4.970905]\n",
      "446 [D loss: 0.668104, acc: 60.94%] [G loss: 4.842405]\n",
      "447 [D loss: 0.284389, acc: 87.50%] [G loss: 4.486209]\n",
      "448 [D loss: 0.236400, acc: 93.75%] [G loss: 4.531005]\n",
      "449 [D loss: 0.618416, acc: 71.88%] [G loss: 4.921111]\n",
      "450 [D loss: 0.292003, acc: 93.75%] [G loss: 4.437847]\n",
      "451 [D loss: 0.456919, acc: 76.56%] [G loss: 4.689266]\n",
      "452 [D loss: 0.257107, acc: 92.19%] [G loss: 4.580830]\n",
      "453 [D loss: 0.304183, acc: 92.19%] [G loss: 4.769463]\n",
      "454 [D loss: 0.293006, acc: 92.19%] [G loss: 4.421643]\n",
      "455 [D loss: 0.461784, acc: 78.12%] [G loss: 4.234420]\n",
      "456 [D loss: 0.142510, acc: 100.00%] [G loss: 4.290384]\n",
      "457 [D loss: 0.470489, acc: 75.00%] [G loss: 5.108297]\n",
      "458 [D loss: 0.224818, acc: 96.88%] [G loss: 3.720759]\n",
      "459 [D loss: 0.751809, acc: 64.06%] [G loss: 4.101781]\n",
      "460 [D loss: 0.167741, acc: 95.31%] [G loss: 4.347928]\n",
      "461 [D loss: 0.408954, acc: 84.38%] [G loss: 5.134630]\n",
      "462 [D loss: 0.352484, acc: 84.38%] [G loss: 4.617553]\n",
      "463 [D loss: 0.310483, acc: 89.06%] [G loss: 4.404832]\n",
      "464 [D loss: 0.354256, acc: 81.25%] [G loss: 4.611936]\n",
      "465 [D loss: 0.321064, acc: 89.06%] [G loss: 4.770113]\n",
      "466 [D loss: 0.287866, acc: 93.75%] [G loss: 4.818310]\n",
      "467 [D loss: 0.358836, acc: 82.81%] [G loss: 4.340114]\n",
      "468 [D loss: 0.368204, acc: 81.25%] [G loss: 5.073511]\n",
      "469 [D loss: 0.303726, acc: 87.50%] [G loss: 4.891345]\n",
      "470 [D loss: 0.330022, acc: 84.38%] [G loss: 4.730756]\n",
      "471 [D loss: 0.513171, acc: 76.56%] [G loss: 4.219704]\n",
      "472 [D loss: 0.269568, acc: 90.62%] [G loss: 4.596179]\n",
      "473 [D loss: 0.288396, acc: 90.62%] [G loss: 4.916891]\n",
      "474 [D loss: 0.387687, acc: 84.38%] [G loss: 5.140061]\n",
      "475 [D loss: 0.399032, acc: 85.94%] [G loss: 5.415308]\n",
      "476 [D loss: 0.349581, acc: 81.25%] [G loss: 5.150934]\n",
      "477 [D loss: 0.348476, acc: 84.38%] [G loss: 4.604671]\n",
      "478 [D loss: 0.386079, acc: 82.81%] [G loss: 4.855443]\n",
      "479 [D loss: 0.433943, acc: 79.69%] [G loss: 5.150975]\n",
      "480 [D loss: 0.320684, acc: 85.94%] [G loss: 4.268455]\n",
      "481 [D loss: 0.568824, acc: 71.88%] [G loss: 4.998442]\n",
      "482 [D loss: 0.379294, acc: 85.94%] [G loss: 4.922046]\n",
      "483 [D loss: 0.378172, acc: 84.38%] [G loss: 4.524938]\n",
      "484 [D loss: 0.245334, acc: 92.19%] [G loss: 4.586943]\n",
      "485 [D loss: 0.526704, acc: 70.31%] [G loss: 4.798398]\n",
      "486 [D loss: 0.347055, acc: 84.38%] [G loss: 4.589395]\n",
      "487 [D loss: 0.317264, acc: 84.38%] [G loss: 4.540242]\n",
      "488 [D loss: 0.273266, acc: 92.19%] [G loss: 4.695613]\n",
      "489 [D loss: 0.431151, acc: 76.56%] [G loss: 4.548620]\n",
      "490 [D loss: 0.363207, acc: 84.38%] [G loss: 4.143239]\n",
      "491 [D loss: 0.365468, acc: 87.50%] [G loss: 4.974551]\n",
      "492 [D loss: 0.341612, acc: 85.94%] [G loss: 4.331718]\n",
      "493 [D loss: 0.680033, acc: 65.62%] [G loss: 5.553291]\n",
      "494 [D loss: 0.190995, acc: 95.31%] [G loss: 4.595575]\n",
      "495 [D loss: 0.304934, acc: 85.94%] [G loss: 3.864104]\n",
      "496 [D loss: 0.512953, acc: 76.56%] [G loss: 4.579229]\n",
      "497 [D loss: 0.560143, acc: 73.44%] [G loss: 4.594096]\n",
      "498 [D loss: 0.312986, acc: 89.06%] [G loss: 3.676957]\n",
      "499 [D loss: 0.274062, acc: 92.19%] [G loss: 3.976607]\n",
      "500 [D loss: 0.390232, acc: 78.12%] [G loss: 4.027295]\n",
      "501 [D loss: 0.266916, acc: 92.19%] [G loss: 4.655145]\n",
      "502 [D loss: 0.546903, acc: 68.75%] [G loss: 4.381651]\n",
      "503 [D loss: 0.470461, acc: 82.81%] [G loss: 3.708457]\n",
      "504 [D loss: 0.525668, acc: 67.19%] [G loss: 4.095740]\n",
      "505 [D loss: 0.298349, acc: 89.06%] [G loss: 4.041022]\n",
      "506 [D loss: 0.379607, acc: 82.81%] [G loss: 4.311034]\n",
      "507 [D loss: 0.353713, acc: 85.94%] [G loss: 4.322783]\n",
      "508 [D loss: 0.399380, acc: 81.25%] [G loss: 3.576376]\n",
      "509 [D loss: 0.315727, acc: 87.50%] [G loss: 3.807074]\n",
      "510 [D loss: 0.333460, acc: 89.06%] [G loss: 4.048711]\n",
      "511 [D loss: 0.478966, acc: 81.25%] [G loss: 3.916827]\n",
      "512 [D loss: 0.443206, acc: 84.38%] [G loss: 4.556697]\n",
      "513 [D loss: 0.332462, acc: 84.38%] [G loss: 3.735121]\n",
      "514 [D loss: 0.695345, acc: 59.38%] [G loss: 4.212462]\n",
      "515 [D loss: 0.611740, acc: 64.06%] [G loss: 4.981406]\n",
      "516 [D loss: 0.288599, acc: 92.19%] [G loss: 4.502306]\n",
      "517 [D loss: 0.297129, acc: 89.06%] [G loss: 4.308028]\n",
      "518 [D loss: 0.348105, acc: 85.94%] [G loss: 3.814339]\n",
      "519 [D loss: 0.461015, acc: 79.69%] [G loss: 4.264165]\n",
      "520 [D loss: 0.343493, acc: 87.50%] [G loss: 4.106626]\n",
      "521 [D loss: 0.309818, acc: 93.75%] [G loss: 4.290465]\n",
      "522 [D loss: 0.426022, acc: 76.56%] [G loss: 4.300514]\n",
      "523 [D loss: 0.360053, acc: 85.94%] [G loss: 3.629744]\n",
      "524 [D loss: 0.462895, acc: 78.12%] [G loss: 3.831893]\n",
      "525 [D loss: 0.348097, acc: 87.50%] [G loss: 4.236800]\n",
      "526 [D loss: 0.316521, acc: 82.81%] [G loss: 4.116126]\n",
      "527 [D loss: 0.523051, acc: 75.00%] [G loss: 4.343662]\n",
      "528 [D loss: 0.329683, acc: 90.62%] [G loss: 4.372851]\n",
      "529 [D loss: 0.246782, acc: 95.31%] [G loss: 4.269160]\n",
      "530 [D loss: 0.343430, acc: 84.38%] [G loss: 4.690548]\n",
      "531 [D loss: 0.249880, acc: 95.31%] [G loss: 5.092809]\n",
      "532 [D loss: 0.297126, acc: 92.19%] [G loss: 4.285561]\n",
      "533 [D loss: 0.466064, acc: 79.69%] [G loss: 4.565200]\n",
      "534 [D loss: 0.361798, acc: 82.81%] [G loss: 4.374395]\n",
      "535 [D loss: 0.363467, acc: 85.94%] [G loss: 4.223675]\n",
      "536 [D loss: 0.373178, acc: 81.25%] [G loss: 4.381464]\n",
      "537 [D loss: 0.223637, acc: 96.88%] [G loss: 4.213737]\n",
      "538 [D loss: 0.468969, acc: 78.12%] [G loss: 3.963102]\n",
      "539 [D loss: 0.504888, acc: 76.56%] [G loss: 4.139072]\n",
      "540 [D loss: 0.378522, acc: 81.25%] [G loss: 4.557796]\n",
      "541 [D loss: 0.321809, acc: 87.50%] [G loss: 4.537283]\n",
      "542 [D loss: 0.443341, acc: 76.56%] [G loss: 4.221441]\n",
      "543 [D loss: 0.407313, acc: 78.12%] [G loss: 4.090792]\n",
      "544 [D loss: 0.402089, acc: 84.38%] [G loss: 4.215200]\n",
      "545 [D loss: 0.320009, acc: 87.50%] [G loss: 3.952685]\n",
      "546 [D loss: 0.471483, acc: 81.25%] [G loss: 4.500585]\n",
      "547 [D loss: 0.439797, acc: 78.12%] [G loss: 3.795306]\n",
      "548 [D loss: 0.463408, acc: 82.81%] [G loss: 4.419830]\n",
      "549 [D loss: 0.425497, acc: 81.25%] [G loss: 4.555739]\n",
      "550 [D loss: 0.403181, acc: 87.50%] [G loss: 4.494542]\n",
      "551 [D loss: 0.511217, acc: 71.88%] [G loss: 4.539959]\n",
      "552 [D loss: 0.236477, acc: 89.06%] [G loss: 4.760133]\n",
      "553 [D loss: 0.310539, acc: 89.06%] [G loss: 4.535366]\n",
      "554 [D loss: 0.575968, acc: 73.44%] [G loss: 5.048271]\n",
      "555 [D loss: 0.300892, acc: 89.06%] [G loss: 4.334158]\n",
      "556 [D loss: 0.591400, acc: 73.44%] [G loss: 4.380829]\n",
      "557 [D loss: 0.433060, acc: 81.25%] [G loss: 4.403501]\n",
      "558 [D loss: 0.525897, acc: 76.56%] [G loss: 3.785094]\n",
      "559 [D loss: 0.419415, acc: 85.94%] [G loss: 4.518581]\n",
      "560 [D loss: 0.297148, acc: 90.62%] [G loss: 4.267562]\n",
      "561 [D loss: 0.346282, acc: 92.19%] [G loss: 4.466688]\n",
      "562 [D loss: 0.442302, acc: 82.81%] [G loss: 3.743406]\n",
      "563 [D loss: 0.521614, acc: 75.00%] [G loss: 4.027635]\n",
      "564 [D loss: 0.338031, acc: 87.50%] [G loss: 3.962495]\n",
      "565 [D loss: 0.371685, acc: 84.38%] [G loss: 4.248939]\n",
      "566 [D loss: 0.445766, acc: 76.56%] [G loss: 4.490697]\n",
      "567 [D loss: 0.516361, acc: 70.31%] [G loss: 3.885754]\n",
      "568 [D loss: 0.419788, acc: 82.81%] [G loss: 4.403924]\n",
      "569 [D loss: 0.328711, acc: 87.50%] [G loss: 4.210970]\n",
      "570 [D loss: 0.431951, acc: 82.81%] [G loss: 3.983752]\n",
      "571 [D loss: 0.427923, acc: 81.25%] [G loss: 4.111195]\n",
      "572 [D loss: 0.477318, acc: 78.12%] [G loss: 3.382806]\n",
      "573 [D loss: 0.402678, acc: 79.69%] [G loss: 3.507297]\n",
      "574 [D loss: 0.383946, acc: 84.38%] [G loss: 4.214012]\n",
      "575 [D loss: 0.237750, acc: 96.88%] [G loss: 4.360505]\n",
      "576 [D loss: 0.467361, acc: 75.00%] [G loss: 4.584280]\n",
      "577 [D loss: 0.341765, acc: 82.81%] [G loss: 4.359650]\n",
      "578 [D loss: 0.286789, acc: 90.62%] [G loss: 4.163211]\n",
      "579 [D loss: 0.345629, acc: 85.94%] [G loss: 3.847304]\n",
      "580 [D loss: 0.495170, acc: 75.00%] [G loss: 3.880986]\n",
      "581 [D loss: 0.371029, acc: 87.50%] [G loss: 3.960740]\n",
      "582 [D loss: 0.463398, acc: 79.69%] [G loss: 4.179132]\n",
      "583 [D loss: 0.379218, acc: 89.06%] [G loss: 4.342580]\n",
      "584 [D loss: 0.414639, acc: 81.25%] [G loss: 4.931101]\n",
      "585 [D loss: 0.628216, acc: 60.94%] [G loss: 4.024881]\n",
      "586 [D loss: 0.339561, acc: 84.38%] [G loss: 3.918558]\n",
      "587 [D loss: 0.705406, acc: 65.62%] [G loss: 3.920245]\n",
      "588 [D loss: 0.344287, acc: 87.50%] [G loss: 4.160210]\n",
      "589 [D loss: 0.474315, acc: 75.00%] [G loss: 3.565779]\n",
      "590 [D loss: 0.357839, acc: 87.50%] [G loss: 4.132611]\n",
      "591 [D loss: 0.338518, acc: 87.50%] [G loss: 4.197970]\n",
      "592 [D loss: 0.419286, acc: 76.56%] [G loss: 4.032624]\n",
      "593 [D loss: 0.544877, acc: 75.00%] [G loss: 4.080054]\n",
      "594 [D loss: 0.397091, acc: 89.06%] [G loss: 3.943908]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595 [D loss: 0.503360, acc: 75.00%] [G loss: 4.142297]\n",
      "596 [D loss: 0.415188, acc: 85.94%] [G loss: 3.597692]\n",
      "597 [D loss: 0.376872, acc: 87.50%] [G loss: 3.771755]\n",
      "598 [D loss: 0.521124, acc: 73.44%] [G loss: 3.532707]\n",
      "599 [D loss: 0.412000, acc: 79.69%] [G loss: 4.125612]\n",
      "600 [D loss: 0.427861, acc: 79.69%] [G loss: 3.886374]\n",
      "601 [D loss: 0.291729, acc: 90.62%] [G loss: 3.689352]\n",
      "602 [D loss: 0.681352, acc: 68.75%] [G loss: 4.274494]\n",
      "603 [D loss: 0.312471, acc: 85.94%] [G loss: 4.506380]\n",
      "604 [D loss: 0.472190, acc: 76.56%] [G loss: 4.066715]\n",
      "605 [D loss: 0.356869, acc: 85.94%] [G loss: 3.851837]\n",
      "606 [D loss: 0.449578, acc: 75.00%] [G loss: 4.336165]\n",
      "607 [D loss: 0.371600, acc: 84.38%] [G loss: 4.078756]\n",
      "608 [D loss: 0.516854, acc: 75.00%] [G loss: 3.971596]\n",
      "609 [D loss: 0.398160, acc: 78.12%] [G loss: 3.719134]\n",
      "610 [D loss: 0.361815, acc: 82.81%] [G loss: 3.873234]\n",
      "611 [D loss: 0.595503, acc: 75.00%] [G loss: 4.006230]\n",
      "612 [D loss: 0.509030, acc: 75.00%] [G loss: 4.299947]\n",
      "613 [D loss: 0.390044, acc: 84.38%] [G loss: 3.493833]\n",
      "614 [D loss: 0.302880, acc: 90.62%] [G loss: 4.280192]\n",
      "615 [D loss: 0.403636, acc: 79.69%] [G loss: 4.226792]\n",
      "616 [D loss: 0.447768, acc: 76.56%] [G loss: 3.950435]\n",
      "617 [D loss: 0.404398, acc: 84.38%] [G loss: 3.558991]\n",
      "618 [D loss: 0.492402, acc: 81.25%] [G loss: 3.615276]\n",
      "619 [D loss: 0.430564, acc: 79.69%] [G loss: 3.507828]\n",
      "620 [D loss: 0.368235, acc: 81.25%] [G loss: 3.813261]\n",
      "621 [D loss: 0.576661, acc: 70.31%] [G loss: 3.624109]\n",
      "622 [D loss: 0.461891, acc: 76.56%] [G loss: 3.724069]\n",
      "623 [D loss: 0.429108, acc: 78.12%] [G loss: 3.738637]\n",
      "624 [D loss: 0.552751, acc: 73.44%] [G loss: 4.025091]\n",
      "625 [D loss: 0.581862, acc: 70.31%] [G loss: 3.722498]\n",
      "626 [D loss: 0.397584, acc: 82.81%] [G loss: 3.588661]\n",
      "627 [D loss: 0.525548, acc: 71.88%] [G loss: 3.869155]\n",
      "628 [D loss: 0.387140, acc: 84.38%] [G loss: 4.466708]\n",
      "629 [D loss: 0.344331, acc: 89.06%] [G loss: 4.061980]\n",
      "630 [D loss: 0.498345, acc: 78.12%] [G loss: 3.502425]\n",
      "631 [D loss: 0.507328, acc: 73.44%] [G loss: 3.716300]\n",
      "632 [D loss: 0.599709, acc: 67.19%] [G loss: 3.388063]\n",
      "633 [D loss: 0.736705, acc: 60.94%] [G loss: 3.455542]\n",
      "634 [D loss: 0.469314, acc: 85.94%] [G loss: 3.377571]\n",
      "635 [D loss: 0.411407, acc: 85.94%] [G loss: 3.389277]\n",
      "636 [D loss: 0.502189, acc: 79.69%] [G loss: 3.368413]\n",
      "637 [D loss: 0.396169, acc: 87.50%] [G loss: 3.746918]\n",
      "638 [D loss: 0.481719, acc: 76.56%] [G loss: 3.181523]\n",
      "639 [D loss: 0.386627, acc: 81.25%] [G loss: 3.644283]\n",
      "640 [D loss: 0.435938, acc: 85.94%] [G loss: 3.555648]\n",
      "641 [D loss: 0.481598, acc: 76.56%] [G loss: 3.428275]\n",
      "642 [D loss: 0.431879, acc: 75.00%] [G loss: 4.116936]\n",
      "643 [D loss: 0.610643, acc: 79.69%] [G loss: 3.358182]\n",
      "644 [D loss: 0.418159, acc: 76.56%] [G loss: 3.473352]\n",
      "645 [D loss: 0.522345, acc: 75.00%] [G loss: 3.745385]\n",
      "646 [D loss: 0.564839, acc: 67.19%] [G loss: 3.384000]\n",
      "647 [D loss: 0.414720, acc: 82.81%] [G loss: 3.706189]\n",
      "648 [D loss: 0.466896, acc: 78.12%] [G loss: 3.986095]\n",
      "649 [D loss: 0.460414, acc: 81.25%] [G loss: 3.690375]\n",
      "650 [D loss: 0.284192, acc: 95.31%] [G loss: 3.565735]\n",
      "651 [D loss: 0.504562, acc: 68.75%] [G loss: 3.536726]\n",
      "652 [D loss: 0.593900, acc: 62.50%] [G loss: 3.367421]\n",
      "653 [D loss: 0.427330, acc: 81.25%] [G loss: 3.712791]\n",
      "654 [D loss: 0.587716, acc: 71.88%] [G loss: 3.656340]\n",
      "655 [D loss: 0.662328, acc: 64.06%] [G loss: 3.297478]\n",
      "656 [D loss: 0.447617, acc: 79.69%] [G loss: 3.245258]\n",
      "657 [D loss: 0.544113, acc: 73.44%] [G loss: 3.489176]\n",
      "658 [D loss: 0.514536, acc: 75.00%] [G loss: 3.644713]\n",
      "659 [D loss: 0.417095, acc: 81.25%] [G loss: 3.336071]\n",
      "660 [D loss: 0.423618, acc: 82.81%] [G loss: 3.737928]\n",
      "661 [D loss: 0.386047, acc: 85.94%] [G loss: 3.302630]\n",
      "662 [D loss: 0.542550, acc: 68.75%] [G loss: 3.609056]\n",
      "663 [D loss: 0.393299, acc: 84.38%] [G loss: 3.370738]\n",
      "664 [D loss: 0.655145, acc: 65.62%] [G loss: 3.282572]\n",
      "665 [D loss: 0.416065, acc: 87.50%] [G loss: 3.614973]\n",
      "666 [D loss: 0.448498, acc: 81.25%] [G loss: 3.614905]\n",
      "667 [D loss: 0.434282, acc: 85.94%] [G loss: 3.413494]\n",
      "668 [D loss: 0.602213, acc: 67.19%] [G loss: 3.747689]\n",
      "669 [D loss: 0.506041, acc: 73.44%] [G loss: 3.045089]\n",
      "670 [D loss: 0.635462, acc: 65.62%] [G loss: 3.589957]\n",
      "671 [D loss: 0.534232, acc: 78.12%] [G loss: 3.656818]\n",
      "672 [D loss: 0.531219, acc: 76.56%] [G loss: 3.714303]\n",
      "673 [D loss: 0.457174, acc: 81.25%] [G loss: 3.628299]\n",
      "674 [D loss: 0.489004, acc: 78.12%] [G loss: 3.592053]\n",
      "675 [D loss: 0.447179, acc: 75.00%] [G loss: 3.628382]\n",
      "676 [D loss: 0.574080, acc: 78.12%] [G loss: 2.921379]\n",
      "677 [D loss: 0.526868, acc: 71.88%] [G loss: 3.906833]\n",
      "678 [D loss: 0.405954, acc: 84.38%] [G loss: 3.546604]\n",
      "679 [D loss: 0.364733, acc: 84.38%] [G loss: 3.300950]\n",
      "680 [D loss: 0.459949, acc: 75.00%] [G loss: 3.203329]\n",
      "681 [D loss: 0.554367, acc: 67.19%] [G loss: 3.634291]\n",
      "682 [D loss: 0.431694, acc: 85.94%] [G loss: 3.820609]\n",
      "683 [D loss: 0.628667, acc: 70.31%] [G loss: 3.305358]\n",
      "684 [D loss: 0.565658, acc: 68.75%] [G loss: 3.367615]\n",
      "685 [D loss: 0.560112, acc: 71.88%] [G loss: 3.262320]\n",
      "686 [D loss: 0.560052, acc: 71.88%] [G loss: 3.317510]\n",
      "687 [D loss: 0.384335, acc: 87.50%] [G loss: 3.481142]\n",
      "688 [D loss: 0.439445, acc: 79.69%] [G loss: 3.247284]\n",
      "689 [D loss: 0.400912, acc: 85.94%] [G loss: 3.682988]\n",
      "690 [D loss: 0.474869, acc: 81.25%] [G loss: 3.626974]\n",
      "691 [D loss: 0.414810, acc: 82.81%] [G loss: 3.615029]\n",
      "692 [D loss: 0.549504, acc: 71.88%] [G loss: 3.532432]\n",
      "693 [D loss: 0.425816, acc: 82.81%] [G loss: 3.524927]\n",
      "694 [D loss: 0.431895, acc: 84.38%] [G loss: 3.746214]\n",
      "695 [D loss: 0.409648, acc: 87.50%] [G loss: 3.477248]\n",
      "696 [D loss: 0.445366, acc: 81.25%] [G loss: 3.353386]\n",
      "697 [D loss: 0.628562, acc: 70.31%] [G loss: 3.599795]\n",
      "698 [D loss: 0.403410, acc: 85.94%] [G loss: 3.539815]\n",
      "699 [D loss: 0.593163, acc: 70.31%] [G loss: 3.270568]\n",
      "700 [D loss: 0.363221, acc: 87.50%] [G loss: 3.411161]\n",
      "701 [D loss: 0.634564, acc: 65.62%] [G loss: 3.745386]\n",
      "702 [D loss: 0.568428, acc: 64.06%] [G loss: 2.886366]\n",
      "703 [D loss: 0.495972, acc: 79.69%] [G loss: 3.256085]\n",
      "704 [D loss: 0.421705, acc: 84.38%] [G loss: 3.301062]\n",
      "705 [D loss: 0.467549, acc: 71.88%] [G loss: 3.137797]\n",
      "706 [D loss: 0.413170, acc: 81.25%] [G loss: 3.369255]\n",
      "707 [D loss: 0.538331, acc: 73.44%] [G loss: 3.621940]\n",
      "708 [D loss: 0.479179, acc: 79.69%] [G loss: 3.363406]\n",
      "709 [D loss: 0.545920, acc: 68.75%] [G loss: 3.684249]\n",
      "710 [D loss: 0.418952, acc: 78.12%] [G loss: 3.546869]\n",
      "711 [D loss: 0.809144, acc: 53.12%] [G loss: 3.346545]\n",
      "712 [D loss: 0.491401, acc: 76.56%] [G loss: 2.823991]\n",
      "713 [D loss: 0.430825, acc: 78.12%] [G loss: 3.308694]\n",
      "714 [D loss: 0.562625, acc: 73.44%] [G loss: 3.305566]\n",
      "715 [D loss: 0.532697, acc: 70.31%] [G loss: 3.264633]\n",
      "716 [D loss: 0.607207, acc: 67.19%] [G loss: 3.123507]\n",
      "717 [D loss: 0.541509, acc: 79.69%] [G loss: 3.081747]\n",
      "718 [D loss: 0.764169, acc: 54.69%] [G loss: 3.154824]\n",
      "719 [D loss: 0.450463, acc: 81.25%] [G loss: 3.120576]\n",
      "720 [D loss: 0.521909, acc: 67.19%] [G loss: 3.662520]\n",
      "721 [D loss: 0.448310, acc: 76.56%] [G loss: 3.320660]\n",
      "722 [D loss: 0.539343, acc: 75.00%] [G loss: 3.424088]\n",
      "723 [D loss: 0.598553, acc: 70.31%] [G loss: 3.518888]\n",
      "724 [D loss: 0.527412, acc: 73.44%] [G loss: 3.107933]\n",
      "725 [D loss: 0.377626, acc: 89.06%] [G loss: 3.031010]\n",
      "726 [D loss: 0.597917, acc: 68.75%] [G loss: 3.124919]\n",
      "727 [D loss: 0.609939, acc: 68.75%] [G loss: 3.452847]\n",
      "728 [D loss: 0.538647, acc: 76.56%] [G loss: 3.123520]\n",
      "729 [D loss: 0.434727, acc: 73.44%] [G loss: 3.500896]\n",
      "730 [D loss: 0.453612, acc: 81.25%] [G loss: 3.048331]\n",
      "731 [D loss: 0.477250, acc: 75.00%] [G loss: 3.214150]\n",
      "732 [D loss: 0.461272, acc: 79.69%] [G loss: 3.627681]\n",
      "733 [D loss: 0.582913, acc: 68.75%] [G loss: 3.277497]\n",
      "734 [D loss: 0.491588, acc: 79.69%] [G loss: 3.262142]\n",
      "735 [D loss: 0.474210, acc: 79.69%] [G loss: 3.536216]\n",
      "736 [D loss: 0.571321, acc: 65.62%] [G loss: 3.091630]\n",
      "737 [D loss: 0.546773, acc: 75.00%] [G loss: 3.133542]\n",
      "738 [D loss: 0.490629, acc: 76.56%] [G loss: 3.038700]\n",
      "739 [D loss: 0.480053, acc: 79.69%] [G loss: 3.426839]\n",
      "740 [D loss: 0.461601, acc: 84.38%] [G loss: 3.159836]\n",
      "741 [D loss: 0.702670, acc: 59.38%] [G loss: 2.673130]\n",
      "742 [D loss: 0.499831, acc: 78.12%] [G loss: 3.177417]\n",
      "743 [D loss: 0.441801, acc: 87.50%] [G loss: 3.327209]\n",
      "744 [D loss: 0.595724, acc: 62.50%] [G loss: 3.134096]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "745 [D loss: 0.565713, acc: 73.44%] [G loss: 3.241807]\n",
      "746 [D loss: 0.466891, acc: 75.00%] [G loss: 3.158059]\n",
      "747 [D loss: 0.469696, acc: 84.38%] [G loss: 3.230169]\n",
      "748 [D loss: 0.512032, acc: 73.44%] [G loss: 2.676220]\n",
      "749 [D loss: 0.587824, acc: 59.38%] [G loss: 3.556659]\n",
      "750 [D loss: 0.499714, acc: 71.88%] [G loss: 3.265871]\n",
      "751 [D loss: 0.732827, acc: 59.38%] [G loss: 2.957512]\n",
      "752 [D loss: 0.462601, acc: 76.56%] [G loss: 3.197873]\n",
      "753 [D loss: 0.562937, acc: 65.62%] [G loss: 2.950189]\n",
      "754 [D loss: 0.528406, acc: 71.88%] [G loss: 3.210142]\n",
      "755 [D loss: 0.436181, acc: 79.69%] [G loss: 3.190323]\n",
      "756 [D loss: 0.760725, acc: 59.38%] [G loss: 3.425975]\n",
      "757 [D loss: 0.510606, acc: 68.75%] [G loss: 3.286462]\n",
      "758 [D loss: 0.518351, acc: 78.12%] [G loss: 3.347046]\n",
      "759 [D loss: 0.518854, acc: 76.56%] [G loss: 3.316479]\n",
      "760 [D loss: 0.582839, acc: 73.44%] [G loss: 3.174687]\n",
      "761 [D loss: 0.523247, acc: 71.88%] [G loss: 2.948381]\n",
      "762 [D loss: 0.483944, acc: 76.56%] [G loss: 3.152718]\n",
      "763 [D loss: 0.562950, acc: 70.31%] [G loss: 3.269940]\n",
      "764 [D loss: 0.530666, acc: 70.31%] [G loss: 3.176546]\n",
      "765 [D loss: 0.461635, acc: 76.56%] [G loss: 3.021317]\n",
      "766 [D loss: 0.459997, acc: 78.12%] [G loss: 3.334877]\n",
      "767 [D loss: 0.509076, acc: 78.12%] [G loss: 3.427718]\n",
      "768 [D loss: 0.422243, acc: 85.94%] [G loss: 3.436608]\n",
      "769 [D loss: 0.447824, acc: 79.69%] [G loss: 2.995162]\n",
      "770 [D loss: 0.575123, acc: 73.44%] [G loss: 3.139642]\n",
      "771 [D loss: 0.614796, acc: 65.62%] [G loss: 3.363435]\n",
      "772 [D loss: 0.487765, acc: 71.88%] [G loss: 3.006943]\n",
      "773 [D loss: 0.483294, acc: 73.44%] [G loss: 3.392278]\n",
      "774 [D loss: 0.407211, acc: 85.94%] [G loss: 3.334965]\n",
      "775 [D loss: 0.597811, acc: 60.94%] [G loss: 3.089496]\n",
      "776 [D loss: 0.451553, acc: 78.12%] [G loss: 3.137993]\n",
      "777 [D loss: 0.571749, acc: 65.62%] [G loss: 3.340129]\n",
      "778 [D loss: 0.543164, acc: 79.69%] [G loss: 2.902922]\n",
      "779 [D loss: 0.669839, acc: 54.69%] [G loss: 3.310216]\n",
      "780 [D loss: 0.387525, acc: 85.94%] [G loss: 3.242651]\n",
      "781 [D loss: 0.568899, acc: 68.75%] [G loss: 3.365157]\n",
      "782 [D loss: 0.742974, acc: 62.50%] [G loss: 3.018600]\n",
      "783 [D loss: 0.534136, acc: 71.88%] [G loss: 3.250120]\n",
      "784 [D loss: 0.456099, acc: 75.00%] [G loss: 2.933998]\n",
      "785 [D loss: 0.552740, acc: 64.06%] [G loss: 3.339458]\n",
      "786 [D loss: 0.507856, acc: 73.44%] [G loss: 2.877510]\n",
      "787 [D loss: 0.511720, acc: 71.88%] [G loss: 3.049798]\n",
      "788 [D loss: 0.495735, acc: 75.00%] [G loss: 3.051015]\n",
      "789 [D loss: 0.532085, acc: 73.44%] [G loss: 3.033173]\n",
      "790 [D loss: 0.623492, acc: 62.50%] [G loss: 3.248632]\n",
      "791 [D loss: 0.583624, acc: 71.88%] [G loss: 3.363331]\n",
      "792 [D loss: 0.643008, acc: 59.38%] [G loss: 3.042924]\n",
      "793 [D loss: 0.610049, acc: 68.75%] [G loss: 3.271259]\n",
      "794 [D loss: 0.641723, acc: 59.38%] [G loss: 3.017478]\n",
      "795 [D loss: 0.597348, acc: 64.06%] [G loss: 3.096365]\n",
      "796 [D loss: 0.630279, acc: 67.19%] [G loss: 2.944520]\n",
      "797 [D loss: 0.526989, acc: 73.44%] [G loss: 3.126212]\n",
      "798 [D loss: 0.488981, acc: 81.25%] [G loss: 3.031356]\n",
      "799 [D loss: 0.580321, acc: 68.75%] [G loss: 2.887292]\n",
      "800 [D loss: 0.535216, acc: 73.44%] [G loss: 2.634348]\n",
      "801 [D loss: 0.552170, acc: 68.75%] [G loss: 3.306905]\n",
      "802 [D loss: 0.608917, acc: 70.31%] [G loss: 3.266866]\n",
      "803 [D loss: 0.595293, acc: 68.75%] [G loss: 3.122153]\n",
      "804 [D loss: 0.556909, acc: 67.19%] [G loss: 2.951880]\n",
      "805 [D loss: 0.448484, acc: 84.38%] [G loss: 2.945866]\n",
      "806 [D loss: 0.621189, acc: 65.62%] [G loss: 3.335619]\n",
      "807 [D loss: 0.527405, acc: 68.75%] [G loss: 3.002710]\n",
      "808 [D loss: 0.449238, acc: 76.56%] [G loss: 3.000674]\n",
      "809 [D loss: 0.637344, acc: 62.50%] [G loss: 2.435929]\n",
      "810 [D loss: 0.441112, acc: 78.12%] [G loss: 3.059183]\n",
      "811 [D loss: 0.407351, acc: 85.94%] [G loss: 3.173731]\n",
      "812 [D loss: 0.522627, acc: 73.44%] [G loss: 3.048323]\n",
      "813 [D loss: 0.472916, acc: 82.81%] [G loss: 3.220094]\n",
      "814 [D loss: 0.525883, acc: 70.31%] [G loss: 2.989519]\n",
      "815 [D loss: 0.596987, acc: 68.75%] [G loss: 2.966959]\n",
      "816 [D loss: 0.481378, acc: 78.12%] [G loss: 3.205868]\n",
      "817 [D loss: 0.470364, acc: 76.56%] [G loss: 3.523464]\n",
      "818 [D loss: 0.601032, acc: 68.75%] [G loss: 3.015006]\n",
      "819 [D loss: 0.619086, acc: 71.88%] [G loss: 3.444768]\n",
      "820 [D loss: 0.566491, acc: 65.62%] [G loss: 2.980549]\n",
      "821 [D loss: 0.492685, acc: 78.12%] [G loss: 2.836874]\n",
      "822 [D loss: 0.463645, acc: 78.12%] [G loss: 3.218353]\n",
      "823 [D loss: 0.571994, acc: 73.44%] [G loss: 2.674546]\n",
      "824 [D loss: 0.546749, acc: 71.88%] [G loss: 2.856953]\n",
      "825 [D loss: 0.538147, acc: 70.31%] [G loss: 3.085017]\n",
      "826 [D loss: 0.528974, acc: 75.00%] [G loss: 3.075349]\n",
      "827 [D loss: 0.662798, acc: 68.75%] [G loss: 3.104121]\n",
      "828 [D loss: 0.493003, acc: 67.19%] [G loss: 2.872230]\n",
      "829 [D loss: 0.529416, acc: 70.31%] [G loss: 3.253635]\n",
      "830 [D loss: 0.593914, acc: 71.88%] [G loss: 2.859329]\n",
      "831 [D loss: 0.544260, acc: 73.44%] [G loss: 2.941276]\n",
      "832 [D loss: 0.547479, acc: 73.44%] [G loss: 2.919755]\n",
      "833 [D loss: 0.580449, acc: 71.88%] [G loss: 3.038652]\n",
      "834 [D loss: 0.593267, acc: 67.19%] [G loss: 3.244910]\n",
      "835 [D loss: 0.467641, acc: 82.81%] [G loss: 3.097701]\n",
      "836 [D loss: 0.655750, acc: 62.50%] [G loss: 3.088827]\n",
      "837 [D loss: 0.526705, acc: 73.44%] [G loss: 3.096550]\n",
      "838 [D loss: 0.547945, acc: 73.44%] [G loss: 2.914865]\n",
      "839 [D loss: 0.617801, acc: 64.06%] [G loss: 3.355436]\n",
      "840 [D loss: 0.595063, acc: 65.62%] [G loss: 3.121078]\n",
      "841 [D loss: 0.591259, acc: 65.62%] [G loss: 2.864871]\n",
      "842 [D loss: 0.398184, acc: 85.94%] [G loss: 3.055539]\n",
      "843 [D loss: 0.590382, acc: 65.62%] [G loss: 2.846269]\n",
      "844 [D loss: 0.594468, acc: 67.19%] [G loss: 3.178049]\n",
      "845 [D loss: 0.566886, acc: 67.19%] [G loss: 3.089694]\n",
      "846 [D loss: 0.488788, acc: 76.56%] [G loss: 3.056314]\n",
      "847 [D loss: 0.560941, acc: 73.44%] [G loss: 3.208475]\n",
      "848 [D loss: 0.517545, acc: 70.31%] [G loss: 2.939214]\n",
      "849 [D loss: 0.570563, acc: 68.75%] [G loss: 2.841927]\n",
      "850 [D loss: 0.571432, acc: 65.62%] [G loss: 2.812034]\n",
      "851 [D loss: 0.408218, acc: 87.50%] [G loss: 2.755855]\n",
      "852 [D loss: 0.617877, acc: 64.06%] [G loss: 2.992970]\n",
      "853 [D loss: 0.469964, acc: 78.12%] [G loss: 3.336018]\n",
      "854 [D loss: 0.574888, acc: 71.88%] [G loss: 3.067450]\n",
      "855 [D loss: 0.554025, acc: 76.56%] [G loss: 2.698273]\n",
      "856 [D loss: 0.458411, acc: 76.56%] [G loss: 3.264296]\n",
      "857 [D loss: 0.418608, acc: 87.50%] [G loss: 3.237629]\n",
      "858 [D loss: 0.621029, acc: 65.62%] [G loss: 2.973824]\n",
      "859 [D loss: 0.508041, acc: 79.69%] [G loss: 2.816735]\n",
      "860 [D loss: 0.729607, acc: 51.56%] [G loss: 3.240890]\n",
      "861 [D loss: 0.402501, acc: 82.81%] [G loss: 3.373984]\n",
      "862 [D loss: 0.626204, acc: 65.62%] [G loss: 3.409441]\n",
      "863 [D loss: 0.558536, acc: 73.44%] [G loss: 2.788428]\n",
      "864 [D loss: 0.532327, acc: 73.44%] [G loss: 2.906650]\n",
      "865 [D loss: 0.522466, acc: 70.31%] [G loss: 3.070566]\n",
      "866 [D loss: 0.581502, acc: 73.44%] [G loss: 3.182159]\n",
      "867 [D loss: 0.510134, acc: 73.44%] [G loss: 3.269729]\n",
      "868 [D loss: 0.485028, acc: 81.25%] [G loss: 3.205127]\n",
      "869 [D loss: 0.627252, acc: 68.75%] [G loss: 2.787860]\n",
      "870 [D loss: 0.508464, acc: 75.00%] [G loss: 3.025070]\n",
      "871 [D loss: 0.559325, acc: 71.88%] [G loss: 3.012463]\n",
      "872 [D loss: 0.476370, acc: 73.44%] [G loss: 2.928982]\n",
      "873 [D loss: 0.604976, acc: 70.31%] [G loss: 3.232896]\n",
      "874 [D loss: 0.591378, acc: 71.88%] [G loss: 3.061195]\n",
      "875 [D loss: 0.437131, acc: 84.38%] [G loss: 2.628212]\n",
      "876 [D loss: 0.823141, acc: 50.00%] [G loss: 2.734498]\n",
      "877 [D loss: 0.586187, acc: 78.12%] [G loss: 2.428399]\n",
      "878 [D loss: 0.673921, acc: 59.38%] [G loss: 2.978984]\n",
      "879 [D loss: 0.490967, acc: 76.56%] [G loss: 2.961942]\n",
      "880 [D loss: 0.522058, acc: 76.56%] [G loss: 2.822984]\n",
      "881 [D loss: 0.529110, acc: 70.31%] [G loss: 3.078175]\n",
      "882 [D loss: 0.430444, acc: 76.56%] [G loss: 2.908958]\n",
      "883 [D loss: 0.583564, acc: 68.75%] [G loss: 3.130441]\n",
      "884 [D loss: 0.478934, acc: 76.56%] [G loss: 3.172356]\n",
      "885 [D loss: 0.446767, acc: 81.25%] [G loss: 3.405348]\n",
      "886 [D loss: 0.558608, acc: 73.44%] [G loss: 3.108246]\n",
      "887 [D loss: 0.675751, acc: 68.75%] [G loss: 2.869148]\n",
      "888 [D loss: 0.563270, acc: 62.50%] [G loss: 3.065025]\n",
      "889 [D loss: 0.542700, acc: 68.75%] [G loss: 3.048213]\n",
      "890 [D loss: 0.624610, acc: 65.62%] [G loss: 2.883860]\n",
      "891 [D loss: 0.616128, acc: 68.75%] [G loss: 2.665741]\n",
      "892 [D loss: 0.471190, acc: 78.12%] [G loss: 3.336658]\n",
      "893 [D loss: 0.573354, acc: 70.31%] [G loss: 3.132677]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "894 [D loss: 0.540031, acc: 65.62%] [G loss: 3.055088]\n",
      "895 [D loss: 0.580447, acc: 70.31%] [G loss: 3.025200]\n",
      "896 [D loss: 0.530810, acc: 81.25%] [G loss: 3.122701]\n",
      "897 [D loss: 0.724169, acc: 60.94%] [G loss: 2.956584]\n",
      "898 [D loss: 0.459320, acc: 79.69%] [G loss: 2.727070]\n",
      "899 [D loss: 0.607559, acc: 68.75%] [G loss: 2.831520]\n",
      "900 [D loss: 0.428687, acc: 81.25%] [G loss: 3.249316]\n",
      "901 [D loss: 0.445381, acc: 79.69%] [G loss: 2.828544]\n",
      "902 [D loss: 0.522684, acc: 75.00%] [G loss: 3.000340]\n",
      "903 [D loss: 0.511234, acc: 73.44%] [G loss: 3.043961]\n",
      "904 [D loss: 0.562486, acc: 68.75%] [G loss: 3.139113]\n",
      "905 [D loss: 0.654504, acc: 60.94%] [G loss: 2.750283]\n",
      "906 [D loss: 0.621668, acc: 62.50%] [G loss: 2.902838]\n",
      "907 [D loss: 0.487829, acc: 75.00%] [G loss: 2.879560]\n",
      "908 [D loss: 0.509939, acc: 75.00%] [G loss: 2.874039]\n",
      "909 [D loss: 0.465119, acc: 81.25%] [G loss: 3.209242]\n",
      "910 [D loss: 0.440158, acc: 84.38%] [G loss: 3.235978]\n",
      "911 [D loss: 0.524613, acc: 71.88%] [G loss: 2.839190]\n",
      "912 [D loss: 0.607992, acc: 70.31%] [G loss: 2.955752]\n",
      "913 [D loss: 0.478035, acc: 78.12%] [G loss: 3.030357]\n",
      "914 [D loss: 0.622181, acc: 70.31%] [G loss: 2.966542]\n",
      "915 [D loss: 0.648356, acc: 64.06%] [G loss: 2.994962]\n",
      "916 [D loss: 0.610566, acc: 70.31%] [G loss: 2.597056]\n",
      "917 [D loss: 0.453431, acc: 81.25%] [G loss: 2.836483]\n",
      "918 [D loss: 0.559990, acc: 68.75%] [G loss: 2.982588]\n",
      "919 [D loss: 0.547140, acc: 76.56%] [G loss: 3.789074]\n",
      "920 [D loss: 0.613822, acc: 68.75%] [G loss: 3.164151]\n",
      "921 [D loss: 0.591897, acc: 73.44%] [G loss: 2.842755]\n",
      "922 [D loss: 0.451188, acc: 73.44%] [G loss: 2.603509]\n",
      "923 [D loss: 0.653565, acc: 60.94%] [G loss: 2.868067]\n",
      "924 [D loss: 0.522236, acc: 75.00%] [G loss: 2.896698]\n",
      "925 [D loss: 0.525011, acc: 75.00%] [G loss: 2.929844]\n",
      "926 [D loss: 0.626216, acc: 59.38%] [G loss: 2.815563]\n",
      "927 [D loss: 0.525604, acc: 70.31%] [G loss: 2.826286]\n",
      "928 [D loss: 0.668795, acc: 67.19%] [G loss: 2.541976]\n",
      "929 [D loss: 0.680210, acc: 56.25%] [G loss: 2.901155]\n",
      "930 [D loss: 0.652250, acc: 67.19%] [G loss: 2.693444]\n",
      "931 [D loss: 0.696431, acc: 60.94%] [G loss: 2.806529]\n",
      "932 [D loss: 0.519550, acc: 78.12%] [G loss: 2.897156]\n",
      "933 [D loss: 0.518613, acc: 71.88%] [G loss: 2.866633]\n",
      "934 [D loss: 0.541937, acc: 67.19%] [G loss: 3.016463]\n",
      "935 [D loss: 0.538796, acc: 76.56%] [G loss: 2.972008]\n",
      "936 [D loss: 0.560243, acc: 71.88%] [G loss: 3.319133]\n",
      "937 [D loss: 0.518223, acc: 71.88%] [G loss: 2.893931]\n",
      "938 [D loss: 0.596447, acc: 68.75%] [G loss: 2.799973]\n",
      "939 [D loss: 0.614673, acc: 68.75%] [G loss: 2.722353]\n",
      "940 [D loss: 0.638151, acc: 62.50%] [G loss: 2.698312]\n",
      "941 [D loss: 0.520012, acc: 73.44%] [G loss: 3.080194]\n",
      "942 [D loss: 0.508334, acc: 68.75%] [G loss: 2.746025]\n",
      "943 [D loss: 0.518739, acc: 70.31%] [G loss: 2.710526]\n",
      "944 [D loss: 0.453942, acc: 79.69%] [G loss: 3.316054]\n",
      "945 [D loss: 0.558384, acc: 70.31%] [G loss: 2.858561]\n",
      "946 [D loss: 0.513697, acc: 70.31%] [G loss: 2.699640]\n",
      "947 [D loss: 0.557440, acc: 75.00%] [G loss: 2.985137]\n",
      "948 [D loss: 0.650340, acc: 64.06%] [G loss: 2.729531]\n",
      "949 [D loss: 0.622365, acc: 64.06%] [G loss: 2.618267]\n",
      "950 [D loss: 0.515048, acc: 76.56%] [G loss: 2.787079]\n",
      "951 [D loss: 0.487342, acc: 76.56%] [G loss: 2.775802]\n",
      "952 [D loss: 0.533909, acc: 73.44%] [G loss: 3.179712]\n",
      "953 [D loss: 0.562195, acc: 70.31%] [G loss: 2.983637]\n",
      "954 [D loss: 0.545197, acc: 65.62%] [G loss: 2.880230]\n",
      "955 [D loss: 0.501420, acc: 75.00%] [G loss: 2.848493]\n",
      "956 [D loss: 0.570727, acc: 75.00%] [G loss: 3.215304]\n",
      "957 [D loss: 0.487505, acc: 75.00%] [G loss: 2.839323]\n",
      "958 [D loss: 0.613085, acc: 70.31%] [G loss: 2.627951]\n",
      "959 [D loss: 0.639643, acc: 65.62%] [G loss: 2.981795]\n",
      "960 [D loss: 0.547666, acc: 73.44%] [G loss: 3.008262]\n",
      "961 [D loss: 0.489635, acc: 75.00%] [G loss: 3.079944]\n",
      "962 [D loss: 0.510797, acc: 79.69%] [G loss: 3.325968]\n",
      "963 [D loss: 0.550574, acc: 71.88%] [G loss: 3.048025]\n",
      "964 [D loss: 0.653124, acc: 62.50%] [G loss: 3.046209]\n",
      "965 [D loss: 0.485673, acc: 76.56%] [G loss: 3.091236]\n",
      "966 [D loss: 0.710769, acc: 59.38%] [G loss: 2.822998]\n",
      "967 [D loss: 0.409600, acc: 89.06%] [G loss: 3.019702]\n",
      "968 [D loss: 0.702202, acc: 59.38%] [G loss: 2.961778]\n",
      "969 [D loss: 0.554377, acc: 76.56%] [G loss: 2.374135]\n",
      "970 [D loss: 0.572020, acc: 73.44%] [G loss: 3.068485]\n",
      "971 [D loss: 0.586224, acc: 65.62%] [G loss: 2.761548]\n",
      "972 [D loss: 0.514523, acc: 78.12%] [G loss: 2.717841]\n",
      "973 [D loss: 0.628384, acc: 59.38%] [G loss: 2.788615]\n",
      "974 [D loss: 0.676668, acc: 59.38%] [G loss: 2.954268]\n",
      "975 [D loss: 0.601587, acc: 68.75%] [G loss: 2.713101]\n",
      "976 [D loss: 0.627694, acc: 62.50%] [G loss: 2.787184]\n",
      "977 [D loss: 0.633033, acc: 65.62%] [G loss: 2.781124]\n",
      "978 [D loss: 0.539298, acc: 71.88%] [G loss: 3.009580]\n",
      "979 [D loss: 0.696477, acc: 65.62%] [G loss: 2.749523]\n",
      "980 [D loss: 0.524632, acc: 76.56%] [G loss: 2.770954]\n",
      "981 [D loss: 0.476529, acc: 76.56%] [G loss: 2.731005]\n",
      "982 [D loss: 0.620258, acc: 65.62%] [G loss: 3.033670]\n",
      "983 [D loss: 0.570449, acc: 71.88%] [G loss: 2.800666]\n",
      "984 [D loss: 0.554248, acc: 67.19%] [G loss: 3.102052]\n",
      "985 [D loss: 0.524201, acc: 76.56%] [G loss: 2.805006]\n",
      "986 [D loss: 0.812201, acc: 53.12%] [G loss: 2.776561]\n",
      "987 [D loss: 0.482373, acc: 78.12%] [G loss: 2.934666]\n",
      "988 [D loss: 0.679068, acc: 57.81%] [G loss: 2.862852]\n",
      "989 [D loss: 0.582942, acc: 70.31%] [G loss: 2.581579]\n",
      "990 [D loss: 0.519642, acc: 76.56%] [G loss: 2.920929]\n",
      "991 [D loss: 0.673784, acc: 53.12%] [G loss: 2.611674]\n",
      "992 [D loss: 0.501256, acc: 81.25%] [G loss: 2.868979]\n",
      "993 [D loss: 0.623670, acc: 67.19%] [G loss: 2.703591]\n",
      "994 [D loss: 0.622014, acc: 59.38%] [G loss: 2.495040]\n",
      "995 [D loss: 0.650332, acc: 64.06%] [G loss: 2.903276]\n",
      "996 [D loss: 0.707385, acc: 62.50%] [G loss: 3.022508]\n",
      "997 [D loss: 0.510383, acc: 68.75%] [G loss: 2.874469]\n",
      "998 [D loss: 0.460627, acc: 76.56%] [G loss: 3.109721]\n",
      "999 [D loss: 0.432624, acc: 82.81%] [G loss: 2.689740]\n",
      "1000 [D loss: 0.684425, acc: 60.94%] [G loss: 2.575232]\n",
      "1001 [D loss: 0.554889, acc: 70.31%] [G loss: 2.629781]\n",
      "1002 [D loss: 0.564893, acc: 71.88%] [G loss: 2.493426]\n",
      "1003 [D loss: 0.537223, acc: 70.31%] [G loss: 2.990492]\n",
      "1004 [D loss: 0.564100, acc: 70.31%] [G loss: 2.643785]\n",
      "1005 [D loss: 0.621691, acc: 59.38%] [G loss: 2.388488]\n",
      "1006 [D loss: 0.518131, acc: 75.00%] [G loss: 2.639477]\n",
      "1007 [D loss: 0.635779, acc: 73.44%] [G loss: 2.783010]\n",
      "1008 [D loss: 0.554493, acc: 60.94%] [G loss: 2.438618]\n",
      "1009 [D loss: 0.556983, acc: 68.75%] [G loss: 2.860549]\n",
      "1010 [D loss: 0.489662, acc: 76.56%] [G loss: 2.678260]\n",
      "1011 [D loss: 0.565661, acc: 70.31%] [G loss: 3.111024]\n",
      "1012 [D loss: 0.555531, acc: 73.44%] [G loss: 2.510795]\n",
      "1013 [D loss: 0.566192, acc: 75.00%] [G loss: 2.621028]\n",
      "1014 [D loss: 0.617976, acc: 70.31%] [G loss: 2.708953]\n",
      "1015 [D loss: 0.622320, acc: 60.94%] [G loss: 2.715570]\n",
      "1016 [D loss: 0.511883, acc: 75.00%] [G loss: 2.947455]\n",
      "1017 [D loss: 0.470674, acc: 76.56%] [G loss: 3.077206]\n",
      "1018 [D loss: 0.621956, acc: 68.75%] [G loss: 2.614098]\n",
      "1019 [D loss: 0.645084, acc: 65.62%] [G loss: 2.882772]\n",
      "1020 [D loss: 0.638977, acc: 67.19%] [G loss: 2.569051]\n",
      "1021 [D loss: 0.456963, acc: 79.69%] [G loss: 2.600410]\n",
      "1022 [D loss: 0.599216, acc: 68.75%] [G loss: 2.782257]\n",
      "1023 [D loss: 0.635384, acc: 65.62%] [G loss: 2.469580]\n",
      "1024 [D loss: 0.571248, acc: 71.88%] [G loss: 3.009340]\n",
      "1025 [D loss: 0.691233, acc: 64.06%] [G loss: 2.682908]\n",
      "1026 [D loss: 0.536948, acc: 73.44%] [G loss: 2.663846]\n",
      "1027 [D loss: 0.464092, acc: 76.56%] [G loss: 2.832318]\n",
      "1028 [D loss: 0.564664, acc: 71.88%] [G loss: 2.990334]\n",
      "1029 [D loss: 0.514755, acc: 76.56%] [G loss: 2.564781]\n",
      "1030 [D loss: 0.651933, acc: 71.88%] [G loss: 2.698308]\n",
      "1031 [D loss: 0.543349, acc: 68.75%] [G loss: 2.735917]\n",
      "1032 [D loss: 0.514400, acc: 73.44%] [G loss: 2.951594]\n",
      "1033 [D loss: 0.686444, acc: 60.94%] [G loss: 2.854402]\n",
      "1034 [D loss: 0.585888, acc: 68.75%] [G loss: 2.788270]\n",
      "1035 [D loss: 0.554866, acc: 67.19%] [G loss: 2.691642]\n",
      "1036 [D loss: 0.565058, acc: 73.44%] [G loss: 2.991604]\n",
      "1037 [D loss: 0.522539, acc: 81.25%] [G loss: 2.583140]\n",
      "1038 [D loss: 0.564150, acc: 75.00%] [G loss: 2.569695]\n",
      "1039 [D loss: 0.553877, acc: 70.31%] [G loss: 2.685586]\n",
      "1040 [D loss: 0.597163, acc: 65.62%] [G loss: 2.782034]\n",
      "1041 [D loss: 0.709603, acc: 60.94%] [G loss: 2.849515]\n",
      "1042 [D loss: 0.517159, acc: 78.12%] [G loss: 2.862052]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1043 [D loss: 0.545526, acc: 71.88%] [G loss: 2.582049]\n",
      "1044 [D loss: 0.567107, acc: 71.88%] [G loss: 3.002445]\n",
      "1045 [D loss: 0.537011, acc: 76.56%] [G loss: 2.691355]\n",
      "1046 [D loss: 0.609517, acc: 64.06%] [G loss: 2.825548]\n",
      "1047 [D loss: 0.551482, acc: 73.44%] [G loss: 2.701110]\n",
      "1048 [D loss: 0.583902, acc: 65.62%] [G loss: 2.966018]\n",
      "1049 [D loss: 0.693659, acc: 64.06%] [G loss: 2.861208]\n",
      "1050 [D loss: 0.677051, acc: 60.94%] [G loss: 2.780712]\n",
      "1051 [D loss: 0.528837, acc: 76.56%] [G loss: 2.534616]\n",
      "1052 [D loss: 0.596487, acc: 67.19%] [G loss: 2.897629]\n",
      "1053 [D loss: 0.633004, acc: 64.06%] [G loss: 2.638061]\n",
      "1054 [D loss: 0.698757, acc: 62.50%] [G loss: 3.003698]\n",
      "1055 [D loss: 0.534305, acc: 71.88%] [G loss: 3.012428]\n",
      "1056 [D loss: 0.588122, acc: 71.88%] [G loss: 2.553753]\n",
      "1057 [D loss: 0.506948, acc: 70.31%] [G loss: 2.799271]\n",
      "1058 [D loss: 0.505193, acc: 75.00%] [G loss: 2.639468]\n",
      "1059 [D loss: 0.605672, acc: 65.62%] [G loss: 2.635161]\n",
      "1060 [D loss: 0.593327, acc: 67.19%] [G loss: 2.517589]\n",
      "1061 [D loss: 0.515856, acc: 78.12%] [G loss: 2.805178]\n",
      "1062 [D loss: 0.579139, acc: 75.00%] [G loss: 2.748854]\n",
      "1063 [D loss: 0.511405, acc: 82.81%] [G loss: 2.901650]\n",
      "1064 [D loss: 0.461227, acc: 79.69%] [G loss: 3.123981]\n",
      "1065 [D loss: 0.690701, acc: 60.94%] [G loss: 2.855794]\n",
      "1066 [D loss: 0.501918, acc: 75.00%] [G loss: 2.705805]\n",
      "1067 [D loss: 0.497814, acc: 78.12%] [G loss: 2.603140]\n",
      "1068 [D loss: 0.626658, acc: 70.31%] [G loss: 3.011678]\n",
      "1069 [D loss: 0.442098, acc: 82.81%] [G loss: 2.855484]\n",
      "1070 [D loss: 0.597028, acc: 64.06%] [G loss: 3.001740]\n",
      "1071 [D loss: 0.751198, acc: 51.56%] [G loss: 2.816054]\n",
      "1072 [D loss: 0.595296, acc: 75.00%] [G loss: 2.799993]\n",
      "1073 [D loss: 0.558431, acc: 71.88%] [G loss: 2.610302]\n",
      "1074 [D loss: 0.607386, acc: 62.50%] [G loss: 2.964684]\n",
      "1075 [D loss: 0.576247, acc: 68.75%] [G loss: 3.018506]\n",
      "1076 [D loss: 0.643632, acc: 62.50%] [G loss: 2.777552]\n",
      "1077 [D loss: 0.563354, acc: 68.75%] [G loss: 2.828971]\n",
      "1078 [D loss: 0.461019, acc: 81.25%] [G loss: 2.876235]\n",
      "1079 [D loss: 0.512922, acc: 71.88%] [G loss: 2.955029]\n",
      "1080 [D loss: 0.529394, acc: 68.75%] [G loss: 3.031214]\n",
      "1081 [D loss: 0.562038, acc: 73.44%] [G loss: 3.033690]\n",
      "1082 [D loss: 0.614951, acc: 62.50%] [G loss: 3.155968]\n",
      "1083 [D loss: 0.583363, acc: 68.75%] [G loss: 3.029292]\n",
      "1084 [D loss: 0.547753, acc: 68.75%] [G loss: 3.296004]\n",
      "1085 [D loss: 0.472985, acc: 75.00%] [G loss: 2.916561]\n",
      "1086 [D loss: 0.530895, acc: 75.00%] [G loss: 2.880211]\n",
      "1087 [D loss: 0.453965, acc: 82.81%] [G loss: 3.205712]\n",
      "1088 [D loss: 0.515474, acc: 76.56%] [G loss: 2.681261]\n",
      "1089 [D loss: 0.559205, acc: 75.00%] [G loss: 3.008174]\n",
      "1090 [D loss: 0.517130, acc: 76.56%] [G loss: 2.755389]\n",
      "1091 [D loss: 0.616813, acc: 67.19%] [G loss: 3.040550]\n",
      "1092 [D loss: 0.607215, acc: 70.31%] [G loss: 2.855030]\n",
      "1093 [D loss: 0.512187, acc: 82.81%] [G loss: 3.051402]\n",
      "1094 [D loss: 0.493099, acc: 78.12%] [G loss: 3.168697]\n",
      "1095 [D loss: 0.673562, acc: 62.50%] [G loss: 2.973123]\n",
      "1096 [D loss: 0.558542, acc: 65.62%] [G loss: 2.807916]\n",
      "1097 [D loss: 0.539994, acc: 76.56%] [G loss: 3.035039]\n",
      "1098 [D loss: 0.482219, acc: 75.00%] [G loss: 2.796267]\n",
      "1099 [D loss: 0.638913, acc: 62.50%] [G loss: 2.954359]\n",
      "1100 [D loss: 0.647818, acc: 60.94%] [G loss: 2.881226]\n",
      "1101 [D loss: 0.704592, acc: 56.25%] [G loss: 3.008452]\n",
      "1102 [D loss: 0.540807, acc: 75.00%] [G loss: 3.024887]\n",
      "1103 [D loss: 0.694686, acc: 57.81%] [G loss: 2.228992]\n",
      "1104 [D loss: 0.553031, acc: 75.00%] [G loss: 3.096559]\n",
      "1105 [D loss: 0.515724, acc: 73.44%] [G loss: 2.675057]\n",
      "1106 [D loss: 0.493148, acc: 75.00%] [G loss: 2.744051]\n",
      "1107 [D loss: 0.528190, acc: 76.56%] [G loss: 2.847415]\n",
      "1108 [D loss: 0.721797, acc: 59.38%] [G loss: 3.013580]\n",
      "1109 [D loss: 0.388167, acc: 85.94%] [G loss: 2.979937]\n",
      "1110 [D loss: 0.542856, acc: 76.56%] [G loss: 2.831880]\n",
      "1111 [D loss: 0.615298, acc: 59.38%] [G loss: 2.897261]\n",
      "1112 [D loss: 0.498983, acc: 75.00%] [G loss: 2.706134]\n",
      "1113 [D loss: 0.563489, acc: 71.88%] [G loss: 3.263798]\n",
      "1114 [D loss: 0.423816, acc: 82.81%] [G loss: 3.030321]\n",
      "1115 [D loss: 0.493311, acc: 78.12%] [G loss: 3.047866]\n",
      "1116 [D loss: 0.480758, acc: 81.25%] [G loss: 3.122950]\n",
      "1117 [D loss: 0.515878, acc: 70.31%] [G loss: 2.855253]\n",
      "1118 [D loss: 0.639447, acc: 70.31%] [G loss: 3.023572]\n",
      "1119 [D loss: 0.523444, acc: 79.69%] [G loss: 2.593054]\n",
      "1120 [D loss: 0.665673, acc: 62.50%] [G loss: 2.505799]\n",
      "1121 [D loss: 0.556725, acc: 73.44%] [G loss: 2.732615]\n",
      "1122 [D loss: 0.804097, acc: 51.56%] [G loss: 2.975232]\n",
      "1123 [D loss: 0.502545, acc: 81.25%] [G loss: 2.869558]\n",
      "1124 [D loss: 0.586425, acc: 68.75%] [G loss: 2.867799]\n",
      "1125 [D loss: 0.590712, acc: 70.31%] [G loss: 3.192733]\n",
      "1126 [D loss: 0.630090, acc: 62.50%] [G loss: 2.777792]\n",
      "1127 [D loss: 0.488978, acc: 73.44%] [G loss: 3.388288]\n",
      "1128 [D loss: 0.451559, acc: 78.12%] [G loss: 3.032538]\n",
      "1129 [D loss: 0.603067, acc: 68.75%] [G loss: 2.655956]\n",
      "1130 [D loss: 0.620708, acc: 65.62%] [G loss: 2.979733]\n",
      "1131 [D loss: 0.521756, acc: 76.56%] [G loss: 3.083552]\n",
      "1132 [D loss: 0.732498, acc: 54.69%] [G loss: 3.120124]\n",
      "1133 [D loss: 0.588639, acc: 65.62%] [G loss: 2.848940]\n",
      "1134 [D loss: 0.636259, acc: 56.25%] [G loss: 2.841236]\n",
      "1135 [D loss: 0.550596, acc: 70.31%] [G loss: 2.788332]\n",
      "1136 [D loss: 0.540122, acc: 79.69%] [G loss: 2.800183]\n",
      "1137 [D loss: 0.572204, acc: 68.75%] [G loss: 2.884754]\n",
      "1138 [D loss: 0.468219, acc: 82.81%] [G loss: 3.044397]\n",
      "1139 [D loss: 0.547160, acc: 71.88%] [G loss: 3.021066]\n",
      "1140 [D loss: 0.552183, acc: 67.19%] [G loss: 3.302958]\n",
      "1141 [D loss: 0.549661, acc: 76.56%] [G loss: 3.060424]\n",
      "1142 [D loss: 0.503735, acc: 71.88%] [G loss: 2.672203]\n",
      "1143 [D loss: 0.549837, acc: 71.88%] [G loss: 2.994168]\n",
      "1144 [D loss: 0.756191, acc: 56.25%] [G loss: 2.572304]\n",
      "1145 [D loss: 0.508396, acc: 78.12%] [G loss: 3.027687]\n",
      "1146 [D loss: 0.429976, acc: 81.25%] [G loss: 3.014472]\n",
      "1147 [D loss: 0.671889, acc: 64.06%] [G loss: 2.569577]\n",
      "1148 [D loss: 0.704438, acc: 65.62%] [G loss: 2.623203]\n",
      "1149 [D loss: 0.546058, acc: 65.62%] [G loss: 2.732083]\n",
      "1150 [D loss: 0.494061, acc: 73.44%] [G loss: 2.935143]\n",
      "1151 [D loss: 0.725438, acc: 59.38%] [G loss: 2.708528]\n",
      "1152 [D loss: 0.551559, acc: 73.44%] [G loss: 2.574534]\n",
      "1153 [D loss: 0.557627, acc: 73.44%] [G loss: 2.678333]\n",
      "1154 [D loss: 0.710172, acc: 62.50%] [G loss: 2.478014]\n",
      "1155 [D loss: 0.600103, acc: 67.19%] [G loss: 2.931161]\n",
      "1156 [D loss: 0.591356, acc: 64.06%] [G loss: 2.766183]\n",
      "1157 [D loss: 0.587534, acc: 71.88%] [G loss: 2.372811]\n",
      "1158 [D loss: 0.684924, acc: 67.19%] [G loss: 2.569843]\n",
      "1159 [D loss: 0.553124, acc: 75.00%] [G loss: 2.617423]\n",
      "1160 [D loss: 0.683310, acc: 60.94%] [G loss: 2.569120]\n",
      "1161 [D loss: 0.504508, acc: 70.31%] [G loss: 2.797678]\n",
      "1162 [D loss: 0.617180, acc: 62.50%] [G loss: 3.041209]\n",
      "1163 [D loss: 0.558731, acc: 71.88%] [G loss: 2.675760]\n",
      "1164 [D loss: 0.650284, acc: 67.19%] [G loss: 2.885199]\n",
      "1165 [D loss: 0.472005, acc: 82.81%] [G loss: 2.701028]\n",
      "1166 [D loss: 0.622047, acc: 70.31%] [G loss: 2.780991]\n",
      "1167 [D loss: 0.653250, acc: 59.38%] [G loss: 2.856991]\n",
      "1168 [D loss: 0.614749, acc: 65.62%] [G loss: 2.873521]\n",
      "1169 [D loss: 0.738776, acc: 60.94%] [G loss: 2.734744]\n",
      "1170 [D loss: 0.636107, acc: 62.50%] [G loss: 2.789329]\n",
      "1171 [D loss: 0.683648, acc: 64.06%] [G loss: 2.212774]\n",
      "1172 [D loss: 0.651556, acc: 65.62%] [G loss: 2.742122]\n",
      "1173 [D loss: 0.514172, acc: 71.88%] [G loss: 2.499122]\n",
      "1174 [D loss: 0.619536, acc: 65.62%] [G loss: 2.452868]\n",
      "1175 [D loss: 0.541972, acc: 73.44%] [G loss: 2.715336]\n",
      "1176 [D loss: 0.526745, acc: 71.88%] [G loss: 2.571129]\n",
      "1177 [D loss: 0.525818, acc: 70.31%] [G loss: 2.940436]\n",
      "1178 [D loss: 0.651151, acc: 70.31%] [G loss: 2.719650]\n",
      "1179 [D loss: 0.494862, acc: 81.25%] [G loss: 2.879539]\n",
      "1180 [D loss: 0.720592, acc: 59.38%] [G loss: 2.511619]\n",
      "1181 [D loss: 0.610768, acc: 62.50%] [G loss: 2.719153]\n",
      "1182 [D loss: 0.607288, acc: 76.56%] [G loss: 2.995032]\n",
      "1183 [D loss: 0.644493, acc: 60.94%] [G loss: 3.147460]\n",
      "1184 [D loss: 0.563469, acc: 70.31%] [G loss: 2.702937]\n",
      "1185 [D loss: 0.534073, acc: 73.44%] [G loss: 2.703320]\n",
      "1186 [D loss: 0.621570, acc: 65.62%] [G loss: 2.537234]\n",
      "1187 [D loss: 0.438913, acc: 81.25%] [G loss: 2.658010]\n",
      "1188 [D loss: 0.626144, acc: 65.62%] [G loss: 2.398211]\n",
      "1189 [D loss: 0.604138, acc: 64.06%] [G loss: 2.903486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1190 [D loss: 0.590061, acc: 70.31%] [G loss: 2.597369]\n",
      "1191 [D loss: 0.616155, acc: 67.19%] [G loss: 2.664262]\n",
      "1192 [D loss: 0.550600, acc: 68.75%] [G loss: 2.657630]\n",
      "1193 [D loss: 0.590390, acc: 70.31%] [G loss: 3.025695]\n",
      "1194 [D loss: 0.639629, acc: 62.50%] [G loss: 2.686191]\n",
      "1195 [D loss: 0.605298, acc: 67.19%] [G loss: 2.896614]\n",
      "1196 [D loss: 0.561170, acc: 70.31%] [G loss: 2.746134]\n",
      "1197 [D loss: 0.621627, acc: 59.38%] [G loss: 2.842017]\n",
      "1198 [D loss: 0.553435, acc: 71.88%] [G loss: 2.754693]\n",
      "1199 [D loss: 0.730080, acc: 57.81%] [G loss: 2.392956]\n",
      "1200 [D loss: 0.609166, acc: 68.75%] [G loss: 2.545686]\n",
      "1201 [D loss: 0.614706, acc: 64.06%] [G loss: 2.739901]\n",
      "1202 [D loss: 0.574839, acc: 65.62%] [G loss: 2.325250]\n",
      "1203 [D loss: 0.535075, acc: 75.00%] [G loss: 2.822335]\n",
      "1204 [D loss: 0.618724, acc: 62.50%] [G loss: 2.250563]\n",
      "1205 [D loss: 0.473771, acc: 78.12%] [G loss: 2.527232]\n",
      "1206 [D loss: 0.503397, acc: 71.88%] [G loss: 2.422122]\n",
      "1207 [D loss: 0.629589, acc: 70.31%] [G loss: 2.698131]\n",
      "1208 [D loss: 0.688961, acc: 65.62%] [G loss: 2.632476]\n",
      "1209 [D loss: 0.576608, acc: 65.62%] [G loss: 2.511105]\n",
      "1210 [D loss: 0.657231, acc: 62.50%] [G loss: 2.629459]\n",
      "1211 [D loss: 0.513775, acc: 70.31%] [G loss: 2.520781]\n",
      "1212 [D loss: 0.700346, acc: 60.94%] [G loss: 2.616042]\n",
      "1213 [D loss: 0.648228, acc: 62.50%] [G loss: 2.634397]\n",
      "1214 [D loss: 0.475903, acc: 75.00%] [G loss: 2.578281]\n",
      "1215 [D loss: 0.647017, acc: 68.75%] [G loss: 2.838027]\n",
      "1216 [D loss: 0.476952, acc: 71.88%] [G loss: 3.168954]\n",
      "1217 [D loss: 0.537027, acc: 71.88%] [G loss: 2.594709]\n",
      "1218 [D loss: 0.533711, acc: 71.88%] [G loss: 2.752751]\n",
      "1219 [D loss: 0.658993, acc: 60.94%] [G loss: 2.640109]\n",
      "1220 [D loss: 0.503079, acc: 78.12%] [G loss: 2.806638]\n",
      "1221 [D loss: 0.576428, acc: 64.06%] [G loss: 2.584260]\n",
      "1222 [D loss: 0.542362, acc: 78.12%] [G loss: 3.165225]\n",
      "1223 [D loss: 0.612863, acc: 68.75%] [G loss: 2.706486]\n",
      "1224 [D loss: 0.575691, acc: 67.19%] [G loss: 2.810975]\n",
      "1225 [D loss: 0.486235, acc: 78.12%] [G loss: 2.599631]\n",
      "1226 [D loss: 0.708236, acc: 59.38%] [G loss: 2.407560]\n",
      "1227 [D loss: 0.768162, acc: 53.12%] [G loss: 2.536950]\n",
      "1228 [D loss: 0.513705, acc: 79.69%] [G loss: 2.754466]\n",
      "1229 [D loss: 0.514168, acc: 70.31%] [G loss: 2.783391]\n",
      "1230 [D loss: 0.576939, acc: 67.19%] [G loss: 2.723578]\n",
      "1231 [D loss: 0.662175, acc: 59.38%] [G loss: 2.685042]\n",
      "1232 [D loss: 0.571836, acc: 68.75%] [G loss: 2.984498]\n",
      "1233 [D loss: 0.573970, acc: 68.75%] [G loss: 2.933806]\n",
      "1234 [D loss: 0.636566, acc: 65.62%] [G loss: 2.693611]\n",
      "1235 [D loss: 0.687200, acc: 57.81%] [G loss: 2.373495]\n",
      "1236 [D loss: 0.550174, acc: 68.75%] [G loss: 2.940248]\n",
      "1237 [D loss: 0.727216, acc: 59.38%] [G loss: 2.583224]\n",
      "1238 [D loss: 0.635700, acc: 62.50%] [G loss: 2.620334]\n",
      "1239 [D loss: 0.616784, acc: 64.06%] [G loss: 2.889132]\n",
      "1240 [D loss: 0.665461, acc: 65.62%] [G loss: 2.494834]\n",
      "1241 [D loss: 0.533184, acc: 75.00%] [G loss: 2.690468]\n",
      "1242 [D loss: 0.566395, acc: 67.19%] [G loss: 2.933614]\n",
      "1243 [D loss: 0.589909, acc: 62.50%] [G loss: 2.472131]\n",
      "1244 [D loss: 0.522278, acc: 68.75%] [G loss: 2.845942]\n",
      "1245 [D loss: 0.563353, acc: 73.44%] [G loss: 2.654115]\n",
      "1246 [D loss: 0.692230, acc: 54.69%] [G loss: 2.900774]\n",
      "1247 [D loss: 0.675842, acc: 59.38%] [G loss: 2.702736]\n",
      "1248 [D loss: 0.643931, acc: 62.50%] [G loss: 2.861766]\n",
      "1249 [D loss: 0.567593, acc: 67.19%] [G loss: 2.608082]\n",
      "1250 [D loss: 0.615894, acc: 68.75%] [G loss: 2.660327]\n",
      "1251 [D loss: 0.585019, acc: 76.56%] [G loss: 2.479738]\n",
      "1252 [D loss: 0.556616, acc: 70.31%] [G loss: 2.469595]\n",
      "1253 [D loss: 0.664320, acc: 62.50%] [G loss: 2.491808]\n",
      "1254 [D loss: 0.683214, acc: 65.62%] [G loss: 2.574295]\n",
      "1255 [D loss: 0.664595, acc: 59.38%] [G loss: 2.614079]\n",
      "1256 [D loss: 0.592359, acc: 71.88%] [G loss: 2.580845]\n",
      "1257 [D loss: 0.710885, acc: 56.25%] [G loss: 2.565120]\n",
      "1258 [D loss: 0.700115, acc: 54.69%] [G loss: 2.564314]\n",
      "1259 [D loss: 0.491679, acc: 81.25%] [G loss: 2.905914]\n",
      "1260 [D loss: 0.681252, acc: 60.94%] [G loss: 2.605381]\n",
      "1261 [D loss: 0.569980, acc: 67.19%] [G loss: 2.624640]\n",
      "1262 [D loss: 0.575174, acc: 70.31%] [G loss: 2.318678]\n",
      "1263 [D loss: 0.550941, acc: 70.31%] [G loss: 2.437548]\n",
      "1264 [D loss: 0.571325, acc: 73.44%] [G loss: 2.533617]\n",
      "1265 [D loss: 0.738391, acc: 67.19%] [G loss: 2.489826]\n",
      "1266 [D loss: 0.459808, acc: 87.50%] [G loss: 2.790543]\n",
      "1267 [D loss: 0.792033, acc: 48.44%] [G loss: 2.430272]\n",
      "1268 [D loss: 0.536201, acc: 67.19%] [G loss: 2.608379]\n",
      "1269 [D loss: 0.664882, acc: 64.06%] [G loss: 2.605976]\n",
      "1270 [D loss: 0.644566, acc: 62.50%] [G loss: 2.899929]\n",
      "1271 [D loss: 0.646437, acc: 62.50%] [G loss: 2.542418]\n",
      "1272 [D loss: 0.641037, acc: 62.50%] [G loss: 2.380825]\n",
      "1273 [D loss: 0.663948, acc: 70.31%] [G loss: 2.635180]\n",
      "1274 [D loss: 0.624454, acc: 64.06%] [G loss: 2.538211]\n",
      "1275 [D loss: 0.607443, acc: 67.19%] [G loss: 2.446287]\n",
      "1276 [D loss: 0.529322, acc: 75.00%] [G loss: 2.596552]\n",
      "1277 [D loss: 0.527377, acc: 75.00%] [G loss: 2.792398]\n",
      "1278 [D loss: 0.622740, acc: 59.38%] [G loss: 2.853194]\n",
      "1279 [D loss: 0.726500, acc: 59.38%] [G loss: 2.349048]\n",
      "1280 [D loss: 0.540195, acc: 70.31%] [G loss: 2.598772]\n",
      "1281 [D loss: 0.574147, acc: 67.19%] [G loss: 2.635212]\n",
      "1282 [D loss: 0.651348, acc: 56.25%] [G loss: 2.860013]\n",
      "1283 [D loss: 0.555038, acc: 68.75%] [G loss: 2.386082]\n",
      "1284 [D loss: 0.587849, acc: 67.19%] [G loss: 2.853858]\n",
      "1285 [D loss: 0.549960, acc: 73.44%] [G loss: 2.793158]\n",
      "1286 [D loss: 0.552910, acc: 70.31%] [G loss: 2.563065]\n",
      "1287 [D loss: 0.650420, acc: 65.62%] [G loss: 2.962315]\n",
      "1288 [D loss: 0.624652, acc: 70.31%] [G loss: 2.802329]\n",
      "1289 [D loss: 0.509731, acc: 73.44%] [G loss: 2.441546]\n",
      "1290 [D loss: 0.646517, acc: 68.75%] [G loss: 2.334234]\n",
      "1291 [D loss: 0.727970, acc: 53.12%] [G loss: 2.712824]\n",
      "1292 [D loss: 0.537641, acc: 75.00%] [G loss: 2.726061]\n",
      "1293 [D loss: 0.591184, acc: 70.31%] [G loss: 2.488820]\n",
      "1294 [D loss: 0.542273, acc: 75.00%] [G loss: 2.789324]\n",
      "1295 [D loss: 0.589503, acc: 70.31%] [G loss: 2.367224]\n",
      "1296 [D loss: 0.593749, acc: 76.56%] [G loss: 2.892022]\n",
      "1297 [D loss: 0.742214, acc: 57.81%] [G loss: 2.682718]\n",
      "1298 [D loss: 0.624833, acc: 68.75%] [G loss: 2.455595]\n",
      "1299 [D loss: 0.714185, acc: 56.25%] [G loss: 2.498654]\n",
      "1300 [D loss: 0.602752, acc: 71.88%] [G loss: 2.628802]\n",
      "1301 [D loss: 0.596383, acc: 67.19%] [G loss: 2.841237]\n",
      "1302 [D loss: 0.587598, acc: 67.19%] [G loss: 3.012859]\n",
      "1303 [D loss: 0.492289, acc: 78.12%] [G loss: 2.641024]\n",
      "1304 [D loss: 0.740787, acc: 50.00%] [G loss: 2.307287]\n",
      "1305 [D loss: 0.476265, acc: 75.00%] [G loss: 2.925756]\n",
      "1306 [D loss: 0.710093, acc: 53.12%] [G loss: 2.518431]\n",
      "1307 [D loss: 0.684348, acc: 59.38%] [G loss: 2.548244]\n",
      "1308 [D loss: 0.476124, acc: 76.56%] [G loss: 3.118338]\n",
      "1309 [D loss: 0.521344, acc: 76.56%] [G loss: 2.656312]\n",
      "1310 [D loss: 0.666079, acc: 65.62%] [G loss: 2.378752]\n",
      "1311 [D loss: 0.578361, acc: 64.06%] [G loss: 2.554746]\n",
      "1312 [D loss: 0.539363, acc: 73.44%] [G loss: 2.616302]\n",
      "1313 [D loss: 0.413172, acc: 87.50%] [G loss: 2.634891]\n",
      "1314 [D loss: 0.603532, acc: 70.31%] [G loss: 2.841856]\n",
      "1315 [D loss: 0.553461, acc: 75.00%] [G loss: 2.575889]\n",
      "1316 [D loss: 0.603121, acc: 62.50%] [G loss: 2.305440]\n",
      "1317 [D loss: 0.628553, acc: 59.38%] [G loss: 2.617262]\n",
      "1318 [D loss: 0.637575, acc: 57.81%] [G loss: 2.906308]\n",
      "1319 [D loss: 0.577207, acc: 70.31%] [G loss: 2.726008]\n",
      "1320 [D loss: 0.619401, acc: 53.12%] [G loss: 2.381766]\n",
      "1321 [D loss: 0.565113, acc: 73.44%] [G loss: 2.637668]\n",
      "1322 [D loss: 0.681611, acc: 62.50%] [G loss: 2.635662]\n",
      "1323 [D loss: 0.594023, acc: 68.75%] [G loss: 2.518736]\n",
      "1324 [D loss: 0.454989, acc: 82.81%] [G loss: 2.863468]\n",
      "1325 [D loss: 0.681431, acc: 59.38%] [G loss: 2.483197]\n",
      "1326 [D loss: 0.587621, acc: 68.75%] [G loss: 2.795418]\n",
      "1327 [D loss: 0.496443, acc: 76.56%] [G loss: 2.733258]\n",
      "1328 [D loss: 0.560529, acc: 70.31%] [G loss: 3.128451]\n",
      "1329 [D loss: 0.604837, acc: 64.06%] [G loss: 2.672751]\n",
      "1330 [D loss: 0.549963, acc: 70.31%] [G loss: 2.984494]\n",
      "1331 [D loss: 0.597285, acc: 65.62%] [G loss: 2.688120]\n",
      "1332 [D loss: 0.696728, acc: 59.38%] [G loss: 2.997218]\n",
      "1333 [D loss: 0.649267, acc: 70.31%] [G loss: 2.491830]\n",
      "1334 [D loss: 0.540305, acc: 73.44%] [G loss: 2.921378]\n",
      "1335 [D loss: 0.562253, acc: 67.19%] [G loss: 2.832517]\n",
      "1336 [D loss: 0.540317, acc: 70.31%] [G loss: 3.070145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1337 [D loss: 0.525431, acc: 73.44%] [G loss: 2.759533]\n",
      "1338 [D loss: 0.762407, acc: 48.44%] [G loss: 2.364902]\n",
      "1339 [D loss: 0.712198, acc: 60.94%] [G loss: 2.553585]\n",
      "1340 [D loss: 0.637168, acc: 68.75%] [G loss: 2.812963]\n",
      "1341 [D loss: 0.539008, acc: 70.31%] [G loss: 2.685045]\n",
      "1342 [D loss: 0.631792, acc: 54.69%] [G loss: 2.502464]\n",
      "1343 [D loss: 0.678346, acc: 48.44%] [G loss: 2.427691]\n",
      "1344 [D loss: 0.554602, acc: 68.75%] [G loss: 2.352800]\n",
      "1345 [D loss: 0.566754, acc: 71.88%] [G loss: 2.665856]\n",
      "1346 [D loss: 0.598624, acc: 62.50%] [G loss: 2.880019]\n",
      "1347 [D loss: 0.622700, acc: 60.94%] [G loss: 2.600010]\n",
      "1348 [D loss: 0.504611, acc: 79.69%] [G loss: 2.525847]\n",
      "1349 [D loss: 0.642503, acc: 60.94%] [G loss: 2.372295]\n",
      "1350 [D loss: 0.659768, acc: 67.19%] [G loss: 2.612552]\n",
      "1351 [D loss: 0.492270, acc: 79.69%] [G loss: 2.618181]\n",
      "1352 [D loss: 0.480834, acc: 81.25%] [G loss: 2.582860]\n",
      "1353 [D loss: 0.588942, acc: 68.75%] [G loss: 3.100073]\n",
      "1354 [D loss: 0.593110, acc: 67.19%] [G loss: 2.773881]\n",
      "1355 [D loss: 0.555556, acc: 75.00%] [G loss: 3.031241]\n",
      "1356 [D loss: 0.635390, acc: 65.62%] [G loss: 2.403220]\n",
      "1357 [D loss: 0.506576, acc: 76.56%] [G loss: 2.706607]\n",
      "1358 [D loss: 0.512492, acc: 73.44%] [G loss: 2.801999]\n",
      "1359 [D loss: 0.573964, acc: 70.31%] [G loss: 2.337342]\n",
      "1360 [D loss: 0.572651, acc: 75.00%] [G loss: 2.583365]\n",
      "1361 [D loss: 0.710830, acc: 54.69%] [G loss: 2.291989]\n",
      "1362 [D loss: 0.631274, acc: 64.06%] [G loss: 2.715087]\n",
      "1363 [D loss: 0.543588, acc: 71.88%] [G loss: 2.743706]\n",
      "1364 [D loss: 0.592358, acc: 67.19%] [G loss: 2.627272]\n",
      "1365 [D loss: 0.402012, acc: 87.50%] [G loss: 3.436469]\n",
      "1366 [D loss: 0.524502, acc: 73.44%] [G loss: 2.665029]\n",
      "1367 [D loss: 0.680452, acc: 57.81%] [G loss: 2.913403]\n",
      "1368 [D loss: 0.539605, acc: 75.00%] [G loss: 2.561800]\n",
      "1369 [D loss: 0.576253, acc: 68.75%] [G loss: 2.548123]\n",
      "1370 [D loss: 0.561659, acc: 65.62%] [G loss: 2.726606]\n",
      "1371 [D loss: 0.684340, acc: 70.31%] [G loss: 2.354681]\n",
      "1372 [D loss: 0.517586, acc: 70.31%] [G loss: 2.469637]\n",
      "1373 [D loss: 0.605722, acc: 60.94%] [G loss: 2.614301]\n",
      "1374 [D loss: 0.647274, acc: 64.06%] [G loss: 2.831061]\n",
      "1375 [D loss: 0.502259, acc: 75.00%] [G loss: 3.084127]\n",
      "1376 [D loss: 0.607166, acc: 65.62%] [G loss: 2.245013]\n",
      "1377 [D loss: 0.652779, acc: 60.94%] [G loss: 2.958960]\n",
      "1378 [D loss: 0.704497, acc: 56.25%] [G loss: 2.467833]\n",
      "1379 [D loss: 0.563849, acc: 65.62%] [G loss: 3.316601]\n",
      "1380 [D loss: 0.648217, acc: 68.75%] [G loss: 2.464931]\n",
      "1381 [D loss: 0.581597, acc: 68.75%] [G loss: 2.869783]\n",
      "1382 [D loss: 0.512644, acc: 78.12%] [G loss: 2.654996]\n",
      "1383 [D loss: 0.640427, acc: 62.50%] [G loss: 2.532697]\n",
      "1384 [D loss: 0.486469, acc: 73.44%] [G loss: 2.879131]\n",
      "1385 [D loss: 0.615302, acc: 64.06%] [G loss: 2.513554]\n",
      "1386 [D loss: 0.609530, acc: 68.75%] [G loss: 2.410518]\n",
      "1387 [D loss: 0.598216, acc: 68.75%] [G loss: 2.583894]\n",
      "1388 [D loss: 0.518212, acc: 71.88%] [G loss: 2.757975]\n",
      "1389 [D loss: 0.564512, acc: 65.62%] [G loss: 2.775535]\n",
      "1390 [D loss: 0.530426, acc: 75.00%] [G loss: 2.520677]\n",
      "1391 [D loss: 0.664961, acc: 65.62%] [G loss: 2.415372]\n",
      "1392 [D loss: 0.555161, acc: 75.00%] [G loss: 2.817208]\n",
      "1393 [D loss: 0.638728, acc: 67.19%] [G loss: 2.263348]\n",
      "1394 [D loss: 0.514430, acc: 68.75%] [G loss: 2.818449]\n",
      "1395 [D loss: 0.579004, acc: 71.88%] [G loss: 2.791825]\n",
      "1396 [D loss: 0.726831, acc: 64.06%] [G loss: 2.929986]\n",
      "1397 [D loss: 0.537323, acc: 71.88%] [G loss: 2.650115]\n",
      "1398 [D loss: 0.498677, acc: 75.00%] [G loss: 2.885584]\n",
      "1399 [D loss: 0.561596, acc: 68.75%] [G loss: 2.630623]\n",
      "1400 [D loss: 0.615941, acc: 67.19%] [G loss: 2.562870]\n",
      "1401 [D loss: 0.565300, acc: 68.75%] [G loss: 2.632674]\n",
      "1402 [D loss: 0.504818, acc: 75.00%] [G loss: 2.523652]\n",
      "1403 [D loss: 0.565763, acc: 64.06%] [G loss: 2.842527]\n",
      "1404 [D loss: 0.522286, acc: 68.75%] [G loss: 2.915178]\n",
      "1405 [D loss: 0.633267, acc: 65.62%] [G loss: 2.821150]\n",
      "1406 [D loss: 0.507358, acc: 79.69%] [G loss: 2.573448]\n",
      "1407 [D loss: 0.464196, acc: 84.38%] [G loss: 2.916111]\n",
      "1408 [D loss: 0.718592, acc: 59.38%] [G loss: 3.053189]\n",
      "1409 [D loss: 0.498837, acc: 71.88%] [G loss: 2.689819]\n",
      "1410 [D loss: 0.438052, acc: 75.00%] [G loss: 2.902891]\n",
      "1411 [D loss: 0.764428, acc: 57.81%] [G loss: 3.105608]\n",
      "1412 [D loss: 0.706443, acc: 53.12%] [G loss: 2.976950]\n",
      "1413 [D loss: 0.468263, acc: 81.25%] [G loss: 2.855267]\n",
      "1414 [D loss: 0.573074, acc: 65.62%] [G loss: 2.640308]\n",
      "1415 [D loss: 0.550843, acc: 75.00%] [G loss: 2.700486]\n",
      "1416 [D loss: 0.586518, acc: 64.06%] [G loss: 2.981942]\n",
      "1417 [D loss: 0.617524, acc: 60.94%] [G loss: 2.434240]\n",
      "1418 [D loss: 0.597489, acc: 65.62%] [G loss: 2.442837]\n",
      "1419 [D loss: 0.633237, acc: 67.19%] [G loss: 3.086873]\n",
      "1420 [D loss: 0.481093, acc: 71.88%] [G loss: 2.764846]\n",
      "1421 [D loss: 0.560244, acc: 76.56%] [G loss: 3.026265]\n",
      "1422 [D loss: 0.555699, acc: 67.19%] [G loss: 2.866459]\n",
      "1423 [D loss: 0.576518, acc: 73.44%] [G loss: 2.762139]\n",
      "1424 [D loss: 0.735535, acc: 57.81%] [G loss: 2.496727]\n",
      "1425 [D loss: 0.552978, acc: 73.44%] [G loss: 3.250984]\n",
      "1426 [D loss: 0.614593, acc: 62.50%] [G loss: 2.613133]\n",
      "1427 [D loss: 0.592504, acc: 68.75%] [G loss: 2.759385]\n",
      "1428 [D loss: 0.639818, acc: 59.38%] [G loss: 2.498164]\n",
      "1429 [D loss: 0.531486, acc: 71.88%] [G loss: 2.753398]\n",
      "1430 [D loss: 0.682680, acc: 57.81%] [G loss: 2.634020]\n",
      "1431 [D loss: 0.597937, acc: 68.75%] [G loss: 2.646879]\n",
      "1432 [D loss: 0.577682, acc: 64.06%] [G loss: 2.719696]\n",
      "1433 [D loss: 0.629395, acc: 67.19%] [G loss: 2.783603]\n",
      "1434 [D loss: 0.638946, acc: 59.38%] [G loss: 2.422149]\n",
      "1435 [D loss: 0.460572, acc: 81.25%] [G loss: 2.847971]\n",
      "1436 [D loss: 0.717648, acc: 53.12%] [G loss: 2.559309]\n",
      "1437 [D loss: 0.535517, acc: 71.88%] [G loss: 2.439184]\n",
      "1438 [D loss: 0.467280, acc: 78.12%] [G loss: 2.457532]\n",
      "1439 [D loss: 0.470126, acc: 79.69%] [G loss: 2.503013]\n",
      "1440 [D loss: 0.557975, acc: 68.75%] [G loss: 2.755517]\n",
      "1441 [D loss: 0.528763, acc: 71.88%] [G loss: 2.979821]\n",
      "1442 [D loss: 0.608756, acc: 67.19%] [G loss: 2.747879]\n",
      "1443 [D loss: 0.547584, acc: 70.31%] [G loss: 2.571916]\n",
      "1444 [D loss: 0.536734, acc: 75.00%] [G loss: 2.960822]\n",
      "1445 [D loss: 0.505914, acc: 70.31%] [G loss: 3.121354]\n",
      "1446 [D loss: 0.749959, acc: 53.12%] [G loss: 3.200581]\n",
      "1447 [D loss: 0.495895, acc: 76.56%] [G loss: 2.887027]\n",
      "1448 [D loss: 0.553869, acc: 68.75%] [G loss: 3.098300]\n",
      "1449 [D loss: 0.495582, acc: 81.25%] [G loss: 2.563847]\n",
      "1450 [D loss: 0.659228, acc: 60.94%] [G loss: 2.805325]\n",
      "1451 [D loss: 0.549765, acc: 70.31%] [G loss: 2.702891]\n",
      "1452 [D loss: 0.489373, acc: 73.44%] [G loss: 3.216069]\n",
      "1453 [D loss: 0.696387, acc: 65.62%] [G loss: 3.052434]\n",
      "1454 [D loss: 0.557759, acc: 73.44%] [G loss: 2.931035]\n",
      "1455 [D loss: 0.606756, acc: 71.88%] [G loss: 2.506329]\n",
      "1456 [D loss: 0.634184, acc: 56.25%] [G loss: 3.338451]\n",
      "1457 [D loss: 0.563666, acc: 68.75%] [G loss: 2.769023]\n",
      "1458 [D loss: 0.596134, acc: 67.19%] [G loss: 2.971194]\n",
      "1459 [D loss: 0.653592, acc: 64.06%] [G loss: 2.678121]\n",
      "1460 [D loss: 0.564525, acc: 75.00%] [G loss: 2.994578]\n",
      "1461 [D loss: 0.494906, acc: 73.44%] [G loss: 2.826240]\n",
      "1462 [D loss: 0.535435, acc: 78.12%] [G loss: 2.803540]\n",
      "1463 [D loss: 0.599042, acc: 59.38%] [G loss: 3.018702]\n",
      "1464 [D loss: 0.612795, acc: 70.31%] [G loss: 3.075574]\n",
      "1465 [D loss: 0.656114, acc: 64.06%] [G loss: 2.702946]\n",
      "1466 [D loss: 0.572024, acc: 65.62%] [G loss: 2.898330]\n",
      "1467 [D loss: 0.552866, acc: 67.19%] [G loss: 2.819046]\n",
      "1468 [D loss: 0.642876, acc: 67.19%] [G loss: 2.693293]\n",
      "1469 [D loss: 0.581581, acc: 71.88%] [G loss: 2.759037]\n",
      "1470 [D loss: 0.547334, acc: 73.44%] [G loss: 2.758515]\n",
      "1471 [D loss: 0.600152, acc: 78.12%] [G loss: 3.000058]\n",
      "1472 [D loss: 0.683130, acc: 57.81%] [G loss: 3.081297]\n",
      "1473 [D loss: 0.491305, acc: 81.25%] [G loss: 2.893736]\n",
      "1474 [D loss: 0.472131, acc: 76.56%] [G loss: 3.097235]\n",
      "1475 [D loss: 0.693766, acc: 62.50%] [G loss: 2.768868]\n",
      "1476 [D loss: 0.606327, acc: 68.75%] [G loss: 2.660820]\n",
      "1477 [D loss: 0.579822, acc: 75.00%] [G loss: 2.701444]\n",
      "1478 [D loss: 0.538261, acc: 76.56%] [G loss: 2.975784]\n",
      "1479 [D loss: 0.739015, acc: 50.00%] [G loss: 2.460898]\n",
      "1480 [D loss: 0.574103, acc: 70.31%] [G loss: 2.975272]\n",
      "1481 [D loss: 0.573512, acc: 73.44%] [G loss: 2.722169]\n",
      "1482 [D loss: 0.455407, acc: 75.00%] [G loss: 3.020949]\n",
      "1483 [D loss: 0.552104, acc: 73.44%] [G loss: 2.617270]\n",
      "1484 [D loss: 0.564825, acc: 70.31%] [G loss: 2.809752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1485 [D loss: 0.590946, acc: 67.19%] [G loss: 2.824907]\n",
      "1486 [D loss: 0.520438, acc: 71.88%] [G loss: 2.821422]\n",
      "1487 [D loss: 0.620692, acc: 70.31%] [G loss: 2.617889]\n",
      "1488 [D loss: 0.647348, acc: 57.81%] [G loss: 2.544348]\n",
      "1489 [D loss: 0.503395, acc: 73.44%] [G loss: 2.890251]\n",
      "1490 [D loss: 0.512752, acc: 78.12%] [G loss: 2.832696]\n",
      "1491 [D loss: 0.524353, acc: 68.75%] [G loss: 3.208224]\n",
      "1492 [D loss: 0.732687, acc: 64.06%] [G loss: 2.823287]\n",
      "1493 [D loss: 0.487255, acc: 76.56%] [G loss: 2.552429]\n",
      "1494 [D loss: 0.694256, acc: 62.50%] [G loss: 2.826921]\n",
      "1495 [D loss: 0.638325, acc: 67.19%] [G loss: 2.554887]\n",
      "1496 [D loss: 0.638508, acc: 65.62%] [G loss: 2.826968]\n",
      "1497 [D loss: 0.699567, acc: 57.81%] [G loss: 2.530822]\n",
      "1498 [D loss: 0.551792, acc: 71.88%] [G loss: 2.976685]\n",
      "1499 [D loss: 0.602603, acc: 67.19%] [G loss: 2.334286]\n",
      "1500 [D loss: 0.589691, acc: 67.19%] [G loss: 2.960347]\n",
      "1501 [D loss: 0.671550, acc: 62.50%] [G loss: 2.550368]\n",
      "1502 [D loss: 0.676243, acc: 57.81%] [G loss: 2.594738]\n",
      "1503 [D loss: 0.672159, acc: 59.38%] [G loss: 2.553720]\n",
      "1504 [D loss: 0.521649, acc: 70.31%] [G loss: 2.215865]\n",
      "1505 [D loss: 0.586776, acc: 70.31%] [G loss: 2.483488]\n",
      "1506 [D loss: 0.534576, acc: 71.88%] [G loss: 2.710351]\n",
      "1507 [D loss: 0.625815, acc: 67.19%] [G loss: 2.381577]\n",
      "1508 [D loss: 0.533086, acc: 70.31%] [G loss: 2.752829]\n",
      "1509 [D loss: 0.518804, acc: 82.81%] [G loss: 2.375490]\n",
      "1510 [D loss: 0.617734, acc: 65.62%] [G loss: 2.694530]\n",
      "1511 [D loss: 0.693184, acc: 64.06%] [G loss: 2.511697]\n",
      "1512 [D loss: 0.564641, acc: 65.62%] [G loss: 2.788847]\n",
      "1513 [D loss: 0.608730, acc: 65.62%] [G loss: 2.467533]\n",
      "1514 [D loss: 0.618534, acc: 65.62%] [G loss: 2.814444]\n",
      "1515 [D loss: 0.700770, acc: 54.69%] [G loss: 2.618445]\n",
      "1516 [D loss: 0.686026, acc: 65.62%] [G loss: 2.498041]\n",
      "1517 [D loss: 0.647812, acc: 67.19%] [G loss: 2.822504]\n",
      "1518 [D loss: 0.607759, acc: 67.19%] [G loss: 2.781851]\n",
      "1519 [D loss: 0.556699, acc: 73.44%] [G loss: 2.345220]\n",
      "1520 [D loss: 0.688316, acc: 53.12%] [G loss: 2.687419]\n",
      "1521 [D loss: 0.536803, acc: 73.44%] [G loss: 2.690937]\n",
      "1522 [D loss: 0.650500, acc: 65.62%] [G loss: 2.724348]\n",
      "1523 [D loss: 0.585346, acc: 67.19%] [G loss: 2.593836]\n",
      "1524 [D loss: 0.558672, acc: 71.88%] [G loss: 2.607993]\n",
      "1525 [D loss: 0.603759, acc: 67.19%] [G loss: 2.545682]\n",
      "1526 [D loss: 0.684806, acc: 64.06%] [G loss: 2.405086]\n",
      "1527 [D loss: 0.592182, acc: 68.75%] [G loss: 2.545245]\n",
      "1528 [D loss: 0.849305, acc: 56.25%] [G loss: 2.260050]\n",
      "1529 [D loss: 0.604451, acc: 71.88%] [G loss: 2.250489]\n",
      "1530 [D loss: 0.623951, acc: 64.06%] [G loss: 2.512518]\n",
      "1531 [D loss: 0.551419, acc: 75.00%] [G loss: 2.454121]\n",
      "1532 [D loss: 0.643785, acc: 60.94%] [G loss: 2.639167]\n",
      "1533 [D loss: 0.649286, acc: 62.50%] [G loss: 2.583060]\n",
      "1534 [D loss: 0.657798, acc: 59.38%] [G loss: 2.508352]\n",
      "1535 [D loss: 0.642596, acc: 62.50%] [G loss: 2.209118]\n",
      "1536 [D loss: 0.668584, acc: 59.38%] [G loss: 2.452160]\n",
      "1537 [D loss: 0.813117, acc: 48.44%] [G loss: 2.265096]\n",
      "1538 [D loss: 0.589735, acc: 65.62%] [G loss: 2.790401]\n",
      "1539 [D loss: 0.771112, acc: 50.00%] [G loss: 2.491087]\n",
      "1540 [D loss: 0.537002, acc: 75.00%] [G loss: 2.595368]\n",
      "1541 [D loss: 0.675915, acc: 59.38%] [G loss: 2.234223]\n",
      "1542 [D loss: 0.623596, acc: 56.25%] [G loss: 2.341613]\n",
      "1543 [D loss: 0.721464, acc: 64.06%] [G loss: 2.133876]\n",
      "1544 [D loss: 0.585006, acc: 70.31%] [G loss: 2.213617]\n",
      "1545 [D loss: 0.715910, acc: 54.69%] [G loss: 2.614450]\n",
      "1546 [D loss: 0.581393, acc: 71.88%] [G loss: 2.672041]\n",
      "1547 [D loss: 0.645829, acc: 65.62%] [G loss: 2.745071]\n",
      "1548 [D loss: 0.607289, acc: 65.62%] [G loss: 2.973408]\n",
      "1549 [D loss: 0.756494, acc: 57.81%] [G loss: 2.451067]\n",
      "1550 [D loss: 0.679443, acc: 64.06%] [G loss: 2.581452]\n",
      "1551 [D loss: 0.639873, acc: 65.62%] [G loss: 2.235793]\n",
      "1552 [D loss: 0.642182, acc: 67.19%] [G loss: 2.409647]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-bec328bf77ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# epochs=40000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbigan_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-4b3570aafb0e>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(G, D, encoder, bigan_generator, latent_dim, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# Train the discriminator (img -> z is valid, z -> img is fake)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0md_loss_real\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0md_loss_fake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimgs_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0md_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_loss_real\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3476\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mc:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs=4000\n",
    "# epochs=40000\n",
    "train(G, D, encoder, bigan_generator, latent_dim, epochs=epochs, batch_size=32, sample_interval=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
