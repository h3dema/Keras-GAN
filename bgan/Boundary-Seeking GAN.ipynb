{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boundary-Seeking Generative Adversarial Networks\n",
    "### BGAN\n",
    "\n",
    "Ref. HJELM, R. Devon et al. Boundary-seeking generative adversarial networks. \n",
    "     arXiv preprint arXiv:1702.08431, 2017.\n",
    "     https://arxiv.org/abs/1702.08431"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(latent_dim, img_shape):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(256, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
    "    model.add(Reshape(img_shape))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "    img = model(noise)\n",
    "\n",
    "    return Model(noise, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(img_shape):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Flatten(input_shape=img_shape))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "    validity = model(img)\n",
    "\n",
    "    return Model(img, validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Boundary seeking loss.\n",
    "    Reference: https://wiseodd.github.io/techblog/2017/03/07/boundary-seeking-gan/\n",
    "    \"\"\"\n",
    "    return 0.5 * K.mean((K.log(y_pred) - K.log(1 - y_pred))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(epoch, latent_dim, G):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
    "    gen_imgs = G.predict(noise)\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(G, D, combined, latent_dim, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "    # Load the dataset\n",
    "    (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "    # Rescale -1 to 1\n",
    "    X_train = X_train / 127.5 - 1.\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "    # Adversarial ground truths\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        # Select a random batch of images\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        imgs = X_train[idx]\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "\n",
    "        # Generate a batch of new images\n",
    "        gen_imgs = G.predict(noise)\n",
    "\n",
    "        # Train the discriminator\n",
    "        d_loss_real = D.train_on_batch(imgs, valid)\n",
    "        d_loss_fake = D.train_on_batch(gen_imgs, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "\n",
    "        g_loss = combined.train_on_batch(noise, valid)\n",
    "\n",
    "        # Plot the progress\n",
    "        print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % sample_interval == 0:\n",
    "            sample_images(epoch, latent_dim, G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# create optimizer\n",
    "optimizer = Adam(0.0002, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Build and compile the discriminator\n",
    "D = build_discriminator(img_shape)\n",
    "D.compile(loss='binary_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the generator\n",
    "G = build_generator(latent_dim, img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generator takes noise as input and generated imgs\n",
    "z = Input(shape=(latent_dim,))\n",
    "img = G(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the combined model we will only train the generator\n",
    "D.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The valid takes generated images as input and determines validity\n",
    "valid = D(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The combined model  (stacked generator and discriminator)\n",
    "# Trains the generator to fool the discriminator\n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss=boundary_loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.782158, acc.: 35.94%] [G loss: 0.097417]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 0.399313, acc.: 81.25%] [G loss: 0.134174]\n",
      "2 [D loss: 0.336565, acc.: 92.19%] [G loss: 0.160209]\n",
      "3 [D loss: 0.351600, acc.: 75.00%] [G loss: 0.146210]\n",
      "4 [D loss: 0.303937, acc.: 92.19%] [G loss: 0.293512]\n",
      "5 [D loss: 0.296092, acc.: 90.62%] [G loss: 0.314436]\n",
      "6 [D loss: 0.244841, acc.: 100.00%] [G loss: 0.579249]\n",
      "7 [D loss: 0.206001, acc.: 100.00%] [G loss: 0.801301]\n",
      "8 [D loss: 0.174973, acc.: 100.00%] [G loss: 1.150580]\n",
      "9 [D loss: 0.160552, acc.: 100.00%] [G loss: 1.411719]\n",
      "10 [D loss: 0.126105, acc.: 100.00%] [G loss: 1.551523]\n",
      "11 [D loss: 0.121308, acc.: 100.00%] [G loss: 2.101370]\n",
      "12 [D loss: 0.101214, acc.: 100.00%] [G loss: 2.141961]\n",
      "13 [D loss: 0.084514, acc.: 100.00%] [G loss: 2.587679]\n",
      "14 [D loss: 0.082388, acc.: 100.00%] [G loss: 2.711448]\n",
      "15 [D loss: 0.075534, acc.: 100.00%] [G loss: 2.615901]\n",
      "16 [D loss: 0.064850, acc.: 100.00%] [G loss: 2.984964]\n",
      "17 [D loss: 0.074986, acc.: 100.00%] [G loss: 2.957314]\n",
      "18 [D loss: 0.052110, acc.: 100.00%] [G loss: 3.321602]\n",
      "19 [D loss: 0.052506, acc.: 100.00%] [G loss: 3.783040]\n",
      "20 [D loss: 0.053844, acc.: 100.00%] [G loss: 3.802474]\n",
      "21 [D loss: 0.043852, acc.: 100.00%] [G loss: 4.035452]\n",
      "22 [D loss: 0.044814, acc.: 100.00%] [G loss: 3.973818]\n",
      "23 [D loss: 0.046167, acc.: 100.00%] [G loss: 4.318154]\n",
      "24 [D loss: 0.040286, acc.: 100.00%] [G loss: 4.560961]\n",
      "25 [D loss: 0.038234, acc.: 100.00%] [G loss: 4.637583]\n",
      "26 [D loss: 0.030171, acc.: 100.00%] [G loss: 5.045265]\n",
      "27 [D loss: 0.032963, acc.: 100.00%] [G loss: 4.784357]\n",
      "28 [D loss: 0.033127, acc.: 100.00%] [G loss: 5.516167]\n",
      "29 [D loss: 0.033646, acc.: 100.00%] [G loss: 5.343327]\n",
      "30 [D loss: 0.023189, acc.: 100.00%] [G loss: 5.918276]\n",
      "31 [D loss: 0.035252, acc.: 100.00%] [G loss: 5.503857]\n",
      "32 [D loss: 0.019851, acc.: 100.00%] [G loss: 5.974209]\n",
      "33 [D loss: 0.026539, acc.: 100.00%] [G loss: 5.851301]\n",
      "34 [D loss: 0.028812, acc.: 100.00%] [G loss: 5.806625]\n",
      "35 [D loss: 0.023085, acc.: 100.00%] [G loss: 6.518964]\n",
      "36 [D loss: 0.021179, acc.: 100.00%] [G loss: 6.022921]\n",
      "37 [D loss: 0.022034, acc.: 100.00%] [G loss: 6.597759]\n",
      "38 [D loss: 0.024386, acc.: 100.00%] [G loss: 6.103551]\n",
      "39 [D loss: 0.020764, acc.: 100.00%] [G loss: 6.823599]\n",
      "40 [D loss: 0.017877, acc.: 100.00%] [G loss: 6.722742]\n",
      "41 [D loss: 0.018794, acc.: 100.00%] [G loss: 7.355837]\n",
      "42 [D loss: 0.021589, acc.: 100.00%] [G loss: 6.614923]\n",
      "43 [D loss: 0.026046, acc.: 100.00%] [G loss: 7.003388]\n",
      "44 [D loss: 0.016824, acc.: 100.00%] [G loss: 7.537347]\n",
      "45 [D loss: 0.018065, acc.: 100.00%] [G loss: 7.564662]\n",
      "46 [D loss: 0.021443, acc.: 100.00%] [G loss: 7.527894]\n",
      "47 [D loss: 0.020259, acc.: 100.00%] [G loss: 7.101048]\n",
      "48 [D loss: 0.016579, acc.: 100.00%] [G loss: 7.886468]\n",
      "49 [D loss: 0.014994, acc.: 100.00%] [G loss: 7.816776]\n",
      "50 [D loss: 0.016449, acc.: 100.00%] [G loss: 7.655493]\n",
      "51 [D loss: 0.017012, acc.: 100.00%] [G loss: 7.530135]\n",
      "52 [D loss: 0.015050, acc.: 100.00%] [G loss: 8.441214]\n",
      "53 [D loss: 0.013364, acc.: 100.00%] [G loss: 8.489429]\n",
      "54 [D loss: 0.019826, acc.: 100.00%] [G loss: 8.496405]\n",
      "55 [D loss: 0.016940, acc.: 100.00%] [G loss: 8.244938]\n",
      "56 [D loss: 0.021281, acc.: 100.00%] [G loss: 8.017836]\n",
      "57 [D loss: 0.012948, acc.: 100.00%] [G loss: 8.747339]\n",
      "58 [D loss: 0.012203, acc.: 100.00%] [G loss: 8.413901]\n",
      "59 [D loss: 0.016023, acc.: 100.00%] [G loss: 9.270609]\n",
      "60 [D loss: 0.017303, acc.: 100.00%] [G loss: 9.130947]\n",
      "61 [D loss: 0.018944, acc.: 100.00%] [G loss: 9.036597]\n",
      "62 [D loss: 0.012798, acc.: 100.00%] [G loss: 8.737789]\n",
      "63 [D loss: 0.018691, acc.: 100.00%] [G loss: 8.940077]\n",
      "64 [D loss: 0.015712, acc.: 100.00%] [G loss: 10.135054]\n",
      "65 [D loss: 0.012438, acc.: 100.00%] [G loss: 9.238020]\n",
      "66 [D loss: 0.012110, acc.: 100.00%] [G loss: 9.652466]\n",
      "67 [D loss: 0.012178, acc.: 100.00%] [G loss: 9.458008]\n",
      "68 [D loss: 0.014009, acc.: 100.00%] [G loss: 10.021704]\n",
      "69 [D loss: 0.014988, acc.: 100.00%] [G loss: 9.372328]\n",
      "70 [D loss: 0.015953, acc.: 100.00%] [G loss: 9.572781]\n",
      "71 [D loss: 0.015818, acc.: 100.00%] [G loss: 10.077752]\n",
      "72 [D loss: 0.009694, acc.: 100.00%] [G loss: 9.297288]\n",
      "73 [D loss: 0.010829, acc.: 100.00%] [G loss: 9.212664]\n",
      "74 [D loss: 0.017990, acc.: 100.00%] [G loss: 10.300663]\n",
      "75 [D loss: 0.011516, acc.: 100.00%] [G loss: 9.827273]\n",
      "76 [D loss: 0.011867, acc.: 100.00%] [G loss: 10.209408]\n",
      "77 [D loss: 0.011278, acc.: 100.00%] [G loss: 10.266155]\n",
      "78 [D loss: 0.010730, acc.: 100.00%] [G loss: 10.644885]\n",
      "79 [D loss: 0.011179, acc.: 100.00%] [G loss: 10.278938]\n",
      "80 [D loss: 0.009702, acc.: 100.00%] [G loss: 10.743679]\n",
      "81 [D loss: 0.010379, acc.: 100.00%] [G loss: 9.744990]\n",
      "82 [D loss: 0.014888, acc.: 100.00%] [G loss: 9.786741]\n",
      "83 [D loss: 0.011045, acc.: 100.00%] [G loss: 10.273764]\n",
      "84 [D loss: 0.013935, acc.: 100.00%] [G loss: 10.459539]\n",
      "85 [D loss: 0.016992, acc.: 100.00%] [G loss: 10.367231]\n",
      "86 [D loss: 0.007728, acc.: 100.00%] [G loss: 10.365275]\n",
      "87 [D loss: 0.011655, acc.: 100.00%] [G loss: 11.168174]\n",
      "88 [D loss: 0.018150, acc.: 100.00%] [G loss: 10.335861]\n",
      "89 [D loss: 0.012080, acc.: 100.00%] [G loss: 11.208858]\n",
      "90 [D loss: 0.010254, acc.: 100.00%] [G loss: 11.298365]\n",
      "91 [D loss: 0.014102, acc.: 100.00%] [G loss: 12.002075]\n",
      "92 [D loss: 0.008552, acc.: 100.00%] [G loss: 12.023780]\n",
      "93 [D loss: 0.010874, acc.: 100.00%] [G loss: 11.953854]\n",
      "94 [D loss: 0.009090, acc.: 100.00%] [G loss: 10.797733]\n",
      "95 [D loss: 0.012373, acc.: 100.00%] [G loss: 11.337392]\n",
      "96 [D loss: 0.015592, acc.: 100.00%] [G loss: 11.208616]\n",
      "97 [D loss: 0.016696, acc.: 100.00%] [G loss: 11.104389]\n",
      "98 [D loss: 0.010469, acc.: 100.00%] [G loss: 11.875347]\n",
      "99 [D loss: 0.011648, acc.: 100.00%] [G loss: 11.516505]\n",
      "100 [D loss: 0.011767, acc.: 100.00%] [G loss: 12.285606]\n",
      "101 [D loss: 0.010328, acc.: 100.00%] [G loss: 12.278283]\n",
      "102 [D loss: 0.021856, acc.: 100.00%] [G loss: 11.553396]\n",
      "103 [D loss: 0.010055, acc.: 100.00%] [G loss: 11.488790]\n",
      "104 [D loss: 0.012373, acc.: 100.00%] [G loss: 11.359900]\n",
      "105 [D loss: 0.022214, acc.: 100.00%] [G loss: 13.951878]\n",
      "106 [D loss: 0.016783, acc.: 100.00%] [G loss: 11.592185]\n",
      "107 [D loss: 0.014666, acc.: 100.00%] [G loss: 12.717324]\n",
      "108 [D loss: 0.019118, acc.: 100.00%] [G loss: 12.401684]\n",
      "109 [D loss: 0.015036, acc.: 100.00%] [G loss: 11.514698]\n",
      "110 [D loss: 0.044848, acc.: 98.44%] [G loss: 16.651501]\n",
      "111 [D loss: 0.338031, acc.: 89.06%] [G loss: 10.077261]\n",
      "112 [D loss: 0.019896, acc.: 100.00%] [G loss: 14.891320]\n",
      "113 [D loss: 0.284137, acc.: 85.94%] [G loss: 14.455707]\n",
      "114 [D loss: 0.019478, acc.: 100.00%] [G loss: 17.575241]\n",
      "115 [D loss: 0.804737, acc.: 71.88%] [G loss: inf]\n",
      "116 [D loss: nan, acc.: 50.00%] [G loss: nan]\n",
      "117 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "118 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "119 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "120 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "121 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "122 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "123 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "124 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "125 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "126 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "127 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "128 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "129 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "130 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "131 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "132 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "133 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "134 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "135 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "136 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "137 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "138 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "139 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "140 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "141 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "142 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "143 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "144 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "145 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "146 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "147 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "148 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "149 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "150 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "151 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "152 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "153 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "154 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "155 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "156 [D loss: nan, acc.: 0.00%] [G loss: nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "158 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "159 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "160 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "161 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "162 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "163 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "164 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "165 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "166 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "167 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "168 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "169 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "170 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "171 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "172 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "173 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "174 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "175 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "176 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "177 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "178 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "179 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "180 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "181 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "182 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "183 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "184 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "185 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "186 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "187 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "188 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "189 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "190 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "191 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "192 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "193 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "194 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "195 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "196 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "197 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "198 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "199 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "200 [D loss: nan, acc.: 0.00%] [G loss: nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\matplotlib\\image.py:397: UserWarning: Warning: converting a masked element to nan.\n",
      "  dv = (np.float64(self.norm.vmax) -\n",
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\matplotlib\\image.py:398: UserWarning: Warning: converting a masked element to nan.\n",
      "  np.float64(self.norm.vmin))\n",
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\matplotlib\\image.py:405: UserWarning: Warning: converting a masked element to nan.\n",
      "  a_min = np.float64(newmin)\n",
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\matplotlib\\image.py:410: UserWarning: Warning: converting a masked element to nan.\n",
      "  a_max = np.float64(newmax)\n",
      "<string>:6: UserWarning: Warning: converting a masked element to nan.\n",
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\ma\\core.py:722: UserWarning: Warning: converting a masked element to nan.\n",
      "  data = np.array(a, copy=False, subok=subok)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "202 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "203 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "204 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "205 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "206 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "207 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "208 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "209 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "210 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "211 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "212 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "213 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "214 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "215 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "216 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "217 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "218 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "219 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "220 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "221 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "222 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "223 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "224 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "225 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "226 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "227 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "228 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "229 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "230 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "231 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "232 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "233 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "234 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "235 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "236 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "237 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "238 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "239 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "240 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "241 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "242 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "243 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "244 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "245 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "246 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "247 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "248 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "249 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "250 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "251 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "252 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "253 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "254 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "255 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "256 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "257 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "258 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "259 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "260 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "261 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "262 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "263 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "264 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "265 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "266 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "267 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "268 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "269 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "270 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "271 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "272 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "273 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "274 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "275 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "276 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "277 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "278 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "279 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "280 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "281 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "282 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "283 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "284 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "285 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "286 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "287 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "288 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "289 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "290 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "291 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "292 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "293 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "294 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "295 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "296 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "297 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "298 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "299 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "300 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "301 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "302 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "303 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "304 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "305 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "306 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "307 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "308 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "309 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "310 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "311 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "312 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "313 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "314 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "315 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "316 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "317 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "318 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "319 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "320 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "321 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "322 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "323 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "324 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "325 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "326 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "327 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "328 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "329 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "330 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "331 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "332 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "333 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "334 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "335 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "336 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "337 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "338 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "339 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "340 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "341 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "342 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "343 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "344 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "345 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "346 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "347 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "348 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "349 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "350 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "351 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "352 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "353 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "354 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "355 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "356 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "357 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "358 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "359 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "360 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "361 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "362 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "363 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "364 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "365 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "366 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "367 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "368 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "369 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "370 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "371 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "372 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "373 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "374 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "375 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "376 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "377 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "378 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "379 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "380 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "381 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "382 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "383 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "384 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "385 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "386 [D loss: nan, acc.: 0.00%] [G loss: nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "388 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "389 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "390 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "391 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "392 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "393 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "394 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "395 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "396 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "397 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "398 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "399 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "400 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "401 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "402 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "403 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "404 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "405 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "406 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "407 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "408 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "409 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "410 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "411 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "412 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "413 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "414 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "415 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "416 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "417 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "418 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "419 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "420 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "421 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "422 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "423 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "424 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "425 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "426 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "427 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "428 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "429 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "430 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "431 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "432 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "433 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "434 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "435 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "436 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "437 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "438 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "439 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "440 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "441 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "442 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "443 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "444 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "445 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "446 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "447 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "448 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "449 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "450 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "451 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "452 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "453 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "454 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "455 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "456 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "457 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "458 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "459 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "460 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "461 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "462 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "463 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "464 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "465 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "466 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "467 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "468 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "469 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "470 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "471 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "472 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "473 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "474 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "475 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "476 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "477 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "478 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "479 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "480 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "481 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "482 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "483 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "484 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "485 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "486 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "487 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "488 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "489 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "490 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "491 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "492 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "493 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "494 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "495 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "496 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "497 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "498 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "499 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "500 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "501 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "502 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "503 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "504 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "505 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "506 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "507 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "508 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "509 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "510 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "511 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "512 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "513 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "514 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "515 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "516 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "517 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "518 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "519 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "520 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "521 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "522 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "523 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "524 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "525 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "526 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "527 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "528 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "529 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "530 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "531 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "532 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "533 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "534 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "535 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "536 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "537 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "538 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "539 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "540 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "541 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "542 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "543 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "544 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "545 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "546 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "547 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "548 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "549 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "550 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "551 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "552 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "553 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "554 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "555 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "556 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "557 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "558 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "559 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "560 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "561 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "562 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "563 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "564 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "565 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "566 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "567 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "568 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "569 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "570 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "571 [D loss: nan, acc.: 0.00%] [G loss: nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "573 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "574 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "575 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "576 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "577 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "578 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "579 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "580 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "581 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "582 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "583 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "584 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "585 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "586 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "587 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "588 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "589 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "590 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "591 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "592 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "593 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "594 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "595 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "596 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "597 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "598 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "599 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "600 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "601 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "602 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "603 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "604 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "605 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "606 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "607 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "608 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "609 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "610 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "611 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "612 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "613 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "614 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "615 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "616 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "617 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "618 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "619 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "620 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "621 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "622 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "623 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "624 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "625 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "626 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "627 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "628 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "629 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "630 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "631 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "632 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "633 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "634 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "635 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "636 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "637 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "638 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "639 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "640 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "641 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "642 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "643 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "644 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "645 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "646 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "647 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "648 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "649 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "650 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "651 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "652 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "653 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "654 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "655 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "656 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "657 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "658 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "659 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "660 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "661 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "662 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "663 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "664 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "665 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "666 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "667 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "668 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "669 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "670 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "671 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "672 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "673 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "674 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "675 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "676 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "677 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "678 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "679 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "680 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "681 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "682 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "683 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "684 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "685 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "686 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "687 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "688 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "689 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "690 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "691 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "692 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "693 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "694 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "695 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "696 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "697 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "698 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "699 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "700 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "701 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "702 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "703 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "704 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "705 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "706 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "707 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "708 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "709 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "710 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "711 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "712 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "713 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "714 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "715 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "716 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "717 [D loss: nan, acc.: 0.00%] [G loss: nan]\n"
     ]
    }
   ],
   "source": [
    "epochs=3000\n",
    "# epochs=30000\n",
    "train(G, D, combined, latent_dim, epochs=epochs, batch_size=32, sample_interval=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
