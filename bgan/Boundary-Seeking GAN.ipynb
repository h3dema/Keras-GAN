{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boundary-Seeking Generative Adversarial Networks\n",
    "### BGAN\n",
    "\n",
    "Ref. HJELM, R. Devon et al. Boundary-seeking generative adversarial networks. \n",
    "     arXiv preprint arXiv:1702.08431, 2017.\n",
    "     https://arxiv.org/abs/1702.08431"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(latent_dim, img_shape):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(256, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
    "    model.add(Reshape(img_shape))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "    img = model(noise)\n",
    "\n",
    "    return Model(noise, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(img_shape):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Flatten(input_shape=img_shape))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "    validity = model(img)\n",
    "\n",
    "    return Model(img, validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Boundary seeking loss.\n",
    "    Reference: https://wiseodd.github.io/techblog/2017/03/07/boundary-seeking-gan/\n",
    "    \"\"\"\n",
    "    return 0.5 * K.mean((K.log(y_pred) - K.log(1 - y_pred))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(epoch, latent_dim, G):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
    "    gen_imgs = G.predict(noise)\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(G, D, combined, latent_dim, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "    # Load the dataset\n",
    "    (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "    # Rescale -1 to 1\n",
    "    X_train = X_train / 127.5 - 1.\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "    # Adversarial ground truths\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        # Select a random batch of images\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        imgs = X_train[idx]\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "\n",
    "        # Generate a batch of new images\n",
    "        gen_imgs = G.predict(noise)\n",
    "\n",
    "        # Train the discriminator\n",
    "        d_loss_real = D.train_on_batch(imgs, valid)\n",
    "        d_loss_fake = D.train_on_batch(gen_imgs, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "\n",
    "        g_loss = combined.train_on_batch(noise, valid)\n",
    "\n",
    "        # Plot the progress\n",
    "        print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % sample_interval == 0:\n",
    "            sample_images(epoch, latent_dim, G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# create optimizer\n",
    "optimizer = Adam(0.0002, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Build and compile the discriminator\n",
    "D = build_discriminator(img_shape)\n",
    "D.compile(loss='binary_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the generator\n",
    "G = build_generator(latent_dim, img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generator takes noise as input and generated imgs\n",
    "z = Input(shape=(latent_dim,))\n",
    "img = G(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the combined model we will only train the generator\n",
    "D.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The valid takes generated images as input and determines validity\n",
    "valid = D(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The combined model  (stacked generator and discriminator)\n",
    "# Trains the generator to fool the discriminator\n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss=boundary_loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run\n",
    "\n",
    "there is some bug in the code, because G loss goes to inf in iteration 143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.495093, acc.: 64.06%] [G loss: 0.118298]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 0.308667, acc.: 98.44%] [G loss: 0.129122]\n",
      "2 [D loss: 0.281581, acc.: 96.88%] [G loss: 0.207400]\n",
      "3 [D loss: 0.273255, acc.: 96.88%] [G loss: 0.433630]\n",
      "4 [D loss: 0.273254, acc.: 87.50%] [G loss: 0.627098]\n",
      "5 [D loss: 0.230863, acc.: 96.88%] [G loss: 0.885139]\n",
      "6 [D loss: 0.163481, acc.: 100.00%] [G loss: 1.453845]\n",
      "7 [D loss: 0.137503, acc.: 100.00%] [G loss: 1.909683]\n",
      "8 [D loss: 0.103650, acc.: 100.00%] [G loss: 2.438941]\n",
      "9 [D loss: 0.097468, acc.: 100.00%] [G loss: 2.603986]\n",
      "10 [D loss: 0.074304, acc.: 100.00%] [G loss: 3.110146]\n",
      "11 [D loss: 0.062242, acc.: 100.00%] [G loss: 3.676673]\n",
      "12 [D loss: 0.053192, acc.: 100.00%] [G loss: 4.028837]\n",
      "13 [D loss: 0.052831, acc.: 100.00%] [G loss: 3.959708]\n",
      "14 [D loss: 0.043390, acc.: 100.00%] [G loss: 4.470638]\n",
      "15 [D loss: 0.042539, acc.: 100.00%] [G loss: 4.747778]\n",
      "16 [D loss: 0.038933, acc.: 100.00%] [G loss: 5.010965]\n",
      "17 [D loss: 0.029929, acc.: 100.00%] [G loss: 5.738226]\n",
      "18 [D loss: 0.033720, acc.: 100.00%] [G loss: 5.467397]\n",
      "19 [D loss: 0.030827, acc.: 100.00%] [G loss: 5.783936]\n",
      "20 [D loss: 0.022516, acc.: 100.00%] [G loss: 5.970081]\n",
      "21 [D loss: 0.020663, acc.: 100.00%] [G loss: 6.188041]\n",
      "22 [D loss: 0.026842, acc.: 100.00%] [G loss: 6.022173]\n",
      "23 [D loss: 0.027805, acc.: 100.00%] [G loss: 6.341897]\n",
      "24 [D loss: 0.021746, acc.: 100.00%] [G loss: 7.057618]\n",
      "25 [D loss: 0.017615, acc.: 100.00%] [G loss: 8.157464]\n",
      "26 [D loss: 0.019411, acc.: 100.00%] [G loss: 7.448387]\n",
      "27 [D loss: 0.017023, acc.: 100.00%] [G loss: 7.350662]\n",
      "28 [D loss: 0.013325, acc.: 100.00%] [G loss: 7.784717]\n",
      "29 [D loss: 0.017675, acc.: 100.00%] [G loss: 7.646319]\n",
      "30 [D loss: 0.017844, acc.: 100.00%] [G loss: 7.794071]\n",
      "31 [D loss: 0.016563, acc.: 100.00%] [G loss: 7.636062]\n",
      "32 [D loss: 0.015618, acc.: 100.00%] [G loss: 8.382646]\n",
      "33 [D loss: 0.014928, acc.: 100.00%] [G loss: 8.472347]\n",
      "34 [D loss: 0.015291, acc.: 100.00%] [G loss: 8.684670]\n",
      "35 [D loss: 0.011756, acc.: 100.00%] [G loss: 8.330337]\n",
      "36 [D loss: 0.010068, acc.: 100.00%] [G loss: 9.638817]\n",
      "37 [D loss: 0.015309, acc.: 100.00%] [G loss: 8.564861]\n",
      "38 [D loss: 0.015231, acc.: 100.00%] [G loss: 8.763495]\n",
      "39 [D loss: 0.018480, acc.: 100.00%] [G loss: 9.656267]\n",
      "40 [D loss: 0.012644, acc.: 100.00%] [G loss: 10.035812]\n",
      "41 [D loss: 0.011298, acc.: 100.00%] [G loss: 9.416029]\n",
      "42 [D loss: 0.006068, acc.: 100.00%] [G loss: 10.236092]\n",
      "43 [D loss: 0.010907, acc.: 100.00%] [G loss: 10.192892]\n",
      "44 [D loss: 0.009265, acc.: 100.00%] [G loss: 10.094459]\n",
      "45 [D loss: 0.009225, acc.: 100.00%] [G loss: 10.287329]\n",
      "46 [D loss: 0.014323, acc.: 100.00%] [G loss: 10.334703]\n",
      "47 [D loss: 0.008150, acc.: 100.00%] [G loss: 10.387736]\n",
      "48 [D loss: 0.011096, acc.: 100.00%] [G loss: 10.041792]\n",
      "49 [D loss: 0.010989, acc.: 100.00%] [G loss: 10.380331]\n",
      "50 [D loss: 0.012364, acc.: 100.00%] [G loss: 10.481133]\n",
      "51 [D loss: 0.010339, acc.: 100.00%] [G loss: 9.771379]\n",
      "52 [D loss: 0.009082, acc.: 100.00%] [G loss: 11.183275]\n",
      "53 [D loss: 0.009412, acc.: 100.00%] [G loss: 11.117115]\n",
      "54 [D loss: 0.007443, acc.: 100.00%] [G loss: 11.428753]\n",
      "55 [D loss: 0.011484, acc.: 100.00%] [G loss: 11.422821]\n",
      "56 [D loss: 0.008644, acc.: 100.00%] [G loss: 11.870095]\n",
      "57 [D loss: 0.012691, acc.: 100.00%] [G loss: 11.714792]\n",
      "58 [D loss: 0.006976, acc.: 100.00%] [G loss: 11.128700]\n",
      "59 [D loss: 0.010068, acc.: 100.00%] [G loss: 11.541443]\n",
      "60 [D loss: 0.009708, acc.: 100.00%] [G loss: 11.658628]\n",
      "61 [D loss: 0.006458, acc.: 100.00%] [G loss: 11.869875]\n",
      "62 [D loss: 0.008178, acc.: 100.00%] [G loss: 11.451118]\n",
      "63 [D loss: 0.007092, acc.: 100.00%] [G loss: 11.669768]\n",
      "64 [D loss: 0.007609, acc.: 100.00%] [G loss: 12.013054]\n",
      "65 [D loss: 0.011749, acc.: 100.00%] [G loss: 12.228474]\n",
      "66 [D loss: 0.008205, acc.: 100.00%] [G loss: 12.255772]\n",
      "67 [D loss: 0.008605, acc.: 100.00%] [G loss: 12.243083]\n",
      "68 [D loss: 0.008319, acc.: 100.00%] [G loss: 12.603165]\n",
      "69 [D loss: 0.006756, acc.: 100.00%] [G loss: 12.573383]\n",
      "70 [D loss: 0.009497, acc.: 100.00%] [G loss: 12.673233]\n",
      "71 [D loss: 0.006392, acc.: 100.00%] [G loss: 12.899219]\n",
      "72 [D loss: 0.009703, acc.: 100.00%] [G loss: 12.951905]\n",
      "73 [D loss: 0.006065, acc.: 100.00%] [G loss: 13.063745]\n",
      "74 [D loss: 0.011475, acc.: 100.00%] [G loss: 12.862453]\n",
      "75 [D loss: 0.009130, acc.: 100.00%] [G loss: 13.421865]\n",
      "76 [D loss: 0.005644, acc.: 100.00%] [G loss: 14.072464]\n",
      "77 [D loss: 0.010222, acc.: 100.00%] [G loss: 13.557691]\n",
      "78 [D loss: 0.004547, acc.: 100.00%] [G loss: 13.187891]\n",
      "79 [D loss: 0.007768, acc.: 100.00%] [G loss: 12.696229]\n",
      "80 [D loss: 0.010431, acc.: 100.00%] [G loss: 12.631165]\n",
      "81 [D loss: 0.009626, acc.: 100.00%] [G loss: 14.013401]\n",
      "82 [D loss: 0.006316, acc.: 100.00%] [G loss: 13.282231]\n",
      "83 [D loss: 0.009140, acc.: 100.00%] [G loss: 13.558535]\n",
      "84 [D loss: 0.006331, acc.: 100.00%] [G loss: 13.243741]\n",
      "85 [D loss: 0.007457, acc.: 100.00%] [G loss: 13.019682]\n",
      "86 [D loss: 0.008218, acc.: 100.00%] [G loss: 13.697681]\n",
      "87 [D loss: 0.013588, acc.: 100.00%] [G loss: 15.128361]\n",
      "88 [D loss: 0.008270, acc.: 100.00%] [G loss: 14.598673]\n",
      "89 [D loss: 0.010682, acc.: 100.00%] [G loss: 14.887500]\n",
      "90 [D loss: 0.011394, acc.: 100.00%] [G loss: 15.079613]\n",
      "91 [D loss: 0.007734, acc.: 100.00%] [G loss: 13.751100]\n",
      "92 [D loss: 0.013089, acc.: 100.00%] [G loss: 13.841949]\n",
      "93 [D loss: 0.006665, acc.: 100.00%] [G loss: 14.474756]\n",
      "94 [D loss: 0.016033, acc.: 100.00%] [G loss: 14.123282]\n",
      "95 [D loss: 0.010521, acc.: 100.00%] [G loss: 13.771183]\n",
      "96 [D loss: 0.007248, acc.: 100.00%] [G loss: 14.148188]\n",
      "97 [D loss: 0.007715, acc.: 100.00%] [G loss: 14.481373]\n",
      "98 [D loss: 0.016685, acc.: 100.00%] [G loss: 12.965490]\n",
      "99 [D loss: 0.008333, acc.: 100.00%] [G loss: 13.772816]\n",
      "100 [D loss: 0.012985, acc.: 100.00%] [G loss: 15.009028]\n",
      "101 [D loss: 0.009077, acc.: 100.00%] [G loss: 13.601442]\n",
      "102 [D loss: 0.009168, acc.: 100.00%] [G loss: 14.280630]\n",
      "103 [D loss: 0.010024, acc.: 100.00%] [G loss: 15.619144]\n",
      "104 [D loss: 0.023744, acc.: 100.00%] [G loss: 18.028290]\n",
      "105 [D loss: 0.043494, acc.: 100.00%] [G loss: 12.840025]\n",
      "106 [D loss: 0.011551, acc.: 100.00%] [G loss: 17.078005]\n",
      "107 [D loss: 0.014021, acc.: 100.00%] [G loss: 19.071894]\n",
      "108 [D loss: 0.655228, acc.: 79.69%] [G loss: 17.992222]\n",
      "109 [D loss: 0.056866, acc.: 100.00%] [G loss: 20.914909]\n",
      "110 [D loss: 0.026763, acc.: 98.44%] [G loss: 23.568821]\n",
      "111 [D loss: 0.016330, acc.: 100.00%] [G loss: 23.442087]\n",
      "112 [D loss: 0.016950, acc.: 100.00%] [G loss: 21.484226]\n",
      "113 [D loss: 0.010689, acc.: 100.00%] [G loss: 19.456131]\n",
      "114 [D loss: 0.015654, acc.: 100.00%] [G loss: 18.225796]\n",
      "115 [D loss: 0.022441, acc.: 100.00%] [G loss: 17.185593]\n",
      "116 [D loss: 0.010652, acc.: 100.00%] [G loss: 18.874672]\n",
      "117 [D loss: 0.010443, acc.: 100.00%] [G loss: 16.021551]\n",
      "118 [D loss: 0.026130, acc.: 100.00%] [G loss: 18.563450]\n",
      "119 [D loss: 0.052328, acc.: 100.00%] [G loss: 12.261777]\n",
      "120 [D loss: 0.070684, acc.: 98.44%] [G loss: 25.116913]\n",
      "121 [D loss: 0.382353, acc.: 85.94%] [G loss: 14.231289]\n",
      "122 [D loss: 0.027296, acc.: 100.00%] [G loss: 19.880714]\n",
      "123 [D loss: 0.513418, acc.: 82.81%] [G loss: 20.589573]\n",
      "124 [D loss: 0.052102, acc.: 98.44%] [G loss: 19.899282]\n",
      "125 [D loss: 0.030643, acc.: 100.00%] [G loss: 17.702255]\n",
      "126 [D loss: 0.107640, acc.: 98.44%] [G loss: 16.929684]\n",
      "127 [D loss: 0.024773, acc.: 100.00%] [G loss: 15.397837]\n",
      "128 [D loss: 0.013945, acc.: 100.00%] [G loss: 13.741981]\n",
      "129 [D loss: 0.035240, acc.: 100.00%] [G loss: 13.768963]\n",
      "130 [D loss: 0.070339, acc.: 98.44%] [G loss: 16.010183]\n",
      "131 [D loss: 0.115524, acc.: 95.31%] [G loss: 19.411888]\n",
      "132 [D loss: 0.415398, acc.: 87.50%] [G loss: 10.791712]\n",
      "133 [D loss: 0.022054, acc.: 100.00%] [G loss: 16.214046]\n",
      "134 [D loss: 0.121114, acc.: 98.44%] [G loss: 11.087959]\n",
      "135 [D loss: 0.052371, acc.: 98.44%] [G loss: 16.786886]\n",
      "136 [D loss: 0.147064, acc.: 95.31%] [G loss: 15.879070]\n",
      "137 [D loss: 0.149554, acc.: 89.06%] [G loss: 18.142578]\n",
      "138 [D loss: 0.235491, acc.: 93.75%] [G loss: 15.654020]\n",
      "139 [D loss: 0.044870, acc.: 100.00%] [G loss: 17.897728]\n",
      "140 [D loss: 0.031577, acc.: 100.00%] [G loss: 13.449511]\n",
      "141 [D loss: 0.077132, acc.: 98.44%] [G loss: 16.928350]\n",
      "142 [D loss: 0.268453, acc.: 90.62%] [G loss: 19.312441]\n",
      "143 [D loss: 0.943262, acc.: 70.31%] [G loss: inf]\n",
      "144 [D loss: nan, acc.: 50.00%] [G loss: nan]\n",
      "145 [D loss: nan, acc.: 0.00%] [G loss: nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "147 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "148 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "149 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "150 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "151 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "152 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "153 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "154 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "155 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "156 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "157 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "158 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "159 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "160 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "161 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "162 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "163 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "164 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "165 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "166 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "167 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "168 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "169 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "170 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "171 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "172 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "173 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "174 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "175 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "176 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "177 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "178 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "179 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "180 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "181 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "182 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "183 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "184 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "185 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "186 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "187 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "188 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "189 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "190 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "191 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "192 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "193 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "194 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "195 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "196 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "197 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "198 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "199 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "200 [D loss: nan, acc.: 0.00%] [G loss: nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\matplotlib\\image.py:397: UserWarning: Warning: converting a masked element to nan.\n",
      "  dv = (np.float64(self.norm.vmax) -\n",
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\matplotlib\\image.py:398: UserWarning: Warning: converting a masked element to nan.\n",
      "  np.float64(self.norm.vmin))\n",
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\matplotlib\\image.py:405: UserWarning: Warning: converting a masked element to nan.\n",
      "  a_min = np.float64(newmin)\n",
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\matplotlib\\image.py:410: UserWarning: Warning: converting a masked element to nan.\n",
      "  a_max = np.float64(newmax)\n",
      "<string>:6: UserWarning: Warning: converting a masked element to nan.\n",
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\ma\\core.py:722: UserWarning: Warning: converting a masked element to nan.\n",
      "  data = np.array(a, copy=False, subok=subok)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "202 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "203 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "204 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "205 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "206 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "207 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "208 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "209 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "210 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "211 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "212 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "213 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "214 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "215 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "216 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "217 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "218 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "219 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "220 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "221 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "222 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "223 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "224 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "225 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "226 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "227 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "228 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "229 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "230 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "231 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "232 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "233 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "234 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "235 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "236 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "237 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "238 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "239 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "240 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "241 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "242 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "243 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "244 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "245 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "246 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "247 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "248 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "249 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "250 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "251 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "252 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "253 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "254 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "255 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "256 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "257 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "258 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "259 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "260 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "261 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "262 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "263 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "264 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "265 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "266 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "267 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "268 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "269 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "270 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "271 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "272 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "273 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "274 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "275 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "276 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "277 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "278 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "279 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "280 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "281 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "282 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "283 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "284 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "285 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "286 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "287 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "288 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "289 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "290 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "291 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "292 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "293 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "294 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "295 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "296 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "297 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "298 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "299 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "300 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "301 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "302 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "303 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "304 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "305 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "306 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "307 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "308 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "309 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "310 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "311 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "312 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "313 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "314 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "315 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "316 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "317 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "318 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "319 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "320 [D loss: nan, acc.: 0.00%] [G loss: nan]\n",
      "321 [D loss: nan, acc.: 0.00%] [G loss: nan]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-72acf437342c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# epochs=30000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcombined\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-055aaa792cce>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(G, D, combined, latent_dim, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# Train the discriminator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0md_loss_real\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0md_loss_fake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0md_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_loss_real\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3476\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mc:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs=3000\n",
    "# epochs=30000\n",
    "train(G, D, combined, latent_dim, epochs=epochs, batch_size=32, sample_interval=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
