{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN\n",
    "\n",
    "\n",
    "Ref: CHU, Casey; ZHMOGINOV, Andrey; SANDLER, Mark. Cyclegan, a master of steganography. arXiv preprint arXiv:1712.02950, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from data_loader import DataLoader\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# from keras.datasets import mnist\n",
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
    "from keras.layers import Input, Dropout, Concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function for the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(layer_input, filters, f_size=4, strides=2, alpha=0.2):\n",
    "    \"\"\"Layers used during downsampling\"\"\"\n",
    "    d = Conv2D(filters, kernel_size=f_size, strides=strides, padding='same')(layer_input)\n",
    "    d = LeakyReLU(alpha=alpha)(d)\n",
    "    d = InstanceNormalization()(d)\n",
    "    return d\n",
    "\n",
    "def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0, up_size=2, strides=1):\n",
    "    \"\"\" Layers used during upsampling\n",
    "        :param skip_input: layer to make skip connection to\n",
    "    \"\"\"\n",
    "    u = UpSampling2D(size=up_size)(layer_input)\n",
    "    u = Conv2D(filters, kernel_size=f_size, strides=strides, padding='same', activation='relu')(u)\n",
    "    if dropout_rate:\n",
    "        u = Dropout(dropout_rate)(u)\n",
    "    u = InstanceNormalization()(u)\n",
    "    u = Concatenate()([u, skip_input])\n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(img_shape, gf, channels):\n",
    "    \"\"\" U-Net Generator\n",
    "        :param gf: generator filter size in the first layer\n",
    "        \n",
    "        \n",
    "        ref.: RONNEBERGER, Olaf; FISCHER, Philipp; BROX, Thomas. U-net: Convolutional networks for biomedical image segmentation. \n",
    "              In: International Conference on Medical image computing and computer-assisted intervention. \n",
    "              Springer, Cham, 2015. p. 234-241.\n",
    "    \"\"\"\n",
    "\n",
    "    # Image input\n",
    "    d0 = Input(shape=img_shape)\n",
    "\n",
    "    # Downsampling\n",
    "    d1 = conv2d(d0, gf)\n",
    "    d2 = conv2d(d1, gf*2)\n",
    "    d3 = conv2d(d2, gf*4)\n",
    "    d4 = conv2d(d3, gf*8)\n",
    "\n",
    "    # Upsampling\n",
    "    u1 = deconv2d(d4, d3, gf*4)\n",
    "    u2 = deconv2d(u1, d2, gf*2)\n",
    "    u3 = deconv2d(u2, d1, gf)\n",
    "\n",
    "    u4 = UpSampling2D(size=2)(u3)\n",
    "    output_img = Conv2D(channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n",
    "\n",
    "    return Model(d0, output_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function for Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_layer(layer_input, filters, f_size=4, normalization=True, strides=2):\n",
    "    \"\"\"Discriminator layer\"\"\"\n",
    "    d = Conv2D(filters, kernel_size=f_size, strides=strides, padding='same')(layer_input)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    if normalization:\n",
    "        d = InstanceNormalization()(d)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(img_shape, df):\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "\n",
    "    d1 = d_layer(img, df, normalization=False)\n",
    "    d2 = d_layer(d1, df*2)\n",
    "    d3 = d_layer(d2, df*4)\n",
    "    d4 = d_layer(d3, df*8)\n",
    "\n",
    "    validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "    return Model(img, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(epoch, batch_i,\n",
    "                  g_AB, g_BA, # generators\n",
    "                  dataset_name, data_loader):\n",
    "    os.makedirs('images/%s' % dataset_name, exist_ok=True)\n",
    "    r, c = 2, 3\n",
    "\n",
    "    imgs_A = data_loader.load_data(domain=\"A\", batch_size=1, is_testing=True)\n",
    "    imgs_B = data_loader.load_data(domain=\"B\", batch_size=1, is_testing=True)\n",
    "\n",
    "    # Translate images to the other domain\n",
    "    fake_B = g_AB.predict(imgs_A)\n",
    "    fake_A = g_BA.predict(imgs_B)\n",
    "    # Translate back to original domain\n",
    "    reconstr_A = g_BA.predict(fake_B)\n",
    "    reconstr_B = g_AB.predict(fake_A)\n",
    "\n",
    "    gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B])\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    titles = ['Original', 'Translated', 'Reconstructed']\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt])\n",
    "            axs[i, j].set_title(titles[j])\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/%s/%d_%d.png\" % (dataset_name, epoch, batch_i))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform training of the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, \n",
    "          g_AB, g_BA, # generators\n",
    "          d_A, d_B, # discriminators\n",
    "          combined, # full GAN\n",
    "          dataset_name, img_rows, img_cols,\n",
    "          batch_size=1, sample_interval=50,\n",
    "          ):\n",
    "    \n",
    "    # Calculate output shape of D (PatchGAN)\n",
    "    patch = int(img_rows / 2**4)\n",
    "    disc_patch = (patch, patch, 1)\n",
    "\n",
    "    # class that loads the data in batches\n",
    "    data_loader = DataLoader(dataset_name=dataset_name,\n",
    "                             img_res=(img_rows, img_cols))\n",
    "\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # Adversarial loss ground truths\n",
    "    valid = np.ones((batch_size,) + disc_patch)\n",
    "    fake = np.zeros((batch_size,) + disc_patch)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch_i, (imgs_A, imgs_B) in enumerate(data_loader.load_batch(batch_size)):\n",
    "\n",
    "            # ----------------------\n",
    "            #  Train Discriminators\n",
    "            # ----------------------\n",
    "\n",
    "            # Translate images to opposite domain\n",
    "            fake_B = g_AB.predict(imgs_A)\n",
    "            fake_A = g_BA.predict(imgs_B)\n",
    "\n",
    "            # Train the discriminators (original images = real / translated = Fake)\n",
    "            dA_loss_real = d_A.train_on_batch(imgs_A, valid)\n",
    "            dA_loss_fake = d_A.train_on_batch(fake_A, fake)\n",
    "            dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
    "\n",
    "            dB_loss_real = d_B.train_on_batch(imgs_B, valid)\n",
    "            dB_loss_fake = d_B.train_on_batch(fake_B, fake)\n",
    "            dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
    "\n",
    "            # Total disciminator loss\n",
    "            d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
    "\n",
    "\n",
    "            # ------------------\n",
    "            #  Train Generators\n",
    "            # ------------------\n",
    "\n",
    "            # Train the generators\n",
    "            g_loss = combined.train_on_batch([imgs_A, imgs_B],\n",
    "                                             [valid, valid,\n",
    "                                              imgs_A, imgs_B,\n",
    "                                              imgs_A, imgs_B])\n",
    "\n",
    "            elapsed_time = datetime.datetime.now() - start_time\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s \" \\\n",
    "                                                                    % ( epoch, epochs,\n",
    "                                                                        batch_i, data_loader.n_batches,\n",
    "                                                                        d_loss[0], 100*d_loss[1],\n",
    "                                                                        g_loss[0],\n",
    "                                                                        np.mean(g_loss[1:3]),\n",
    "                                                                        np.mean(g_loss[3:5]),\n",
    "                                                                        np.mean(g_loss[5:6]),\n",
    "                                                                        elapsed_time))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if batch_i % sample_interval == 0:\n",
    "                sample_images(epoch, batch_i, \n",
    "                              g_AB, g_BA, # generators\n",
    "                              dataset_name, data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 128\n",
    "img_cols = 128\n",
    "channels = 3\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "\n",
    "# Configure data loader\n",
    "dataset_name = 'apple2orange'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss weights\n",
    "lambda_cycle = 10.0               # Cycle-consistency loss\n",
    "lambda_id = 0.1 * lambda_cycle    # Identity loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# optimizer\n",
    "optimizer = Adam(0.0002, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of filters in the first layer of G and D\n",
    "gf = 32\n",
    "df = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the discriminators\n",
    "d_A = build_discriminator(img_shape, df)\n",
    "d_B = build_discriminator(img_shape, df)\n",
    "d_A.compile(loss='mse',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "d_B.compile(loss='mse',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "# Construct Computational\n",
    "#   Graph of Generators\n",
    "#-------------------------\n",
    "\n",
    "# Build the generators\n",
    "g_AB = build_generator(img_shape, gf, channels)\n",
    "g_BA = build_generator(img_shape, gf, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input images from both domains\n",
    "img_A = Input(shape=img_shape)\n",
    "img_B = Input(shape=img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate images to the other domain\n",
    "fake_B = g_AB(img_A)\n",
    "fake_A = g_BA(img_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate images back to original domain\n",
    "reconstr_A = g_BA(fake_B)\n",
    "reconstr_B = g_AB(fake_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identity mapping of images\n",
    "img_A_id = g_BA(img_A)\n",
    "img_B_id = g_AB(img_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the combined model we will only train the generators\n",
    "d_A.trainable = False\n",
    "d_B.trainable = False\n",
    "\n",
    "# Discriminators determines validity of translated images\n",
    "valid_A = d_A(fake_A)\n",
    "valid_B = d_B(fake_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined model trains generators to fool discriminators\n",
    "combined = Model(inputs=[img_A, img_B],\n",
    "                      outputs=[ valid_A, valid_B,\n",
    "                                reconstr_A, reconstr_B,\n",
    "                                img_A_id, img_B_id ])\n",
    "combined.compile(loss=['mse', 'mse',\n",
    "                            'mae', 'mae',\n",
    "                            'mae', 'mae'],\n",
    "                    loss_weights=[  1, 1,\n",
    "                                    lambda_cycle, lambda_cycle,\n",
    "                                    lambda_id, lambda_id ],\n",
    "                    optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 3), <i4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   2679\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2680\u001b[1;33m             \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2681\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ((1, 1, 3), '<i4')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-2424aa9ef563>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m train(epochs=epochs, batch_size=1, sample_interval=200,\n\u001b[0;32m      3\u001b[0m       \u001b[0mg_AB\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mg_AB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_BA\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mg_BA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_A\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0md_A\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_B\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0md_B\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcombined\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcombined\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m       \u001b[0mdataset_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimg_rows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimg_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m       )\n",
      "\u001b[1;32m<ipython-input-9-ad8200604670>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epochs, g_AB, g_BA, d_A, d_B, combined, dataset_name, img_rows, img_cols, batch_size, sample_interval)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimgs_A\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimgs_B\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;31m# ----------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Devel\\Keras-GAN\\cyclegan\\data_loader.py\u001b[0m in \u001b[0;36mload_batch\u001b[1;34m(self, batch_size, is_testing)\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0mimg_B\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_B\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                 \u001b[0mimg_A\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_A\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_res\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m                 \u001b[0mimg_B\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_B\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_res\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Devel\\Keras-GAN\\cyclegan\\data_loader.py\u001b[0m in \u001b[0;36mimresize\u001b[1;34m(self, img, size)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;34m\"\"\"workaroung imresize\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.99999\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mnew_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBICUBIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   2680\u001b[0m             \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2681\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2682\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot handle this data type: %s, %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtypekey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2683\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2684\u001b[0m         \u001b[0mrawmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 3), <i4"
     ]
    }
   ],
   "source": [
    "epochs = 3 #200\n",
    "train(epochs=epochs, batch_size=1, sample_interval=200,\n",
    "      g_AB=g_AB, g_BA=g_BA, d_A=d_A, d_B=d_B, combined=combined,\n",
    "      dataset_name=dataset_name, img_rows=img_rows, img_cols=img_cols\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
