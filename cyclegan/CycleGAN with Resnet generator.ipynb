{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN\n",
    "\n",
    "\n",
    "Ref: CHU, Casey; ZHMOGINOV, Andrey; SANDLER, Mark. Cyclegan, a master of steganography. arXiv preprint arXiv:1712.02950, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from data_loader import DataLoader\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.datasets import mnist\n",
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
    "from keras.layers import Input, Dropout, Concatenate, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D, Conv2DTranspose\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.layers.merge import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.engine.topology import Layer\n",
    "from keras.engine import InputSpec\n",
    "\n",
    "class ReflectionPadding2D(Layer):\n",
    "    \"\"\" ref. https://stackoverflow.com/questions/50677544/reflection-padding-conv2d\n",
    "    \"\"\"\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, s):\n",
    "        \"\"\" If you are using \"channels_last\" configuration\"\"\"\n",
    "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        w_pad,h_pad = self.padding\n",
    "        return tf.pad(x, [[0,0], [h_pad,h_pad], [w_pad,w_pad], [0,0] ], 'REFLECT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader2 import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function for the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv7s1(layer_input, filters, final, weight_init):\n",
    "    y = ReflectionPadding2D(padding =(3,3))(layer_input)\n",
    "    y = Conv2D(filters, kernel_size=(7,7), strides=1, padding='valid', kernel_initializer = weight_init)(y)\n",
    "    if final:\n",
    "        y = Activation('tanh')(y)\n",
    "    else:\n",
    "        y = InstanceNormalization(axis = -1, center = False, scale = False)(y)\n",
    "        y = Activation('relu')(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "def downsample(layer_input,filters, weight_init):\n",
    "    y = Conv2D(filters, kernel_size=(3,3), strides=2, padding='same', kernel_initializer = weight_init)(layer_input)\n",
    "    y = InstanceNormalization(axis = -1, center = False, scale = False)(y)\n",
    "    y = Activation('relu')(y)\n",
    "    \n",
    "    return y\n",
    "\n",
    "def residual(layer_input, filters, weight_init):\n",
    "    shortcut = layer_input\n",
    "    y = ReflectionPadding2D(padding =(1,1))(layer_input)\n",
    "    y = Conv2D(filters, kernel_size=(3, 3), strides=1, padding='valid', kernel_initializer = weight_init)(y)\n",
    "    y = InstanceNormalization(axis = -1, center = False, scale = False)(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = ReflectionPadding2D(padding =(1,1))(y)\n",
    "    y = Conv2D(filters, kernel_size=(3, 3), strides=1, padding='valid', kernel_initializer = weight_init)(y)\n",
    "    y = InstanceNormalization(axis = -1, center = False, scale = False)(y)\n",
    "\n",
    "    return add([shortcut, y])\n",
    "\n",
    "def upsample(layer_input,filters, weight_init):\n",
    "    y = Conv2DTranspose(filters, kernel_size=(3, 3), strides=2, padding='same', kernel_initializer = weight_init)(layer_input)\n",
    "    y = InstanceNormalization(axis = -1, center = False, scale = False)(y)\n",
    "    y = Activation('relu')(y)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(img_shape, gf, weight_init):\n",
    "    \"\"\" U-Net Generator\n",
    "        :param gf: generator filter size in the first layer\n",
    "        \n",
    "        \n",
    "        ref.: Kaiming He et al., “Deep Residual Learning for Image Recognition” \n",
    "              10 December 2015, https://arxiv.org/abs/1512.03385\n",
    "    \"\"\"\n",
    "    # Image input\n",
    "    img = Input(shape=img_shape)\n",
    "\n",
    "    y = img\n",
    "\n",
    "    y = conv7s1(y, gf, False, weight_init)\n",
    "    y = downsample(y, gf * 2, weight_init)\n",
    "    y = downsample(y, gf * 4, weight_init)\n",
    "    y = residual(y, gf * 4, weight_init)\n",
    "    y = residual(y, gf * 4, weight_init)\n",
    "    y = residual(y, gf * 4, weight_init)\n",
    "    y = residual(y, gf * 4, weight_init)\n",
    "    y = residual(y, gf * 4, weight_init)\n",
    "    y = residual(y, gf * 4, weight_init)\n",
    "    y = residual(y, gf * 4, weight_init)\n",
    "    y = residual(y, gf * 4, weight_init)\n",
    "    y = residual(y, gf * 4, weight_init)\n",
    "    y = upsample(y, gf * 2, weight_init)\n",
    "    y = upsample(y, gf, weight_init)\n",
    "    y = conv7s1(y, 3, True, weight_init)\n",
    "    output = y\n",
    "\n",
    "    return Model(img, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function for Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv4(layer_input, filters, weight_init, stride = 2, norm=True):\n",
    "    y = Conv2D(filters, kernel_size=(4,4), strides=stride, padding='same', kernel_initializer = weight_init)(layer_input)\n",
    "    if norm:\n",
    "        y = InstanceNormalization(axis = -1, center = False, scale = False)(y)\n",
    "    y = LeakyReLU(0.2)(y)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(img_shape, df, weight_init):\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "\n",
    "    y = conv4(img, df, weight_init, stride = 2, norm = False)\n",
    "    y = conv4(y, df * 2, weight_init, stride = 2)\n",
    "    y = conv4(y, df * 4, weight_init, stride = 4)\n",
    "    y = conv4(y, df * 8, weight_init, stride = 1)\n",
    "\n",
    "    output = Conv2D(1, kernel_size=4, strides=1, padding='same', kernel_initializer = weight_init)(y)\n",
    "\n",
    "    return Model(img, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(epoch, batch_i,\n",
    "                  g_AB, g_BA, # generators\n",
    "                  dataset_name, data_loader):\n",
    "    os.makedirs('images/%s' % dataset_name, exist_ok=True)\n",
    "    r, c = 2, 3 # rows, columns\n",
    "\n",
    "    imgs_A = data_loader.load_data(domain=\"A\", batch_size=1, is_testing=True)\n",
    "    imgs_B = data_loader.load_data(domain=\"B\", batch_size=1, is_testing=True)\n",
    "\n",
    "    # Translate images to the other domain\n",
    "    fake_B = g_AB.predict(imgs_A)\n",
    "    fake_A = g_BA.predict(imgs_B)\n",
    "    # Translate back to original domain\n",
    "    reconstr_A = g_BA.predict(fake_B)\n",
    "    reconstr_B = g_AB.predict(fake_A)\n",
    "\n",
    "    gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B])\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    titles = ['Original', 'Translated', 'Reconstructed']\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):  # rows\n",
    "        for j in range(c):  # columns\n",
    "            axs[i,j].imshow(gen_imgs[cnt])\n",
    "            axs[i, j].set_title(titles[j])\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/%s/%d_%d.png\" % (dataset_name, epoch, batch_i))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform training of the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, \n",
    "          g_AB, g_BA, # generators\n",
    "          d_A, d_B, # discriminators\n",
    "          combined, # full GAN\n",
    "          dataset_name, img_rows, img_cols,\n",
    "          batch_size=1, sample_interval=50,\n",
    "          ):\n",
    "    \n",
    "    # Calculate output shape of D (PatchGAN)\n",
    "    patch = int(img_rows / 2**4)\n",
    "    disc_patch = (patch, patch, 1)\n",
    "\n",
    "    # class that loads the data in batches\n",
    "    data_loader = DataLoader(dataset_name=dataset_name,\n",
    "                             img_res=(img_rows, img_cols))\n",
    "\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # Adversarial loss ground truths\n",
    "    valid = np.ones((batch_size,) + disc_patch)\n",
    "    fake = np.zeros((batch_size,) + disc_patch)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch_i, (imgs_A, imgs_B) in enumerate(data_loader.load_batch(batch_size)):\n",
    "\n",
    "            # ----------------------\n",
    "            #  Train Discriminators\n",
    "            # ----------------------\n",
    "\n",
    "            # Translate images to opposite domain\n",
    "            fake_B = g_AB.predict(imgs_A)\n",
    "            fake_A = g_BA.predict(imgs_B)\n",
    "\n",
    "            # Train the discriminators (original images = real / translated = Fake)\n",
    "            dA_loss_real = d_A.train_on_batch(imgs_A, valid)\n",
    "            dA_loss_fake = d_A.train_on_batch(fake_A, fake)\n",
    "            dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
    "\n",
    "            dB_loss_real = d_B.train_on_batch(imgs_B, valid)\n",
    "            dB_loss_fake = d_B.train_on_batch(fake_B, fake)\n",
    "            dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
    "\n",
    "            # Total disciminator loss\n",
    "            d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
    "\n",
    "\n",
    "            # ------------------\n",
    "            #  Train Generators\n",
    "            # ------------------\n",
    "\n",
    "            # Train the generators\n",
    "            g_loss = combined.train_on_batch([imgs_A, imgs_B],\n",
    "                                             [valid, valid,\n",
    "                                              imgs_A, imgs_B,\n",
    "                                              imgs_A, imgs_B])\n",
    "\n",
    "            elapsed_time = datetime.datetime.now() - start_time\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s \" \\\n",
    "                                                                    % ( epoch, epochs,\n",
    "                                                                        batch_i, data_loader.n_batches,\n",
    "                                                                        d_loss[0], 100*d_loss[1],\n",
    "                                                                        g_loss[0],\n",
    "                                                                        np.mean(g_loss[1:3]),\n",
    "                                                                        np.mean(g_loss[3:5]),\n",
    "                                                                        np.mean(g_loss[5:6]),\n",
    "                                                                        elapsed_time))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if batch_i % sample_interval == 0:\n",
    "                sample_images(epoch, batch_i, \n",
    "                              g_AB, g_BA, # generators\n",
    "                              dataset_name, data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 128\n",
    "img_cols = 128\n",
    "channels = 3\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "\n",
    "# Configure data loader\n",
    "dataset_name = 'apple2orange'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_init = RandomNormal(mean=0., stddev=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss weights\n",
    "lambda_cycle = 10.0               # Cycle-consistency loss\n",
    "lambda_id = 0.1 * lambda_cycle    # Identity loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = Adam(0.0002, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of filters in the first layer of G and D\n",
    "gf = 32\n",
    "df = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 128, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_65 (Conv2D)           (None, 64, 64, 64)        3136      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_66 (Conv2D)           (None, 32, 32, 128)       131200    \n",
      "_________________________________________________________________\n",
      "instance_normalization_59 (I (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_67 (Conv2D)           (None, 8, 8, 256)         524544    \n",
      "_________________________________________________________________\n",
      "instance_normalization_60 (I (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_68 (Conv2D)           (None, 8, 8, 512)         2097664   \n",
      "_________________________________________________________________\n",
      "instance_normalization_61 (I (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_69 (Conv2D)           (None, 8, 8, 1)           8193      \n",
      "=================================================================\n",
      "Total params: 2,764,737\n",
      "Trainable params: 2,764,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build and compile the discriminators\n",
    "d_A = build_discriminator(img_shape, df, weight_init)\n",
    "d_B = build_discriminator(img_shape, df, weight_init)\n",
    "d_A.compile(loss='mse',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "d_B.compile(loss='mse',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "d_A.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "# Construct Computational\n",
    "#   Graph of Generators\n",
    "#-------------------------\n",
    "\n",
    "# Build the generators\n",
    "g_AB = build_generator(img_shape, gf, weight_init)\n",
    "g_BA = build_generator(img_shape, gf, weight_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input images from both domains\n",
    "img_A = Input(shape=img_shape)\n",
    "img_B = Input(shape=img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate images to the other domain\n",
    "fake_B = g_AB(img_A)\n",
    "fake_A = g_BA(img_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate images back to original domain\n",
    "reconstr_A = g_BA(fake_B)\n",
    "reconstr_B = g_AB(fake_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identity mapping of images\n",
    "img_A_id = g_BA(img_A)\n",
    "img_B_id = g_AB(img_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the combined model we will only train the generators\n",
    "d_A.trainable = False\n",
    "d_B.trainable = False\n",
    "\n",
    "# Discriminators determines validity of translated images\n",
    "valid_A = d_A(fake_A)\n",
    "valid_B = d_B(fake_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined model trains generators to fool discriminators\n",
    "combined = Model(inputs=[img_A, img_B],\n",
    "                      outputs=[ valid_A, valid_B,\n",
    "                                reconstr_A, reconstr_B,\n",
    "                                img_A_id, img_B_id ])\n",
    "combined.compile(loss=['mse', 'mse',\n",
    "                            'mae', 'mae',\n",
    "                            'mae', 'mae'],\n",
    "                    loss_weights=[  1, 1,\n",
    "                                    lambda_cycle, lambda_cycle,\n",
    "                                    lambda_id, lambda_id ],\n",
    "                    optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/2] [Batch 0/995] [D loss: 2.000792, acc:  28%] [G loss: 29.565210, adv: 5.295656, recon: 0.864134, id: 0.956028] time: 0:00:38.102565 \n",
      "[Epoch 0/2] [Batch 1/995] [D loss: 4.242106, acc:  25%] [G loss: 22.354902, adv: 2.932665, recon: 0.749814, id: 0.863008] time: 0:00:40.991532 \n",
      "[Epoch 0/2] [Batch 2/995] [D loss: 3.104193, acc:  25%] [G loss: 19.969934, adv: 3.053705, recon: 0.629672, id: 0.812977] time: 0:00:43.269533 \n",
      "[Epoch 0/2] [Batch 3/995] [D loss: 3.100769, acc:  26%] [G loss: 21.144304, adv: 3.783855, recon: 0.617508, id: 0.729241] time: 0:00:45.609533 \n",
      "[Epoch 0/2] [Batch 4/995] [D loss: 4.996716, acc:  20%] [G loss: 20.225487, adv: 3.791628, recon: 0.575086, id: 0.587869] time: 0:00:48.011532 \n",
      "[Epoch 0/2] [Batch 5/995] [D loss: 2.492306, acc:  25%] [G loss: 19.631672, adv: 2.330153, recon: 0.677704, id: 0.785379] time: 0:00:50.436533 \n",
      "[Epoch 0/2] [Batch 6/995] [D loss: 3.546727, acc:  24%] [G loss: 15.401932, adv: 2.195482, recon: 0.500669, id: 0.578063] time: 0:00:52.860533 \n",
      "[Epoch 0/2] [Batch 7/995] [D loss: 2.591869, acc:  26%] [G loss: 19.448080, adv: 3.999263, recon: 0.518066, id: 0.501434] time: 0:00:55.305531 \n",
      "[Epoch 0/2] [Batch 8/995] [D loss: 4.364550, acc:  16%] [G loss: 24.686495, adv: 6.596521, recon: 0.522392, id: 0.500872] time: 0:00:57.750533 \n",
      "[Epoch 0/2] [Batch 9/995] [D loss: 6.446436, acc:  21%] [G loss: 25.003967, adv: 5.170320, recon: 0.664158, id: 0.730041] time: 0:01:00.189532 \n",
      "[Epoch 0/2] [Batch 10/995] [D loss: 5.094541, acc:  25%] [G loss: 17.235533, adv: 1.954331, recon: 0.607285, id: 0.682555] time: 0:01:02.652531 \n",
      "[Epoch 0/2] [Batch 11/995] [D loss: 3.342232, acc:  21%] [G loss: 23.346970, adv: 4.460479, recon: 0.650186, id: 0.723231] time: 0:01:05.174533 \n",
      "[Epoch 0/2] [Batch 12/995] [D loss: 5.305852, acc:  21%] [G loss: 36.505314, adv: 11.389336, recon: 0.613080, id: 0.973466] time: 0:01:07.670531 \n",
      "[Epoch 0/2] [Batch 13/995] [D loss: 2.396070, acc:  20%] [G loss: 16.460302, adv: 3.111699, recon: 0.462541, id: 0.520784] time: 0:01:10.182531 \n",
      "[Epoch 0/2] [Batch 14/995] [D loss: 3.271909, acc:  23%] [G loss: 20.434711, adv: 4.389930, recon: 0.524062, id: 0.568119] time: 0:01:12.720531 \n",
      "[Epoch 0/2] [Batch 15/995] [D loss: 2.737168, acc:  25%] [G loss: 16.863878, adv: 3.168826, recon: 0.474171, id: 0.546927] time: 0:01:15.242531 \n",
      "[Epoch 0/2] [Batch 16/995] [D loss: 1.847071, acc:  32%] [G loss: 17.238386, adv: 2.681186, recon: 0.537563, id: 0.593743] time: 0:01:17.780533 \n",
      "[Epoch 0/2] [Batch 17/995] [D loss: 3.059309, acc:  25%] [G loss: 16.553595, adv: 2.394882, recon: 0.530123, id: 0.515470] time: 0:01:20.353534 \n",
      "[Epoch 0/2] [Batch 18/995] [D loss: 2.543299, acc:  25%] [G loss: 17.246521, adv: 2.483981, recon: 0.555100, id: 0.647141] time: 0:01:22.942532 \n",
      "[Epoch 0/2] [Batch 19/995] [D loss: 2.062514, acc:  29%] [G loss: 13.734617, adv: 1.693386, recon: 0.464958, id: 0.499136] time: 0:01:25.511531 \n",
      "[Epoch 0/2] [Batch 20/995] [D loss: 1.955794, acc:  30%] [G loss: 13.440829, adv: 1.537756, recon: 0.461633, id: 0.719974] time: 0:01:28.068531 \n",
      "[Epoch 0/2] [Batch 21/995] [D loss: 1.335975, acc:  30%] [G loss: 13.681572, adv: 0.866790, recon: 0.531561, id: 0.769905] time: 0:01:30.640531 \n",
      "[Epoch 0/2] [Batch 22/995] [D loss: 1.263584, acc:  42%] [G loss: 12.509214, adv: 0.862864, recon: 0.484087, id: 0.614623] time: 0:01:33.274532 \n",
      "[Epoch 0/2] [Batch 23/995] [D loss: 1.524261, acc:  36%] [G loss: 13.470928, adv: 1.088484, recon: 0.505616, id: 0.608173] time: 0:01:35.880533 \n",
      "[Epoch 0/2] [Batch 24/995] [D loss: 1.452061, acc:  32%] [G loss: 14.561850, adv: 0.749619, recon: 0.589015, id: 0.675328] time: 0:01:38.497533 \n",
      "[Epoch 0/2] [Batch 25/995] [D loss: 1.232498, acc:  35%] [G loss: 12.225143, adv: 0.851799, recon: 0.466863, id: 0.544320] time: 0:01:41.113531 \n",
      "[Epoch 0/2] [Batch 26/995] [D loss: 1.146847, acc:  39%] [G loss: 14.399107, adv: 0.710085, recon: 0.571911, id: 0.833057] time: 0:01:43.731534 \n",
      "[Epoch 0/2] [Batch 27/995] [D loss: 0.847339, acc:  44%] [G loss: 14.067057, adv: 0.830257, recon: 0.551851, id: 0.637079] time: 0:01:46.441533 \n",
      "[Epoch 0/2] [Batch 28/995] [D loss: 0.980178, acc:  42%] [G loss: 13.252875, adv: 1.074862, recon: 0.488186, id: 0.801598] time: 0:01:49.133531 \n",
      "[Epoch 0/2] [Batch 29/995] [D loss: 1.091101, acc:  40%] [G loss: 10.441741, adv: 0.942678, recon: 0.377131, id: 0.563022] time: 0:01:51.810531 \n",
      "[Epoch 0/2] [Batch 30/995] [D loss: 0.887841, acc:  40%] [G loss: 13.314226, adv: 0.749215, recon: 0.520232, id: 0.676474] time: 0:01:54.503531 \n",
      "[Epoch 0/2] [Batch 31/995] [D loss: 0.800582, acc:  44%] [G loss: 10.478234, adv: 0.844029, recon: 0.387649, id: 0.706811] time: 0:01:57.225531 \n",
      "[Epoch 0/2] [Batch 32/995] [D loss: 0.754914, acc:  42%] [G loss: 9.851802, adv: 0.696864, recon: 0.365770, id: 0.706169] time: 0:01:59.934532 \n",
      "[Epoch 0/2] [Batch 33/995] [D loss: 1.089734, acc:  35%] [G loss: 15.446901, adv: 0.686799, recon: 0.633557, id: 0.762531] time: 0:02:02.627532 \n",
      "[Epoch 0/2] [Batch 34/995] [D loss: 0.838901, acc:  44%] [G loss: 12.958274, adv: 0.691281, recon: 0.516551, id: 0.647225] time: 0:02:05.328531 \n",
      "[Epoch 0/2] [Batch 35/995] [D loss: 1.036970, acc:  41%] [G loss: 9.530817, adv: 0.585677, recon: 0.363784, id: 0.586525] time: 0:02:09.012533 \n",
      "[Epoch 0/2] [Batch 36/995] [D loss: 0.753814, acc:  37%] [G loss: 9.224977, adv: 0.766698, recon: 0.340795, id: 0.414678] time: 0:02:12.859536 \n",
      "[Epoch 0/2] [Batch 37/995] [D loss: 0.809998, acc:  42%] [G loss: 11.289277, adv: 0.716773, recon: 0.438948, id: 0.582448] time: 0:02:16.700536 \n",
      "[Epoch 0/2] [Batch 38/995] [D loss: 0.793907, acc:  44%] [G loss: 15.405552, adv: 0.795137, recon: 0.618241, id: 0.595084] time: 0:02:20.453536 \n",
      "[Epoch 0/2] [Batch 39/995] [D loss: 0.850166, acc:  37%] [G loss: 9.219768, adv: 0.638670, recon: 0.350508, id: 0.478270] time: 0:02:24.180532 \n",
      "[Epoch 0/2] [Batch 40/995] [D loss: 0.653477, acc:  41%] [G loss: 13.150520, adv: 0.573658, recon: 0.538996, id: 0.461617] time: 0:02:27.895532 \n",
      "[Epoch 0/2] [Batch 41/995] [D loss: 0.762013, acc:  41%] [G loss: 11.139969, adv: 0.853349, recon: 0.418210, id: 0.623873] time: 0:02:31.593533 \n",
      "[Epoch 0/2] [Batch 42/995] [D loss: 0.692793, acc:  43%] [G loss: 9.307887, adv: 0.646852, recon: 0.355830, id: 0.558348] time: 0:02:35.307536 \n",
      "[Epoch 0/2] [Batch 43/995] [D loss: 0.850470, acc:  39%] [G loss: 11.983812, adv: 0.913854, recon: 0.453207, id: 0.531374] time: 0:02:39.010532 \n",
      "[Epoch 0/2] [Batch 44/995] [D loss: 0.800856, acc:  37%] [G loss: 9.603039, adv: 0.608435, recon: 0.367065, id: 0.560221] time: 0:02:42.704533 \n",
      "[Epoch 0/2] [Batch 45/995] [D loss: 0.950470, acc:  39%] [G loss: 10.295898, adv: 0.606718, recon: 0.401135, id: 0.452558] time: 0:02:46.477533 \n",
      "[Epoch 0/2] [Batch 46/995] [D loss: 0.544123, acc:  51%] [G loss: 12.851905, adv: 0.731942, recon: 0.506074, id: 0.656241] time: 0:02:50.318533 \n",
      "[Epoch 0/2] [Batch 47/995] [D loss: 0.713853, acc:  39%] [G loss: 11.725964, adv: 0.676751, recon: 0.458321, id: 0.513455] time: 0:02:54.044536 \n",
      "[Epoch 0/2] [Batch 48/995] [D loss: 0.730457, acc:  42%] [G loss: 11.747266, adv: 0.922565, recon: 0.441164, id: 0.444755] time: 0:02:57.792532 \n",
      "[Epoch 0/2] [Batch 49/995] [D loss: 0.834530, acc:  43%] [G loss: 9.960070, adv: 0.787636, recon: 0.370980, id: 0.558686] time: 0:03:01.515536 \n",
      "[Epoch 0/2] [Batch 50/995] [D loss: 0.745950, acc:  37%] [G loss: 9.282835, adv: 0.502917, recon: 0.371136, id: 0.455404] time: 0:03:05.145532 \n",
      "[Epoch 0/2] [Batch 51/995] [D loss: 0.468513, acc:  48%] [G loss: 12.896807, adv: 0.695555, recon: 0.517449, id: 0.388728] time: 0:03:08.823532 \n",
      "[Epoch 0/2] [Batch 52/995] [D loss: 0.652945, acc:  50%] [G loss: 15.284410, adv: 0.822833, recon: 0.613769, id: 0.554246] time: 0:03:12.412532 \n",
      "[Epoch 0/2] [Batch 53/995] [D loss: 0.545863, acc:  53%] [G loss: 14.088789, adv: 0.833141, recon: 0.559104, id: 0.514667] time: 0:03:15.988532 \n",
      "[Epoch 0/2] [Batch 54/995] [D loss: 0.675572, acc:  42%] [G loss: 9.814735, adv: 0.638834, recon: 0.382942, id: 0.388679] time: 0:03:19.481535 \n",
      "[Epoch 0/2] [Batch 55/995] [D loss: 0.478076, acc:  48%] [G loss: 10.170794, adv: 0.530326, recon: 0.403587, id: 0.545147] time: 0:03:22.944533 \n",
      "[Epoch 0/2] [Batch 56/995] [D loss: 0.806116, acc:  46%] [G loss: 11.389519, adv: 0.696056, recon: 0.447007, id: 0.577334] time: 0:03:26.467533 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/2] [Batch 57/995] [D loss: 0.580573, acc:  48%] [G loss: 13.380840, adv: 0.654946, recon: 0.546718, id: 0.508078] time: 0:03:29.930535 \n",
      "[Epoch 0/2] [Batch 58/995] [D loss: 0.659199, acc:  39%] [G loss: 12.706997, adv: 0.539571, recon: 0.525923, id: 0.381460] time: 0:03:33.382532 \n",
      "[Epoch 0/2] [Batch 59/995] [D loss: 0.692901, acc:  40%] [G loss: 14.649514, adv: 0.488370, recon: 0.619799, id: 0.626680] time: 0:03:36.874532 \n",
      "[Epoch 0/2] [Batch 60/995] [D loss: 0.687771, acc:  39%] [G loss: 9.404800, adv: 0.677464, recon: 0.363756, id: 0.372094] time: 0:03:40.271532 \n",
      "[Epoch 0/2] [Batch 61/995] [D loss: 0.807573, acc:  39%] [G loss: 9.734346, adv: 0.772186, recon: 0.370794, id: 0.372183] time: 0:03:43.628533 \n",
      "[Epoch 0/2] [Batch 62/995] [D loss: 0.926175, acc:  39%] [G loss: 9.457861, adv: 0.506066, recon: 0.381674, id: 0.479276] time: 0:03:47.032535 \n",
      "[Epoch 0/2] [Batch 63/995] [D loss: 0.622785, acc:  41%] [G loss: 10.122426, adv: 0.717768, recon: 0.394109, id: 0.338855] time: 0:03:50.362532 \n",
      "[Epoch 0/2] [Batch 64/995] [D loss: 0.806207, acc:  37%] [G loss: 10.396222, adv: 0.640393, recon: 0.414567, id: 0.453852] time: 0:03:53.675535 \n",
      "[Epoch 0/2] [Batch 65/995] [D loss: 0.558716, acc:  48%] [G loss: 8.853960, adv: 0.616266, recon: 0.347412, id: 0.283894] time: 0:03:56.973532 \n",
      "[Epoch 0/2] [Batch 66/995] [D loss: 0.712248, acc:  46%] [G loss: 8.574998, adv: 0.787517, recon: 0.320760, id: 0.253170] time: 0:04:00.252532 \n",
      "[Epoch 0/2] [Batch 67/995] [D loss: 0.775380, acc:  39%] [G loss: 9.424336, adv: 0.708696, recon: 0.363837, id: 0.360178] time: 0:04:03.586532 \n",
      "[Epoch 0/2] [Batch 68/995] [D loss: 0.677300, acc:  41%] [G loss: 9.582786, adv: 0.533572, recon: 0.388184, id: 0.398173] time: 0:04:06.851533 \n",
      "[Epoch 0/2] [Batch 69/995] [D loss: 0.483280, acc:  44%] [G loss: 12.991266, adv: 0.570894, recon: 0.539252, id: 0.378322] time: 0:04:10.314531 \n",
      "[Epoch 0/2] [Batch 70/995] [D loss: 0.466403, acc:  51%] [G loss: 11.392983, adv: 0.671805, recon: 0.455952, id: 0.360751] time: 0:04:13.637533 \n",
      "[Epoch 0/2] [Batch 71/995] [D loss: 0.602409, acc:  44%] [G loss: 9.857615, adv: 0.665825, recon: 0.387614, id: 0.387441] time: 0:04:16.879532 \n",
      "[Epoch 0/2] [Batch 72/995] [D loss: 0.727560, acc:  37%] [G loss: 10.135297, adv: 0.710117, recon: 0.396958, id: 0.306697] time: 0:04:20.118532 \n",
      "[Epoch 0/2] [Batch 73/995] [D loss: 0.634758, acc:  36%] [G loss: 9.280550, adv: 0.818597, recon: 0.347898, id: 0.343948] time: 0:04:23.335535 \n",
      "[Epoch 0/2] [Batch 74/995] [D loss: 0.554967, acc:  58%] [G loss: 8.768783, adv: 0.808760, recon: 0.323389, id: 0.329688] time: 0:04:26.566532 \n",
      "[Epoch 0/2] [Batch 75/995] [D loss: 0.762369, acc:  38%] [G loss: 8.902344, adv: 0.612834, recon: 0.348330, id: 0.348810] time: 0:04:29.880532 \n",
      "[Epoch 0/2] [Batch 76/995] [D loss: 0.706965, acc:  39%] [G loss: 8.662824, adv: 0.594391, recon: 0.340596, id: 0.276636] time: 0:04:33.242077 \n",
      "[Epoch 0/2] [Batch 77/995] [D loss: 0.567920, acc:  42%] [G loss: 8.175052, adv: 0.628962, recon: 0.314247, id: 0.300806] time: 0:04:36.616075 \n",
      "[Epoch 0/2] [Batch 78/995] [D loss: 0.518454, acc:  49%] [G loss: 7.996388, adv: 0.609829, recon: 0.309558, id: 0.280057] time: 0:04:40.238079 \n",
      "[Epoch 0/2] [Batch 79/995] [D loss: 0.642731, acc:  42%] [G loss: 11.989073, adv: 0.561993, recon: 0.496541, id: 0.573083] time: 0:04:43.681075 \n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "train(epochs=epochs, batch_size=1, sample_interval=200,\n",
    "      g_AB=g_AB, g_BA=g_BA, d_A=d_A, d_B=d_B, combined=combined,\n",
    "      dataset_name=dataset_name, img_rows=img_rows, img_cols=img_cols\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
