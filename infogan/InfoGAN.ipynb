{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfoGAN\n",
    "\n",
    "Ref.:\n",
    "CHEN, Xi et al.  \n",
    "Infogan: Interpretable representation learning by information maximizing generative adversarial nets.  \n",
    "In: Advances in neural information processing systems. 2016. p. 2172-2180.\n",
    "\n",
    "![InfoGAN architecture](arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(channels, latent_dim):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=latent_dim))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Conv2D(channels, kernel_size=3, padding='same'))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "\n",
    "    gen_input = Input(shape=(latent_dim,))\n",
    "    img = model(gen_input)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return Model(gen_input, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_disk_and_q_net(img_shape, num_classes):\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "\n",
    "    # Shared layers between discriminator and recognition network\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=3, strides=2, input_shape=img_shape, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Conv2D(256, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Conv2D(512, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    img_embedding = model(img)\n",
    "\n",
    "    # Discriminator\n",
    "    validity = Dense(1, activation='sigmoid')(img_embedding)\n",
    "\n",
    "    # Recognition\n",
    "    q_net = Dense(128, activation='relu')(img_embedding)\n",
    "    label = Dense(num_classes, activation='softmax')(q_net)\n",
    "\n",
    "    # Return discriminator and recognition network\n",
    "    return Model(img, validity), Model(img, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save D, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, model_name):\n",
    "    model_path = \"saved_model/%s.json\" % model_name\n",
    "    weights_path = \"saved_model/%s_weights.hdf5\" % model_name\n",
    "    options = {\"file_arch\": model_path,\n",
    "                \"file_weight\": weights_path}\n",
    "    json_string = model.to_json()\n",
    "    open(options['file_arch'], 'w').write(json_string)\n",
    "    model.save_weights(options['file_weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(G, D):\n",
    "    save(G, \"generator\")\n",
    "    save(D, \"discriminator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_info_loss(c, c_given_x):\n",
    "    \"\"\"The mutual information metric we aim to minimize\"\"\"\n",
    "    eps = 1e-8\n",
    "    conditional_entropy = K.mean(- K.sum(K.log(c_given_x + eps) * c, axis=1))\n",
    "    entropy = K.mean(- K.sum(K.log(c + eps) * c, axis=1))\n",
    "\n",
    "    return conditional_entropy + entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_generator_input(num_classes, batch_size):\n",
    "    # Generator inputs\n",
    "    sampled_noise = np.random.normal(0, 1, (batch_size, 62))\n",
    "    sampled_labels = np.random.randint(0, num_classes, batch_size).reshape(-1, 1)\n",
    "    sampled_labels = to_categorical(sampled_labels, num_classes=num_classes)\n",
    "\n",
    "    return sampled_noise, sampled_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(G, num_classes, epoch):\n",
    "    r, c = 10, 10\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    for i in range(c):\n",
    "        sampled_noise, _ = sample_generator_input(num_classes, c)\n",
    "        label = to_categorical(np.full(fill_value=i, shape=(r,1)), num_classes=num_classes)\n",
    "        gen_input = np.concatenate((sampled_noise, label), axis=1)\n",
    "        gen_imgs = G.predict(gen_input)\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        for j in range(r):\n",
    "            axs[j,i].imshow(gen_imgs[j,:,:,0], cmap='gray')\n",
    "            axs[j,i].axis('off')\n",
    "    fig.savefig(\"images/%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(D, G, combined,\n",
    "          num_classes,\n",
    "          epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "    # Load the dataset\n",
    "    (X_train, y_train), (_, _) = mnist.load_data()\n",
    "\n",
    "    # Rescale -1 to 1\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "    y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "    # Adversarial ground truths\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        # Select a random half batch of images\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        imgs = X_train[idx]\n",
    "\n",
    "        # Sample noise and categorical labels\n",
    "        sampled_noise, sampled_labels = sample_generator_input(num_classes, batch_size)\n",
    "        gen_input = np.concatenate((sampled_noise, sampled_labels), axis=1)\n",
    "\n",
    "        # Generate a half batch of new images\n",
    "        gen_imgs = G.predict(gen_input)\n",
    "\n",
    "        # Train on real and generated data\n",
    "        d_loss_real = D.train_on_batch(imgs, valid)\n",
    "        d_loss_fake = D.train_on_batch(gen_imgs, fake)\n",
    "\n",
    "        # Avg. loss\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator and Q-network\n",
    "        # ---------------------\n",
    "\n",
    "        g_loss = combined.train_on_batch(gen_input, [valid, sampled_labels])\n",
    "\n",
    "        # Plot the progress\n",
    "        print (\"%d [D loss: %.2f, acc.: %.2f%%] [Q loss: %.2f] [G loss: %.2f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[1], g_loss[2]))\n",
    "\n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % sample_interval == 0 or epoch == epochs - 1:\n",
    "            sample_images(G, num_classes, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dirs\n",
    "for d in ['images', 'saved_model']:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "num_classes = 10\n",
    "\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "latent_dim = 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(0.0002, 0.5)\n",
    "losses = ['binary_crossentropy', mutual_info_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Build and the discriminator and recognition network\n",
    "D, auxilliary = build_disk_and_q_net(img_shape, num_classes)\n",
    "\n",
    "D.compile(loss=['binary_crossentropy'],\n",
    "          optimizer=optimizer,\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "# Build and compile the recognition network Q\n",
    "auxilliary.compile(loss=[mutual_info_loss],\n",
    "                   optimizer=optimizer,\n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 6272)              457856    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 64)        73792     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 681,089\n",
      "Trainable params: 680,449\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the generator\n",
    "G = build_generator(channels, latent_dim)\n",
    "\n",
    "# The generator takes noise and the target label as input\n",
    "# and generates the corresponding digit of that label\n",
    "gen_input = Input(shape=(latent_dim,))\n",
    "img = G(gen_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the combined model we will only train the generator\n",
    "D.trainable = False\n",
    "\n",
    "# The discriminator takes generated image as input and determines validity\n",
    "valid = D(img)\n",
    "# The recognition network produces the label\n",
    "target_label = auxilliary(img)\n",
    "\n",
    "# The combined model  (stacked generator and discriminator)\n",
    "combined = Model(gen_input, [valid, target_label])\n",
    "combined.compile(loss=losses, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=50000\n",
    "epochs=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 1.16, acc.: 34.77%] [Q loss: 0.67] [G loss: 2.98]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 2.72, acc.: 50.00%] [Q loss: 0.70] [G loss: 3.06]\n",
      "2 [D loss: 1.29, acc.: 52.73%] [Q loss: 0.99] [G loss: 2.96]\n",
      "3 [D loss: 0.16, acc.: 93.75%] [Q loss: 0.62] [G loss: 2.69]\n",
      "4 [D loss: 0.12, acc.: 96.48%] [Q loss: 0.20] [G loss: 2.83]\n",
      "5 [D loss: 0.11, acc.: 97.27%] [Q loss: 0.08] [G loss: 2.70]\n",
      "6 [D loss: 0.05, acc.: 99.22%] [Q loss: 0.02] [G loss: 2.58]\n",
      "7 [D loss: 0.04, acc.: 99.22%] [Q loss: 0.01] [G loss: 2.68]\n",
      "8 [D loss: 0.03, acc.: 99.61%] [Q loss: 0.01] [G loss: 2.56]\n",
      "9 [D loss: 0.03, acc.: 99.61%] [Q loss: 0.02] [G loss: 2.56]\n",
      "10 [D loss: 0.03, acc.: 99.61%] [Q loss: 0.00] [G loss: 2.73]\n",
      "11 [D loss: 0.02, acc.: 99.61%] [Q loss: 0.00] [G loss: 2.69]\n",
      "12 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.00] [G loss: 2.34]\n",
      "13 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.00] [G loss: 2.57]\n",
      "14 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.00] [G loss: 2.41]\n",
      "15 [D loss: 0.02, acc.: 99.22%] [Q loss: 0.00] [G loss: 2.46]\n",
      "16 [D loss: 0.03, acc.: 99.61%] [Q loss: 0.00] [G loss: 2.41]\n",
      "17 [D loss: 0.03, acc.: 99.22%] [Q loss: 0.00] [G loss: 2.26]\n",
      "18 [D loss: 0.06, acc.: 97.66%] [Q loss: 0.00] [G loss: 2.42]\n",
      "19 [D loss: 0.09, acc.: 97.66%] [Q loss: 0.00] [G loss: 2.44]\n",
      "20 [D loss: 0.19, acc.: 92.19%] [Q loss: 0.01] [G loss: 2.35]\n",
      "21 [D loss: 0.59, acc.: 77.34%] [Q loss: 0.03] [G loss: 2.28]\n",
      "22 [D loss: 0.77, acc.: 64.45%] [Q loss: 0.03] [G loss: 2.19]\n",
      "23 [D loss: 0.43, acc.: 80.08%] [Q loss: 0.15] [G loss: 2.40]\n",
      "24 [D loss: 1.29, acc.: 48.05%] [Q loss: 0.13] [G loss: 2.30]\n",
      "25 [D loss: 0.61, acc.: 72.66%] [Q loss: 0.76] [G loss: 2.27]\n",
      "26 [D loss: 1.06, acc.: 59.77%] [Q loss: 0.80] [G loss: 2.22]\n",
      "27 [D loss: 1.51, acc.: 51.17%] [Q loss: 1.24] [G loss: 2.37]\n",
      "28 [D loss: 1.45, acc.: 51.17%] [Q loss: 1.94] [G loss: 2.26]\n",
      "29 [D loss: 1.87, acc.: 46.09%] [Q loss: 1.90] [G loss: 2.16]\n",
      "30 [D loss: 1.87, acc.: 47.27%] [Q loss: 1.61] [G loss: 2.17]\n",
      "31 [D loss: 1.60, acc.: 50.78%] [Q loss: 1.93] [G loss: 2.14]\n",
      "32 [D loss: 1.23, acc.: 52.34%] [Q loss: 2.00] [G loss: 2.10]\n",
      "33 [D loss: 1.17, acc.: 52.73%] [Q loss: 1.85] [G loss: 2.16]\n",
      "34 [D loss: 1.12, acc.: 53.12%] [Q loss: 1.84] [G loss: 2.10]\n",
      "35 [D loss: 0.92, acc.: 55.47%] [Q loss: 1.75] [G loss: 2.00]\n",
      "36 [D loss: 0.94, acc.: 55.08%] [Q loss: 1.67] [G loss: 2.06]\n",
      "37 [D loss: 0.91, acc.: 54.30%] [Q loss: 1.58] [G loss: 2.05]\n",
      "38 [D loss: 0.74, acc.: 60.55%] [Q loss: 1.56] [G loss: 2.00]\n",
      "39 [D loss: 0.83, acc.: 57.42%] [Q loss: 1.50] [G loss: 2.09]\n",
      "40 [D loss: 0.78, acc.: 58.98%] [Q loss: 1.58] [G loss: 1.93]\n",
      "41 [D loss: 0.69, acc.: 61.33%] [Q loss: 1.69] [G loss: 2.06]\n",
      "42 [D loss: 0.62, acc.: 63.28%] [Q loss: 1.59] [G loss: 1.84]\n",
      "43 [D loss: 0.68, acc.: 59.77%] [Q loss: 1.66] [G loss: 1.82]\n",
      "44 [D loss: 0.74, acc.: 59.38%] [Q loss: 1.57] [G loss: 1.79]\n",
      "45 [D loss: 0.63, acc.: 63.67%] [Q loss: 1.69] [G loss: 1.83]\n",
      "46 [D loss: 0.62, acc.: 64.84%] [Q loss: 1.56] [G loss: 1.65]\n",
      "47 [D loss: 0.59, acc.: 66.41%] [Q loss: 1.52] [G loss: 1.68]\n",
      "48 [D loss: 0.69, acc.: 62.11%] [Q loss: 1.43] [G loss: 1.51]\n",
      "49 [D loss: 0.59, acc.: 65.62%] [Q loss: 1.56] [G loss: 1.38]\n"
     ]
    }
   ],
   "source": [
    "train(D, G, combined, num_classes,\n",
    "      epochs=epochs, batch_size=128, sample_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
