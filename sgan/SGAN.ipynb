{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(latent_dim):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=latent_dim))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Conv2D(1, kernel_size=3, padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "    img = model(noise)\n",
    "\n",
    "    return Model(noise, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(img_shape, num_classes):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "\n",
    "    features = model(img)\n",
    "    valid = Dense(1, activation=\"sigmoid\")(features)\n",
    "    label = Dense(num_classes + 1, activation=\"softmax\")(features)\n",
    "\n",
    "    return Model(img, [valid, label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(G, latent_dim, epoch):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
    "    gen_imgs = G.predict(noise)\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, model_name):\n",
    "    model_path = \"saved_model/%s.json\" % model_name\n",
    "    weights_path = \"saved_model/%s_weights.hdf5\" % model_name\n",
    "    options = {\"file_arch\": model_path,\n",
    "                \"file_weight\": weights_path}\n",
    "    json_string = model.to_json()\n",
    "    open(options['file_arch'], 'w').write(json_string)\n",
    "    model.save_weights(options['file_weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(G, D, combined):\n",
    "    save(G, \"mnist_sgan_generator\")\n",
    "    save(D, \"mnist_sgan_discriminator\")\n",
    "    save(combined, \"mnist_sgan_adversarial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(G, D, combined,\n",
    "          num_classes, latent_dim,\n",
    "          epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "    # Load the dataset\n",
    "    (X_train, y_train), (_, _) = mnist.load_data()\n",
    "\n",
    "    # Rescale -1 to 1\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "    y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "    # Class weights:\n",
    "    # To balance the difference in occurences of digit class labels.\n",
    "    # 50% of labels that the discriminator trains on are 'fake'.\n",
    "    # Weight = 1 / frequency\n",
    "    half_batch = batch_size // 2\n",
    "    cw1 = {0: 1, 1: 1}\n",
    "    cw2 = {i: num_classes / half_batch for i in range(num_classes)}\n",
    "    cw2[num_classes] = 1 / half_batch\n",
    "\n",
    "    # Adversarial ground truths\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        # Select a random batch of images\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        imgs = X_train[idx]\n",
    "\n",
    "        # Sample noise and generate a batch of new images\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        gen_imgs = G.predict(noise)\n",
    "\n",
    "        # One-hot encoding of labels\n",
    "        labels = to_categorical(y_train[idx], num_classes=num_classes + 1)\n",
    "        fake_labels = to_categorical(np.full((batch_size, 1), num_classes), num_classes=num_classes + 1)\n",
    "\n",
    "        # Train the discriminator\n",
    "        d_loss_real = D.train_on_batch(imgs, [valid, labels], class_weight=[cw1, cw2])\n",
    "        d_loss_fake = D.train_on_batch(gen_imgs, [fake, fake_labels], class_weight=[cw1, cw2])\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "\n",
    "        g_loss = combined.train_on_batch(noise, valid, class_weight=[cw1, cw2])\n",
    "\n",
    "        # Plot the progress\n",
    "        print (\"%d [D loss: %f, acc: %.2f%%, op_acc: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[3], 100*d_loss[4], g_loss))\n",
    "\n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % sample_interval == 0 or epoch == epochs - 1:\n",
    "            sample_images(G, latent_dim, epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in ['images']:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "num_classes = 10\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# create optimizer\n",
    "optimizer = Adam(0.0002, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "=================================================================\n",
      "Total params: 388,608\n",
      "Trainable params: 388,224\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Build and compile the discriminator\n",
    "D = build_discriminator(img_shape, num_classes)\n",
    "D.compile(loss=['binary_crossentropy', 'categorical_crossentropy'],\n",
    "          loss_weights=[0.5, 0.5],\n",
    "          optimizer=optimizer,\n",
    "          metrics=['accuracy']\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 64)        73792     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 856,705\n",
      "Trainable params: 856,065\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the generator\n",
    "G = build_generator(latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generator takes noise as input and generates imgs\n",
    "noise = Input(shape=(100,))\n",
    "img = G(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the combined model we will only train the generator\n",
    "D.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The valid takes generated images as input and determines validity\n",
    "valid, _ = D(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The combined model  (stacked generator and discriminator)\n",
    "# Trains generator to fool discriminator\n",
    "combined = Model(noise, valid)\n",
    "combined.compile(loss=['binary_crossentropy'], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.962485, acc: 46.88%, op_acc: 12.50%] [G loss: 0.307589]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 0.842761, acc: 51.56%, op_acc: 6.25%] [G loss: 0.434881]\n",
      "2 [D loss: 0.672172, acc: 79.69%, op_acc: 14.06%] [G loss: 0.575161]\n",
      "3 [D loss: 0.557908, acc: 100.00%, op_acc: 12.50%] [G loss: 0.578829]\n",
      "4 [D loss: 0.499317, acc: 100.00%, op_acc: 23.44%] [G loss: 0.686973]\n",
      "5 [D loss: 0.437283, acc: 100.00%, op_acc: 37.50%] [G loss: 0.596043]\n",
      "6 [D loss: 0.435212, acc: 98.44%, op_acc: 46.88%] [G loss: 0.562973]\n",
      "7 [D loss: 0.540383, acc: 89.06%, op_acc: 34.38%] [G loss: 0.833444]\n",
      "8 [D loss: 0.523059, acc: 82.81%, op_acc: 28.12%] [G loss: 1.060906]\n",
      "9 [D loss: 0.586833, acc: 76.56%, op_acc: 25.00%] [G loss: 1.283327]\n",
      "10 [D loss: 0.543046, acc: 79.69%, op_acc: 31.25%] [G loss: 1.563692]\n",
      "11 [D loss: 0.614470, acc: 70.31%, op_acc: 29.69%] [G loss: 1.464273]\n",
      "12 [D loss: 0.581043, acc: 79.69%, op_acc: 15.62%] [G loss: 1.326742]\n",
      "13 [D loss: 0.510187, acc: 84.38%, op_acc: 28.12%] [G loss: 1.334987]\n",
      "14 [D loss: 0.576156, acc: 85.94%, op_acc: 31.25%] [G loss: 1.032913]\n",
      "15 [D loss: 0.516727, acc: 85.94%, op_acc: 31.25%] [G loss: 0.766213]\n",
      "16 [D loss: 0.473971, acc: 92.19%, op_acc: 35.94%] [G loss: 0.688403]\n",
      "17 [D loss: 0.505092, acc: 85.94%, op_acc: 37.50%] [G loss: 0.432941]\n",
      "18 [D loss: 0.554751, acc: 79.69%, op_acc: 31.25%] [G loss: 0.509824]\n",
      "19 [D loss: 0.521875, acc: 78.12%, op_acc: 39.06%] [G loss: 0.606693]\n",
      "20 [D loss: 0.560527, acc: 73.44%, op_acc: 42.19%] [G loss: 0.384286]\n",
      "21 [D loss: 0.656805, acc: 70.31%, op_acc: 29.69%] [G loss: 0.347501]\n",
      "22 [D loss: 0.595077, acc: 67.19%, op_acc: 39.06%] [G loss: 0.628996]\n",
      "23 [D loss: 0.769718, acc: 45.31%, op_acc: 29.69%] [G loss: 0.541596]\n",
      "24 [D loss: 0.706770, acc: 54.69%, op_acc: 31.25%] [G loss: 0.681450]\n",
      "25 [D loss: 0.885981, acc: 32.81%, op_acc: 18.75%] [G loss: 0.971869]\n",
      "26 [D loss: 0.779299, acc: 43.75%, op_acc: 28.12%] [G loss: 1.286430]\n",
      "27 [D loss: 0.760639, acc: 50.00%, op_acc: 21.88%] [G loss: 1.264707]\n",
      "28 [D loss: 0.646618, acc: 60.94%, op_acc: 28.12%] [G loss: 1.507465]\n",
      "29 [D loss: 0.597702, acc: 60.94%, op_acc: 39.06%] [G loss: 1.827229]\n",
      "30 [D loss: 0.616918, acc: 64.06%, op_acc: 32.81%] [G loss: 1.676387]\n",
      "31 [D loss: 0.603850, acc: 56.25%, op_acc: 42.19%] [G loss: 1.640865]\n",
      "32 [D loss: 0.496907, acc: 76.56%, op_acc: 45.31%] [G loss: 2.078653]\n",
      "33 [D loss: 0.488372, acc: 85.94%, op_acc: 37.50%] [G loss: 1.961503]\n",
      "34 [D loss: 0.473337, acc: 78.12%, op_acc: 45.31%] [G loss: 2.246420]\n",
      "35 [D loss: 0.449247, acc: 85.94%, op_acc: 35.94%] [G loss: 1.878976]\n",
      "36 [D loss: 0.404619, acc: 90.62%, op_acc: 43.75%] [G loss: 2.094285]\n",
      "37 [D loss: 0.439125, acc: 89.06%, op_acc: 53.12%] [G loss: 2.060193]\n",
      "38 [D loss: 0.347006, acc: 95.31%, op_acc: 45.31%] [G loss: 2.279717]\n",
      "39 [D loss: 0.310347, acc: 95.31%, op_acc: 60.94%] [G loss: 2.360281]\n",
      "40 [D loss: 0.266085, acc: 96.88%, op_acc: 70.31%] [G loss: 2.620378]\n",
      "41 [D loss: 0.326727, acc: 95.31%, op_acc: 67.19%] [G loss: 2.719474]\n",
      "42 [D loss: 0.344131, acc: 93.75%, op_acc: 67.19%] [G loss: 2.430474]\n",
      "43 [D loss: 0.361892, acc: 92.19%, op_acc: 64.06%] [G loss: 2.351316]\n",
      "44 [D loss: 0.228080, acc: 98.44%, op_acc: 84.38%] [G loss: 2.822970]\n",
      "45 [D loss: 0.312617, acc: 93.75%, op_acc: 65.62%] [G loss: 2.706007]\n",
      "46 [D loss: 0.282520, acc: 96.88%, op_acc: 62.50%] [G loss: 2.634026]\n",
      "47 [D loss: 0.294138, acc: 95.31%, op_acc: 71.88%] [G loss: 2.806343]\n",
      "48 [D loss: 0.243077, acc: 98.44%, op_acc: 73.44%] [G loss: 2.400892]\n",
      "49 [D loss: 0.374490, acc: 95.31%, op_acc: 50.00%] [G loss: 2.514982]\n",
      "50 [D loss: 0.240431, acc: 93.75%, op_acc: 78.12%] [G loss: 2.695771]\n",
      "51 [D loss: 0.229380, acc: 98.44%, op_acc: 73.44%] [G loss: 2.330443]\n",
      "52 [D loss: 0.221975, acc: 100.00%, op_acc: 76.56%] [G loss: 2.521952]\n",
      "53 [D loss: 0.256608, acc: 98.44%, op_acc: 68.75%] [G loss: 2.744775]\n",
      "54 [D loss: 0.266818, acc: 95.31%, op_acc: 68.75%] [G loss: 2.477410]\n",
      "55 [D loss: 0.239838, acc: 96.88%, op_acc: 78.12%] [G loss: 2.659765]\n",
      "56 [D loss: 0.216493, acc: 96.88%, op_acc: 68.75%] [G loss: 2.850373]\n",
      "57 [D loss: 0.250383, acc: 95.31%, op_acc: 75.00%] [G loss: 2.918981]\n",
      "58 [D loss: 0.227314, acc: 98.44%, op_acc: 82.81%] [G loss: 2.910927]\n",
      "59 [D loss: 0.202706, acc: 100.00%, op_acc: 75.00%] [G loss: 2.613405]\n",
      "60 [D loss: 0.188346, acc: 98.44%, op_acc: 78.12%] [G loss: 2.904491]\n",
      "61 [D loss: 0.211228, acc: 98.44%, op_acc: 75.00%] [G loss: 2.679279]\n",
      "62 [D loss: 0.241926, acc: 98.44%, op_acc: 68.75%] [G loss: 2.621681]\n",
      "63 [D loss: 0.194437, acc: 100.00%, op_acc: 75.00%] [G loss: 2.510894]\n",
      "64 [D loss: 0.200180, acc: 98.44%, op_acc: 76.56%] [G loss: 2.562170]\n",
      "65 [D loss: 0.209065, acc: 98.44%, op_acc: 79.69%] [G loss: 2.869684]\n",
      "66 [D loss: 0.230479, acc: 100.00%, op_acc: 71.88%] [G loss: 2.923221]\n",
      "67 [D loss: 0.191059, acc: 98.44%, op_acc: 81.25%] [G loss: 3.122941]\n",
      "68 [D loss: 0.247701, acc: 98.44%, op_acc: 68.75%] [G loss: 2.963795]\n",
      "69 [D loss: 0.243283, acc: 100.00%, op_acc: 70.31%] [G loss: 3.291347]\n",
      "70 [D loss: 0.202898, acc: 98.44%, op_acc: 78.12%] [G loss: 2.945928]\n",
      "71 [D loss: 0.186914, acc: 96.88%, op_acc: 85.94%] [G loss: 3.489218]\n",
      "72 [D loss: 0.176087, acc: 98.44%, op_acc: 81.25%] [G loss: 3.132158]\n",
      "73 [D loss: 0.188180, acc: 95.31%, op_acc: 85.94%] [G loss: 2.564921]\n",
      "74 [D loss: 0.210836, acc: 93.75%, op_acc: 79.69%] [G loss: 2.640639]\n",
      "75 [D loss: 0.151557, acc: 98.44%, op_acc: 81.25%] [G loss: 2.582687]\n",
      "76 [D loss: 0.201259, acc: 100.00%, op_acc: 81.25%] [G loss: 2.206293]\n",
      "77 [D loss: 0.193398, acc: 95.31%, op_acc: 76.56%] [G loss: 2.537786]\n",
      "78 [D loss: 0.240940, acc: 96.88%, op_acc: 64.06%] [G loss: 3.037982]\n",
      "79 [D loss: 0.265053, acc: 95.31%, op_acc: 68.75%] [G loss: 2.616580]\n",
      "80 [D loss: 0.274989, acc: 93.75%, op_acc: 65.62%] [G loss: 2.400091]\n",
      "81 [D loss: 0.145558, acc: 98.44%, op_acc: 87.50%] [G loss: 2.831394]\n",
      "82 [D loss: 0.203811, acc: 100.00%, op_acc: 81.25%] [G loss: 2.262944]\n",
      "83 [D loss: 0.225693, acc: 95.31%, op_acc: 71.88%] [G loss: 2.555130]\n",
      "84 [D loss: 0.154203, acc: 100.00%, op_acc: 76.56%] [G loss: 2.552141]\n",
      "85 [D loss: 0.244204, acc: 98.44%, op_acc: 68.75%] [G loss: 2.454132]\n",
      "86 [D loss: 0.242473, acc: 98.44%, op_acc: 68.75%] [G loss: 2.048013]\n",
      "87 [D loss: 0.325153, acc: 87.50%, op_acc: 51.56%] [G loss: 2.551873]\n",
      "88 [D loss: 0.232037, acc: 90.62%, op_acc: 82.81%] [G loss: 2.348105]\n",
      "89 [D loss: 0.245519, acc: 90.62%, op_acc: 60.94%] [G loss: 2.707512]\n",
      "90 [D loss: 0.234861, acc: 98.44%, op_acc: 68.75%] [G loss: 3.095700]\n",
      "91 [D loss: 0.262313, acc: 93.75%, op_acc: 60.94%] [G loss: 2.435633]\n",
      "92 [D loss: 0.293332, acc: 89.06%, op_acc: 70.31%] [G loss: 2.771970]\n",
      "93 [D loss: 0.304976, acc: 81.25%, op_acc: 60.94%] [G loss: 3.738369]\n",
      "94 [D loss: 0.670264, acc: 42.19%, op_acc: 46.88%] [G loss: 1.861264]\n",
      "95 [D loss: 0.322437, acc: 87.50%, op_acc: 57.81%] [G loss: 3.118106]\n",
      "96 [D loss: 0.463010, acc: 68.75%, op_acc: 45.31%] [G loss: 3.479836]\n",
      "97 [D loss: 0.552987, acc: 43.75%, op_acc: 56.25%] [G loss: 2.702317]\n",
      "98 [D loss: 0.525378, acc: 64.06%, op_acc: 46.88%] [G loss: 4.115396]\n",
      "99 [D loss: 0.529396, acc: 59.38%, op_acc: 53.12%] [G loss: 2.742138]\n",
      "100 [D loss: 0.540305, acc: 67.19%, op_acc: 46.88%] [G loss: 2.689099]\n",
      "101 [D loss: 0.539132, acc: 54.69%, op_acc: 45.31%] [G loss: 2.030451]\n",
      "102 [D loss: 0.473494, acc: 70.31%, op_acc: 48.44%] [G loss: 2.581322]\n",
      "103 [D loss: 0.590866, acc: 48.44%, op_acc: 56.25%] [G loss: 2.159045]\n",
      "104 [D loss: 0.531078, acc: 57.81%, op_acc: 46.88%] [G loss: 2.687711]\n",
      "105 [D loss: 0.623985, acc: 48.44%, op_acc: 46.88%] [G loss: 1.693286]\n",
      "106 [D loss: 0.659297, acc: 48.44%, op_acc: 46.88%] [G loss: 2.220728]\n",
      "107 [D loss: 0.641276, acc: 53.12%, op_acc: 45.31%] [G loss: 1.926627]\n",
      "108 [D loss: 0.563640, acc: 51.56%, op_acc: 46.88%] [G loss: 1.880270]\n",
      "109 [D loss: 0.492905, acc: 60.94%, op_acc: 40.62%] [G loss: 2.019819]\n",
      "110 [D loss: 0.607431, acc: 54.69%, op_acc: 40.62%] [G loss: 1.810089]\n",
      "111 [D loss: 0.561958, acc: 48.44%, op_acc: 50.00%] [G loss: 1.867726]\n",
      "112 [D loss: 0.568181, acc: 48.44%, op_acc: 50.00%] [G loss: 1.482523]\n",
      "113 [D loss: 0.737824, acc: 28.12%, op_acc: 51.56%] [G loss: 1.719325]\n",
      "114 [D loss: 0.596683, acc: 59.38%, op_acc: 42.19%] [G loss: 1.745723]\n",
      "115 [D loss: 0.584366, acc: 42.19%, op_acc: 43.75%] [G loss: 1.480641]\n",
      "116 [D loss: 0.550355, acc: 54.69%, op_acc: 53.12%] [G loss: 1.662593]\n",
      "117 [D loss: 0.683782, acc: 39.06%, op_acc: 43.75%] [G loss: 1.406857]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118 [D loss: 0.636286, acc: 39.06%, op_acc: 50.00%] [G loss: 1.242940]\n",
      "119 [D loss: 0.678222, acc: 39.06%, op_acc: 40.62%] [G loss: 1.422840]\n",
      "120 [D loss: 0.634135, acc: 46.88%, op_acc: 45.31%] [G loss: 1.611099]\n",
      "121 [D loss: 0.662601, acc: 46.88%, op_acc: 48.44%] [G loss: 1.380791]\n",
      "122 [D loss: 0.636283, acc: 48.44%, op_acc: 42.19%] [G loss: 1.319250]\n",
      "123 [D loss: 0.714793, acc: 42.19%, op_acc: 37.50%] [G loss: 1.306468]\n",
      "124 [D loss: 0.667188, acc: 48.44%, op_acc: 34.38%] [G loss: 1.280994]\n",
      "125 [D loss: 0.722268, acc: 45.31%, op_acc: 43.75%] [G loss: 1.184548]\n",
      "126 [D loss: 0.590395, acc: 53.12%, op_acc: 35.94%] [G loss: 1.298957]\n",
      "127 [D loss: 0.574914, acc: 48.44%, op_acc: 56.25%] [G loss: 1.443805]\n",
      "128 [D loss: 0.572099, acc: 40.62%, op_acc: 46.88%] [G loss: 0.981745]\n",
      "129 [D loss: 0.659817, acc: 34.38%, op_acc: 46.88%] [G loss: 1.162366]\n",
      "130 [D loss: 0.691855, acc: 42.19%, op_acc: 42.19%] [G loss: 1.232838]\n",
      "131 [D loss: 0.577897, acc: 45.31%, op_acc: 45.31%] [G loss: 1.419766]\n",
      "132 [D loss: 0.605342, acc: 43.75%, op_acc: 40.62%] [G loss: 1.476915]\n",
      "133 [D loss: 0.600624, acc: 50.00%, op_acc: 45.31%] [G loss: 1.415817]\n",
      "134 [D loss: 0.475520, acc: 57.81%, op_acc: 45.31%] [G loss: 1.231067]\n",
      "135 [D loss: 0.569030, acc: 46.88%, op_acc: 45.31%] [G loss: 1.499333]\n",
      "136 [D loss: 0.564608, acc: 48.44%, op_acc: 46.88%] [G loss: 1.190982]\n",
      "137 [D loss: 0.631844, acc: 37.50%, op_acc: 43.75%] [G loss: 1.318878]\n",
      "138 [D loss: 0.526585, acc: 46.88%, op_acc: 56.25%] [G loss: 1.400104]\n",
      "139 [D loss: 0.529471, acc: 56.25%, op_acc: 45.31%] [G loss: 1.567781]\n",
      "140 [D loss: 0.515870, acc: 59.38%, op_acc: 50.00%] [G loss: 1.473218]\n",
      "141 [D loss: 0.585329, acc: 45.31%, op_acc: 43.75%] [G loss: 1.585141]\n",
      "142 [D loss: 0.628868, acc: 43.75%, op_acc: 46.88%] [G loss: 1.240713]\n",
      "143 [D loss: 0.507261, acc: 54.69%, op_acc: 48.44%] [G loss: 1.308399]\n",
      "144 [D loss: 0.606793, acc: 40.62%, op_acc: 42.19%] [G loss: 1.473513]\n",
      "145 [D loss: 0.634492, acc: 40.62%, op_acc: 50.00%] [G loss: 1.490929]\n",
      "146 [D loss: 0.595060, acc: 42.19%, op_acc: 45.31%] [G loss: 1.235889]\n",
      "147 [D loss: 0.757297, acc: 35.94%, op_acc: 42.19%] [G loss: 1.179971]\n",
      "148 [D loss: 0.636589, acc: 39.06%, op_acc: 35.94%] [G loss: 1.053152]\n",
      "149 [D loss: 0.509280, acc: 54.69%, op_acc: 50.00%] [G loss: 1.366785]\n",
      "150 [D loss: 0.556410, acc: 57.81%, op_acc: 46.88%] [G loss: 1.518521]\n",
      "151 [D loss: 0.532439, acc: 53.12%, op_acc: 46.88%] [G loss: 1.282357]\n",
      "152 [D loss: 0.565947, acc: 50.00%, op_acc: 54.69%] [G loss: 1.010242]\n",
      "153 [D loss: 0.477097, acc: 51.56%, op_acc: 50.00%] [G loss: 1.367360]\n",
      "154 [D loss: 0.520667, acc: 45.31%, op_acc: 54.69%] [G loss: 1.175901]\n",
      "155 [D loss: 0.537001, acc: 46.88%, op_acc: 57.81%] [G loss: 1.169568]\n",
      "156 [D loss: 0.517218, acc: 54.69%, op_acc: 46.88%] [G loss: 1.250917]\n",
      "157 [D loss: 0.406585, acc: 65.62%, op_acc: 51.56%] [G loss: 1.274161]\n",
      "158 [D loss: 0.511377, acc: 54.69%, op_acc: 53.12%] [G loss: 1.328569]\n",
      "159 [D loss: 0.548389, acc: 54.69%, op_acc: 50.00%] [G loss: 1.481121]\n",
      "160 [D loss: 0.563838, acc: 54.69%, op_acc: 40.62%] [G loss: 1.267908]\n",
      "161 [D loss: 0.506418, acc: 56.25%, op_acc: 53.12%] [G loss: 1.063683]\n",
      "162 [D loss: 0.639421, acc: 42.19%, op_acc: 37.50%] [G loss: 1.299873]\n",
      "163 [D loss: 0.497244, acc: 59.38%, op_acc: 50.00%] [G loss: 1.090665]\n",
      "164 [D loss: 0.518825, acc: 56.25%, op_acc: 50.00%] [G loss: 1.385119]\n",
      "165 [D loss: 0.589473, acc: 45.31%, op_acc: 45.31%] [G loss: 1.087203]\n",
      "166 [D loss: 0.562657, acc: 53.12%, op_acc: 48.44%] [G loss: 1.424352]\n",
      "167 [D loss: 0.541538, acc: 42.19%, op_acc: 51.56%] [G loss: 1.115803]\n",
      "168 [D loss: 0.410023, acc: 62.50%, op_acc: 57.81%] [G loss: 1.151091]\n",
      "169 [D loss: 0.565870, acc: 54.69%, op_acc: 46.88%] [G loss: 1.422865]\n",
      "170 [D loss: 0.519502, acc: 53.12%, op_acc: 50.00%] [G loss: 1.257880]\n",
      "171 [D loss: 0.522616, acc: 56.25%, op_acc: 42.19%] [G loss: 1.464865]\n",
      "172 [D loss: 0.475806, acc: 59.38%, op_acc: 56.25%] [G loss: 1.319970]\n",
      "173 [D loss: 0.599495, acc: 46.88%, op_acc: 48.44%] [G loss: 1.228408]\n",
      "174 [D loss: 0.492958, acc: 50.00%, op_acc: 54.69%] [G loss: 1.192501]\n",
      "175 [D loss: 0.492692, acc: 50.00%, op_acc: 50.00%] [G loss: 1.358724]\n",
      "176 [D loss: 0.624639, acc: 46.88%, op_acc: 43.75%] [G loss: 1.215118]\n",
      "177 [D loss: 0.487211, acc: 54.69%, op_acc: 46.88%] [G loss: 1.211739]\n",
      "178 [D loss: 0.527625, acc: 53.12%, op_acc: 54.69%] [G loss: 1.345042]\n",
      "179 [D loss: 0.532275, acc: 46.88%, op_acc: 48.44%] [G loss: 1.273528]\n",
      "180 [D loss: 0.480884, acc: 56.25%, op_acc: 51.56%] [G loss: 1.148830]\n",
      "181 [D loss: 0.487000, acc: 56.25%, op_acc: 48.44%] [G loss: 1.350991]\n",
      "182 [D loss: 0.497681, acc: 56.25%, op_acc: 51.56%] [G loss: 1.312565]\n",
      "183 [D loss: 0.564401, acc: 45.31%, op_acc: 50.00%] [G loss: 1.307613]\n",
      "184 [D loss: 0.542835, acc: 51.56%, op_acc: 42.19%] [G loss: 1.320544]\n",
      "185 [D loss: 0.514933, acc: 56.25%, op_acc: 45.31%] [G loss: 1.124365]\n",
      "186 [D loss: 0.419615, acc: 68.75%, op_acc: 51.56%] [G loss: 1.339011]\n",
      "187 [D loss: 0.522413, acc: 50.00%, op_acc: 56.25%] [G loss: 1.312731]\n",
      "188 [D loss: 0.526671, acc: 48.44%, op_acc: 50.00%] [G loss: 1.106743]\n",
      "189 [D loss: 0.497204, acc: 64.06%, op_acc: 43.75%] [G loss: 1.224880]\n",
      "190 [D loss: 0.482433, acc: 62.50%, op_acc: 46.88%] [G loss: 1.282491]\n",
      "191 [D loss: 0.437126, acc: 62.50%, op_acc: 53.12%] [G loss: 1.270736]\n",
      "192 [D loss: 0.500133, acc: 54.69%, op_acc: 50.00%] [G loss: 1.307034]\n",
      "193 [D loss: 0.583294, acc: 50.00%, op_acc: 53.12%] [G loss: 1.170385]\n",
      "194 [D loss: 0.481998, acc: 53.12%, op_acc: 50.00%] [G loss: 1.228980]\n",
      "195 [D loss: 0.541827, acc: 53.12%, op_acc: 51.56%] [G loss: 1.178520]\n",
      "196 [D loss: 0.492337, acc: 51.56%, op_acc: 50.00%] [G loss: 1.058360]\n",
      "197 [D loss: 0.417706, acc: 57.81%, op_acc: 53.12%] [G loss: 1.093566]\n",
      "198 [D loss: 0.502138, acc: 46.88%, op_acc: 51.56%] [G loss: 1.342466]\n",
      "199 [D loss: 0.502105, acc: 50.00%, op_acc: 51.56%] [G loss: 1.249081]\n",
      "200 [D loss: 0.413223, acc: 65.62%, op_acc: 53.12%] [G loss: 1.184917]\n",
      "201 [D loss: 0.525545, acc: 56.25%, op_acc: 48.44%] [G loss: 1.190067]\n",
      "202 [D loss: 0.582677, acc: 46.88%, op_acc: 46.88%] [G loss: 1.304383]\n",
      "203 [D loss: 0.492412, acc: 56.25%, op_acc: 46.88%] [G loss: 1.256067]\n",
      "204 [D loss: 0.508537, acc: 54.69%, op_acc: 50.00%] [G loss: 1.245682]\n",
      "205 [D loss: 0.472145, acc: 51.56%, op_acc: 51.56%] [G loss: 1.043021]\n",
      "206 [D loss: 0.456148, acc: 59.38%, op_acc: 45.31%] [G loss: 1.438342]\n",
      "207 [D loss: 0.491193, acc: 51.56%, op_acc: 46.88%] [G loss: 1.343740]\n",
      "208 [D loss: 0.568290, acc: 54.69%, op_acc: 39.06%] [G loss: 1.201357]\n",
      "209 [D loss: 0.533184, acc: 57.81%, op_acc: 40.62%] [G loss: 1.164807]\n",
      "210 [D loss: 0.405649, acc: 68.75%, op_acc: 48.44%] [G loss: 1.251806]\n",
      "211 [D loss: 0.452659, acc: 62.50%, op_acc: 53.12%] [G loss: 1.438171]\n",
      "212 [D loss: 0.561997, acc: 50.00%, op_acc: 45.31%] [G loss: 1.005631]\n",
      "213 [D loss: 0.500010, acc: 54.69%, op_acc: 50.00%] [G loss: 1.221873]\n",
      "214 [D loss: 0.413661, acc: 62.50%, op_acc: 54.69%] [G loss: 1.061590]\n",
      "215 [D loss: 0.515380, acc: 53.12%, op_acc: 46.88%] [G loss: 1.162788]\n",
      "216 [D loss: 0.538372, acc: 40.62%, op_acc: 53.12%] [G loss: 1.206949]\n",
      "217 [D loss: 0.539861, acc: 46.88%, op_acc: 48.44%] [G loss: 1.270278]\n",
      "218 [D loss: 0.455746, acc: 64.06%, op_acc: 53.12%] [G loss: 1.352189]\n",
      "219 [D loss: 0.450482, acc: 64.06%, op_acc: 54.69%] [G loss: 1.427419]\n",
      "220 [D loss: 0.489229, acc: 48.44%, op_acc: 48.44%] [G loss: 1.223783]\n",
      "221 [D loss: 0.474911, acc: 54.69%, op_acc: 54.69%] [G loss: 1.035541]\n",
      "222 [D loss: 0.420431, acc: 73.44%, op_acc: 50.00%] [G loss: 1.198009]\n",
      "223 [D loss: 0.559444, acc: 43.75%, op_acc: 43.75%] [G loss: 1.335724]\n",
      "224 [D loss: 0.574114, acc: 46.88%, op_acc: 43.75%] [G loss: 1.072147]\n",
      "225 [D loss: 0.433665, acc: 56.25%, op_acc: 56.25%] [G loss: 1.057255]\n",
      "226 [D loss: 0.442560, acc: 54.69%, op_acc: 53.12%] [G loss: 1.480889]\n",
      "227 [D loss: 0.540151, acc: 56.25%, op_acc: 53.12%] [G loss: 1.279321]\n",
      "228 [D loss: 0.447238, acc: 59.38%, op_acc: 54.69%] [G loss: 1.148243]\n",
      "229 [D loss: 0.585362, acc: 51.56%, op_acc: 45.31%] [G loss: 1.256558]\n",
      "230 [D loss: 0.604957, acc: 48.44%, op_acc: 37.50%] [G loss: 0.976023]\n",
      "231 [D loss: 0.536148, acc: 40.62%, op_acc: 48.44%] [G loss: 1.251791]\n",
      "232 [D loss: 0.525348, acc: 53.12%, op_acc: 46.88%] [G loss: 1.312644]\n",
      "233 [D loss: 0.464325, acc: 60.94%, op_acc: 51.56%] [G loss: 1.179456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234 [D loss: 0.479889, acc: 51.56%, op_acc: 53.12%] [G loss: 1.297603]\n",
      "235 [D loss: 0.502652, acc: 50.00%, op_acc: 56.25%] [G loss: 1.247050]\n",
      "236 [D loss: 0.485431, acc: 62.50%, op_acc: 42.19%] [G loss: 1.328890]\n",
      "237 [D loss: 0.514294, acc: 51.56%, op_acc: 53.12%] [G loss: 1.222086]\n",
      "238 [D loss: 0.480768, acc: 64.06%, op_acc: 50.00%] [G loss: 1.514277]\n",
      "239 [D loss: 0.524882, acc: 56.25%, op_acc: 50.00%] [G loss: 1.168857]\n",
      "240 [D loss: 0.529448, acc: 51.56%, op_acc: 37.50%] [G loss: 1.403154]\n",
      "241 [D loss: 0.484373, acc: 65.62%, op_acc: 48.44%] [G loss: 1.409695]\n",
      "242 [D loss: 0.582908, acc: 46.88%, op_acc: 43.75%] [G loss: 1.120604]\n",
      "243 [D loss: 0.488423, acc: 57.81%, op_acc: 50.00%] [G loss: 1.293254]\n",
      "244 [D loss: 0.555186, acc: 46.88%, op_acc: 54.69%] [G loss: 1.124138]\n",
      "245 [D loss: 0.542650, acc: 56.25%, op_acc: 43.75%] [G loss: 1.082058]\n",
      "246 [D loss: 0.533609, acc: 54.69%, op_acc: 48.44%] [G loss: 1.085073]\n",
      "247 [D loss: 0.435694, acc: 70.31%, op_acc: 50.00%] [G loss: 1.251329]\n",
      "248 [D loss: 0.428199, acc: 59.38%, op_acc: 51.56%] [G loss: 1.338516]\n",
      "249 [D loss: 0.433626, acc: 59.38%, op_acc: 59.38%] [G loss: 1.296820]\n",
      "250 [D loss: 0.602203, acc: 39.06%, op_acc: 51.56%] [G loss: 1.120929]\n",
      "251 [D loss: 0.570673, acc: 51.56%, op_acc: 50.00%] [G loss: 1.100101]\n",
      "252 [D loss: 0.469017, acc: 62.50%, op_acc: 51.56%] [G loss: 1.136974]\n",
      "253 [D loss: 0.540585, acc: 54.69%, op_acc: 45.31%] [G loss: 1.032121]\n",
      "254 [D loss: 0.465125, acc: 65.62%, op_acc: 43.75%] [G loss: 1.171875]\n",
      "255 [D loss: 0.498716, acc: 48.44%, op_acc: 53.12%] [G loss: 1.049606]\n",
      "256 [D loss: 0.463187, acc: 59.38%, op_acc: 48.44%] [G loss: 1.114748]\n",
      "257 [D loss: 0.560203, acc: 48.44%, op_acc: 42.19%] [G loss: 1.232724]\n",
      "258 [D loss: 0.601622, acc: 50.00%, op_acc: 42.19%] [G loss: 1.162946]\n",
      "259 [D loss: 0.422008, acc: 65.62%, op_acc: 45.31%] [G loss: 1.242083]\n",
      "260 [D loss: 0.528300, acc: 43.75%, op_acc: 45.31%] [G loss: 1.085226]\n",
      "261 [D loss: 0.487721, acc: 54.69%, op_acc: 50.00%] [G loss: 1.279858]\n",
      "262 [D loss: 0.570864, acc: 50.00%, op_acc: 42.19%] [G loss: 1.163006]\n",
      "263 [D loss: 0.486341, acc: 60.94%, op_acc: 54.69%] [G loss: 1.556377]\n",
      "264 [D loss: 0.537404, acc: 46.88%, op_acc: 48.44%] [G loss: 1.155994]\n",
      "265 [D loss: 0.429673, acc: 64.06%, op_acc: 53.12%] [G loss: 1.125055]\n",
      "266 [D loss: 0.448190, acc: 62.50%, op_acc: 54.69%] [G loss: 1.172286]\n",
      "267 [D loss: 0.474769, acc: 60.94%, op_acc: 48.44%] [G loss: 1.155172]\n",
      "268 [D loss: 0.394661, acc: 67.19%, op_acc: 60.94%] [G loss: 0.960338]\n",
      "269 [D loss: 0.541500, acc: 51.56%, op_acc: 46.88%] [G loss: 1.069924]\n",
      "270 [D loss: 0.461920, acc: 54.69%, op_acc: 51.56%] [G loss: 1.335504]\n",
      "271 [D loss: 0.565383, acc: 50.00%, op_acc: 42.19%] [G loss: 1.375367]\n",
      "272 [D loss: 0.406392, acc: 70.31%, op_acc: 48.44%] [G loss: 1.293613]\n",
      "273 [D loss: 0.428469, acc: 65.62%, op_acc: 53.12%] [G loss: 1.084765]\n",
      "274 [D loss: 0.478048, acc: 53.12%, op_acc: 62.50%] [G loss: 1.286882]\n",
      "275 [D loss: 0.452951, acc: 60.94%, op_acc: 51.56%] [G loss: 1.216541]\n",
      "276 [D loss: 0.475082, acc: 50.00%, op_acc: 50.00%] [G loss: 1.231438]\n",
      "277 [D loss: 0.536702, acc: 57.81%, op_acc: 46.88%] [G loss: 1.226716]\n",
      "278 [D loss: 0.395225, acc: 65.62%, op_acc: 57.81%] [G loss: 1.295363]\n",
      "279 [D loss: 0.442450, acc: 65.62%, op_acc: 43.75%] [G loss: 1.137161]\n",
      "280 [D loss: 0.440330, acc: 71.88%, op_acc: 53.12%] [G loss: 1.319146]\n",
      "281 [D loss: 0.455358, acc: 54.69%, op_acc: 60.94%] [G loss: 1.085728]\n",
      "282 [D loss: 0.496714, acc: 50.00%, op_acc: 46.88%] [G loss: 0.935936]\n",
      "283 [D loss: 0.499846, acc: 50.00%, op_acc: 54.69%] [G loss: 1.011306]\n",
      "284 [D loss: 0.481017, acc: 56.25%, op_acc: 35.94%] [G loss: 1.170771]\n",
      "285 [D loss: 0.480439, acc: 51.56%, op_acc: 56.25%] [G loss: 0.904035]\n",
      "286 [D loss: 0.489558, acc: 51.56%, op_acc: 57.81%] [G loss: 1.187960]\n",
      "287 [D loss: 0.416272, acc: 71.88%, op_acc: 53.12%] [G loss: 1.332594]\n",
      "288 [D loss: 0.448722, acc: 56.25%, op_acc: 51.56%] [G loss: 1.232842]\n",
      "289 [D loss: 0.455109, acc: 54.69%, op_acc: 51.56%] [G loss: 1.225748]\n",
      "290 [D loss: 0.471886, acc: 56.25%, op_acc: 48.44%] [G loss: 1.239463]\n",
      "291 [D loss: 0.488642, acc: 45.31%, op_acc: 46.88%] [G loss: 1.069471]\n",
      "292 [D loss: 0.367003, acc: 67.19%, op_acc: 60.94%] [G loss: 1.268844]\n",
      "293 [D loss: 0.497390, acc: 51.56%, op_acc: 54.69%] [G loss: 1.406991]\n",
      "294 [D loss: 0.464152, acc: 59.38%, op_acc: 56.25%] [G loss: 1.395957]\n",
      "295 [D loss: 0.484594, acc: 53.12%, op_acc: 57.81%] [G loss: 1.164481]\n",
      "296 [D loss: 0.479001, acc: 54.69%, op_acc: 50.00%] [G loss: 1.339721]\n",
      "297 [D loss: 0.451893, acc: 57.81%, op_acc: 42.19%] [G loss: 1.123274]\n",
      "298 [D loss: 0.498588, acc: 50.00%, op_acc: 51.56%] [G loss: 1.215426]\n",
      "299 [D loss: 0.457250, acc: 59.38%, op_acc: 54.69%] [G loss: 1.136049]\n",
      "300 [D loss: 0.494549, acc: 50.00%, op_acc: 50.00%] [G loss: 0.898515]\n",
      "301 [D loss: 0.422039, acc: 73.44%, op_acc: 57.81%] [G loss: 1.012915]\n",
      "302 [D loss: 0.515527, acc: 54.69%, op_acc: 48.44%] [G loss: 1.000668]\n",
      "303 [D loss: 0.363978, acc: 70.31%, op_acc: 54.69%] [G loss: 1.185884]\n",
      "304 [D loss: 0.378213, acc: 70.31%, op_acc: 51.56%] [G loss: 1.227610]\n",
      "305 [D loss: 0.491714, acc: 50.00%, op_acc: 54.69%] [G loss: 1.333889]\n",
      "306 [D loss: 0.521496, acc: 56.25%, op_acc: 51.56%] [G loss: 1.208646]\n",
      "307 [D loss: 0.515468, acc: 48.44%, op_acc: 56.25%] [G loss: 1.132051]\n",
      "308 [D loss: 0.560078, acc: 40.62%, op_acc: 48.44%] [G loss: 1.333647]\n",
      "309 [D loss: 0.475424, acc: 59.38%, op_acc: 56.25%] [G loss: 1.250937]\n",
      "310 [D loss: 0.422716, acc: 59.38%, op_acc: 60.94%] [G loss: 1.310495]\n",
      "311 [D loss: 0.432006, acc: 67.19%, op_acc: 56.25%] [G loss: 1.145626]\n",
      "312 [D loss: 0.433494, acc: 75.00%, op_acc: 51.56%] [G loss: 1.247213]\n",
      "313 [D loss: 0.520646, acc: 48.44%, op_acc: 50.00%] [G loss: 1.192871]\n",
      "314 [D loss: 0.460878, acc: 64.06%, op_acc: 51.56%] [G loss: 1.245909]\n",
      "315 [D loss: 0.379093, acc: 73.44%, op_acc: 57.81%] [G loss: 1.201147]\n",
      "316 [D loss: 0.460986, acc: 57.81%, op_acc: 51.56%] [G loss: 1.148657]\n",
      "317 [D loss: 0.454294, acc: 56.25%, op_acc: 59.38%] [G loss: 0.981974]\n",
      "318 [D loss: 0.472379, acc: 62.50%, op_acc: 54.69%] [G loss: 1.214577]\n",
      "319 [D loss: 0.471395, acc: 56.25%, op_acc: 62.50%] [G loss: 1.160748]\n",
      "320 [D loss: 0.370038, acc: 67.19%, op_acc: 56.25%] [G loss: 1.163573]\n",
      "321 [D loss: 0.451533, acc: 62.50%, op_acc: 59.38%] [G loss: 1.221796]\n",
      "322 [D loss: 0.411408, acc: 68.75%, op_acc: 56.25%] [G loss: 1.373089]\n",
      "323 [D loss: 0.494803, acc: 56.25%, op_acc: 53.12%] [G loss: 1.094245]\n"
     ]
    }
   ],
   "source": [
    "# epochs=20000\n",
    "epochs=2000\n",
    "train(G, D, combined,\n",
    "      num_classes, latent_dim,\n",
    "      epochs=epochs,\n",
    "      batch_size=32, sample_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
