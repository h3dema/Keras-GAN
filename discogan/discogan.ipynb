{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
    "from keras.layers import Input, Dropout, Concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(layer_input, filters, f_size=4, normalize=True):\n",
    "    \"\"\"Layers used during downsampling\"\"\"\n",
    "    d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    if normalize:\n",
    "        d = InstanceNormalization()(d)\n",
    "    return d\n",
    "\n",
    "def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "    \"\"\"Layers used during upsampling\"\"\"\n",
    "    u = UpSampling2D(size=2)(layer_input)\n",
    "    u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "    if dropout_rate:\n",
    "        u = Dropout(dropout_rate)(u)\n",
    "    u = InstanceNormalization()(u)\n",
    "    u = Concatenate()([u, skip_input])\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(img_shape, channels, gf):\n",
    "    \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "    # Image input\n",
    "    d0 = Input(shape=img_shape)\n",
    "\n",
    "    # Downsampling\n",
    "    d1 = conv2d(d0, gf, normalize=False)\n",
    "    d2 = conv2d(d1, gf*2)\n",
    "    d3 = conv2d(d2, gf*4)\n",
    "    d4 = conv2d(d3, gf*8)\n",
    "    d5 = conv2d(d4, gf*8)\n",
    "    d6 = conv2d(d5, gf*8)\n",
    "    d7 = conv2d(d6, gf*8)\n",
    "\n",
    "    # Upsampling\n",
    "    u1 = deconv2d(d7, d6, gf*8)\n",
    "    u2 = deconv2d(u1, d5, gf*8)\n",
    "    u3 = deconv2d(u2, d4, gf*8)\n",
    "    u4 = deconv2d(u3, d3, gf*4)\n",
    "    u5 = deconv2d(u4, d2, gf*2)\n",
    "    u6 = deconv2d(u5, d1, gf)\n",
    "\n",
    "    u7 = UpSampling2D(size=2)(u6)\n",
    "    output_img = Conv2D(channels, kernel_size=4, strides=1,\n",
    "                        padding='same', activation='tanh')(u7)\n",
    "\n",
    "    return Model(d0, output_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_layer(layer_input, filters, f_size=4, normalization=True):\n",
    "    \"\"\"Discriminator layer\"\"\"\n",
    "    d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    if normalization:\n",
    "        d = InstanceNormalization()(d)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(img_shape, df):\n",
    "    img = Input(shape=img_shape)\n",
    "\n",
    "    d1 = d_layer(img, df, normalization=False)\n",
    "    d2 = d_layer(d1, df*2)\n",
    "    d3 = d_layer(d2, df*4)\n",
    "    d4 = d_layer(d3, df*8)\n",
    "\n",
    "    validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "    return Model(img, validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(data_loader, dataset_name, g_AB, g_BA, epoch, batch_i):\n",
    "    os.makedirs('images/%s' % dataset_name, exist_ok=True)\n",
    "    r, c = 2, 3\n",
    "\n",
    "    imgs_A, imgs_B = data_loader.load_data(batch_size=1, is_testing=True)\n",
    "\n",
    "    # Translate images to the other domain\n",
    "    fake_B = g_AB.predict(imgs_A)\n",
    "    fake_A = g_BA.predict(imgs_B)\n",
    "    # Translate back to original domain\n",
    "    reconstr_A = g_BA.predict(fake_B)\n",
    "    reconstr_B = g_AB.predict(fake_A)\n",
    "\n",
    "    gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B])\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    titles = ['Original', 'Translated', 'Reconstructed']\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt])\n",
    "            axs[i, j].set_title(titles[j])\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/%s/%d_%d.png\" % (dataset_name, epoch, batch_i))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, dataset_name, \n",
    "          g_AB, g_BA, d_A, d_B, combined, \n",
    "          disc_patch,\n",
    "          epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # Adversarial loss ground truths\n",
    "    valid = np.ones((batch_size,) + disc_patch)\n",
    "    fake = np.zeros((batch_size,) + disc_patch)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch_i, (imgs_A, imgs_B) in enumerate(data_loader.load_batch(batch_size)):\n",
    "            print(\"epoch\", epoch, \"- batch #\", batch_i)\n",
    "\n",
    "            # ----------------------\n",
    "            #  Train Discriminators\n",
    "            # ----------------------\n",
    "\n",
    "            # Translate images to opposite domain\n",
    "            fake_B = g_AB.predict(imgs_A)\n",
    "            fake_A = g_BA.predict(imgs_B)\n",
    "\n",
    "            # Train the discriminators (original images = real / translated = Fake)\n",
    "            dA_loss_real = d_A.train_on_batch(imgs_A, valid)\n",
    "            dA_loss_fake = d_A.train_on_batch(fake_A, fake)\n",
    "            dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
    "\n",
    "            dB_loss_real = d_B.train_on_batch(imgs_B, valid)\n",
    "            dB_loss_fake = d_B.train_on_batch(fake_B, fake)\n",
    "            dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
    "\n",
    "            # Total disciminator loss\n",
    "            d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
    "\n",
    "            # ------------------\n",
    "            #  Train Generators\n",
    "            # ------------------\n",
    "\n",
    "            # Train the generators\n",
    "            g_loss = combined.train_on_batch([imgs_A, imgs_B], [valid, valid, \\\n",
    "                                                                     imgs_B, imgs_A, \\\n",
    "                                                                     imgs_A, imgs_B])\n",
    "\n",
    "            elapsed_time = datetime.datetime.now() - start_time\n",
    "            # Plot the progress\n",
    "            print (\"[%d] [%d/%d] time: %s, [d_loss: %f, g_loss: %f]\" % (epoch, batch_i,\n",
    "                                                                        data_loader.n_batches,\n",
    "                                                                        elapsed_time,\n",
    "                                                                        d_loss[0], g_loss[0]))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if batch_i % sample_interval == 0:\n",
    "                sample_images(data_loader, dataset_name, g_AB, g_BA, epoch, batch_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape\n",
    "img_rows = 128\n",
    "img_cols = 128\n",
    "channels = 3\n",
    "img_shape = (img_rows, img_cols, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure data loader\n",
    "dataset_name = 'edges2shoes'\n",
    "data_loader = DataLoader(dataset_name=dataset_name,\n",
    "                         img_res=(img_rows, img_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate output shape of D (PatchGAN)\n",
    "patch = int(img_rows / 2**4)\n",
    "disc_patch = (patch, patch, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of filters in the first layer of G and D\n",
    "gf = 64\n",
    "df = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# create optimizer\n",
    "optimizer = Adam(0.0002, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the discriminators\n",
    "d_A = build_discriminator(img_shape, df)\n",
    "d_B = build_discriminator(img_shape, df)\n",
    "d_A.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "d_B.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "# Construct Computational\n",
    "#   Graph of Generators\n",
    "#-------------------------\n",
    "\n",
    "# Build the generators\n",
    "g_AB = build_generator(img_shape, channels, gf)\n",
    "g_BA = build_generator(img_shape, channels, gf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input images from both domains\n",
    "img_A = Input(shape=img_shape)\n",
    "img_B = Input(shape=img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate images to the other domain\n",
    "fake_B = g_AB(img_A)\n",
    "fake_A = g_BA(img_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate images back to original domain\n",
    "reconstr_A = g_BA(fake_B)\n",
    "reconstr_B = g_AB(fake_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the combined model we will only train the generators\n",
    "d_A.trainable = False\n",
    "d_B.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminators determines validity of translated images\n",
    "valid_A = d_A(fake_A)\n",
    "valid_B = d_B(fake_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objectives\n",
    "# + Adversarial: Fool domain discriminators\n",
    "# + Translation: Minimize MAE between e.g. fake B and true B\n",
    "# + Cycle-consistency: Minimize MAE between reconstructed images and original\n",
    "combined = Model(inputs=[img_A, img_B],\n",
    "                      outputs=[ valid_A, valid_B,\n",
    "                                fake_B, fake_A,\n",
    "                                reconstr_A, reconstr_B ])\n",
    "combined.compile(loss=['mse', 'mse',\n",
    "                       'mae', 'mae',\n",
    "                       'mae', 'mae'],\n",
    "                 optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] [0/49825] time: 0:00:02.753157, [d_loss: 0.325678, g_loss: 3.150854]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 # 1\n",
      "[0] [1/49825] time: 0:00:06.512421, [d_loss: 0.272776, g_loss: 2.676764]\n",
      "epoch 0 # 2\n",
      "[0] [2/49825] time: 0:00:09.725880, [d_loss: 0.245721, g_loss: 2.466418]\n",
      "epoch 0 # 3\n",
      "[0] [3/49825] time: 0:00:12.909697, [d_loss: 0.258993, g_loss: 1.671192]\n",
      "epoch 0 # 4\n",
      "[0] [4/49825] time: 0:00:15.965880, [d_loss: 0.188796, g_loss: 2.790794]\n",
      "epoch 0 # 5\n",
      "[0] [5/49825] time: 0:00:19.117180, [d_loss: 0.191661, g_loss: 1.591216]\n",
      "epoch 0 # 6\n",
      "[0] [6/49825] time: 0:00:22.311919, [d_loss: 0.196063, g_loss: 2.830503]\n",
      "epoch 0 # 7\n",
      "[0] [7/49825] time: 0:00:25.948970, [d_loss: 0.163234, g_loss: 2.287107]\n",
      "epoch 0 # 8\n",
      "[0] [8/49825] time: 0:00:29.338723, [d_loss: 0.349116, g_loss: 1.565508]\n",
      "epoch 0 # 9\n",
      "[0] [9/49825] time: 0:00:32.507564, [d_loss: 0.321941, g_loss: 1.596231]\n",
      "epoch 0 # 10\n",
      "[0] [10/49825] time: 0:00:35.999136, [d_loss: 0.193477, g_loss: 2.406057]\n",
      "epoch 0 # 11\n",
      "[0] [11/49825] time: 0:00:39.392599, [d_loss: 0.350549, g_loss: 1.200775]\n",
      "epoch 0 # 12\n",
      "[0] [12/49825] time: 0:00:42.521781, [d_loss: 0.284954, g_loss: 1.206507]\n",
      "epoch 0 # 13\n",
      "[0] [13/49825] time: 0:00:45.621128, [d_loss: 0.214391, g_loss: 2.245557]\n",
      "epoch 0 # 14\n",
      "[0] [14/49825] time: 0:00:48.806491, [d_loss: 0.189096, g_loss: 2.367753]\n",
      "epoch 0 # 15\n",
      "[0] [15/49825] time: 0:00:52.332339, [d_loss: 0.304912, g_loss: 1.836153]\n",
      "epoch 0 # 16\n",
      "[0] [16/49825] time: 0:00:55.440027, [d_loss: 0.172708, g_loss: 2.119673]\n",
      "epoch 0 # 17\n",
      "[0] [17/49825] time: 0:00:58.567471, [d_loss: 0.200991, g_loss: 3.077208]\n",
      "epoch 0 # 18\n",
      "[0] [18/49825] time: 0:01:01.736689, [d_loss: 0.209930, g_loss: 2.345078]\n",
      "epoch 0 # 19\n",
      "[0] [19/49825] time: 0:01:04.944784, [d_loss: 0.171238, g_loss: 2.806678]\n",
      "epoch 0 # 20\n",
      "[0] [20/49825] time: 0:01:07.902412, [d_loss: 0.314020, g_loss: 1.341711]\n",
      "epoch 0 # 21\n",
      "[0] [21/49825] time: 0:01:11.173101, [d_loss: 0.203668, g_loss: 1.549214]\n",
      "epoch 0 # 22\n",
      "[0] [22/49825] time: 0:01:14.462557, [d_loss: 0.165117, g_loss: 2.837741]\n",
      "epoch 0 # 23\n",
      "[0] [23/49825] time: 0:01:17.561494, [d_loss: 0.160176, g_loss: 2.994878]\n",
      "epoch 0 # 24\n",
      "[0] [24/49825] time: 0:01:21.335058, [d_loss: 0.163531, g_loss: 2.894049]\n",
      "epoch 0 # 25\n",
      "[0] [25/49825] time: 0:01:25.162753, [d_loss: 0.165561, g_loss: 2.896128]\n",
      "epoch 0 # 26\n",
      "[0] [26/49825] time: 0:01:28.364621, [d_loss: 0.159412, g_loss: 2.777668]\n",
      "epoch 0 # 27\n",
      "[0] [27/49825] time: 0:01:31.417574, [d_loss: 0.158574, g_loss: 2.920268]\n",
      "epoch 0 # 28\n",
      "[0] [28/49825] time: 0:01:34.690043, [d_loss: 0.156414, g_loss: 3.037772]\n",
      "epoch 0 # 29\n",
      "[0] [29/49825] time: 0:01:38.307344, [d_loss: 0.177022, g_loss: 2.457164]\n",
      "epoch 0 # 30\n",
      "[0] [30/49825] time: 0:01:41.966839, [d_loss: 0.292452, g_loss: 1.397886]\n",
      "epoch 0 # 31\n",
      "[0] [31/49825] time: 0:01:45.443384, [d_loss: 0.212737, g_loss: 2.760171]\n",
      "epoch 0 # 32\n",
      "[0] [32/49825] time: 0:01:48.823596, [d_loss: 0.253270, g_loss: 1.598723]\n",
      "epoch 0 # 33\n",
      "[0] [33/49825] time: 0:01:52.119998, [d_loss: 0.177052, g_loss: 2.557954]\n",
      "epoch 0 # 34\n",
      "[0] [34/49825] time: 0:01:55.304529, [d_loss: 0.164892, g_loss: 2.786891]\n",
      "epoch 0 # 35\n",
      "[0] [35/49825] time: 0:01:58.471745, [d_loss: 0.343468, g_loss: 1.393398]\n",
      "epoch 0 # 36\n",
      "[0] [36/49825] time: 0:02:01.689748, [d_loss: 0.176839, g_loss: 2.596984]\n",
      "epoch 0 # 37\n",
      "[0] [37/49825] time: 0:02:04.993511, [d_loss: 0.173810, g_loss: 3.221537]\n",
      "epoch 0 # 38\n",
      "[0] [38/49825] time: 0:02:08.057608, [d_loss: 0.343395, g_loss: 1.723350]\n",
      "epoch 0 # 39\n",
      "[0] [39/49825] time: 0:02:11.164524, [d_loss: 0.207579, g_loss: 2.226166]\n",
      "epoch 0 # 40\n",
      "[0] [40/49825] time: 0:02:14.623725, [d_loss: 0.178454, g_loss: 2.402428]\n",
      "epoch 0 # 41\n",
      "[0] [41/49825] time: 0:02:17.777294, [d_loss: 0.228814, g_loss: 2.309248]\n",
      "epoch 0 # 42\n",
      "[0] [42/49825] time: 0:02:20.909395, [d_loss: 0.232036, g_loss: 2.265249]\n",
      "epoch 0 # 43\n",
      "[0] [43/49825] time: 0:02:24.384232, [d_loss: 0.377421, g_loss: 1.952771]\n",
      "epoch 0 # 44\n",
      "[0] [44/49825] time: 0:02:27.540387, [d_loss: 0.207458, g_loss: 2.136769]\n",
      "epoch 0 # 45\n",
      "[0] [45/49825] time: 0:02:31.427845, [d_loss: 0.237862, g_loss: 1.995426]\n",
      "epoch 0 # 46\n",
      "[0] [46/49825] time: 0:02:34.741340, [d_loss: 0.179025, g_loss: 2.715873]\n",
      "epoch 0 # 47\n",
      "[0] [47/49825] time: 0:02:37.908714, [d_loss: 0.166136, g_loss: 2.407757]\n",
      "epoch 0 # 48\n",
      "[0] [48/49825] time: 0:02:41.234457, [d_loss: 0.201834, g_loss: 2.263564]\n",
      "epoch 0 # 49\n",
      "[0] [49/49825] time: 0:02:44.324906, [d_loss: 0.261291, g_loss: 2.161219]\n",
      "epoch 0 # 50\n",
      "[0] [50/49825] time: 0:02:47.736221, [d_loss: 0.296447, g_loss: 2.301070]\n",
      "epoch 0 # 51\n",
      "[0] [51/49825] time: 0:02:50.974294, [d_loss: 0.627033, g_loss: 1.717102]\n",
      "epoch 0 # 52\n",
      "[0] [52/49825] time: 0:02:54.948016, [d_loss: 0.242065, g_loss: 1.863341]\n",
      "epoch 0 # 53\n",
      "[0] [53/49825] time: 0:02:58.660689, [d_loss: 0.308628, g_loss: 2.213408]\n",
      "epoch 0 # 54\n",
      "[0] [54/49825] time: 0:03:02.396890, [d_loss: 0.298777, g_loss: 2.192206]\n",
      "epoch 0 # 55\n"
     ]
    }
   ],
   "source": [
    "# epochs=20\n",
    "epochs=5\n",
    "train(data_loader, dataset_name, \n",
    "      g_AB, g_BA, d_A, d_B, combined,\n",
    "      disc_patch,\n",
    "      epochs=epochs, batch_size=1, sample_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
