{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Autoencoder\n",
    "\n",
    "ref. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "# from keras.layers import merge\n",
    "from keras.layers import Lambda, Add\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(img_shape, latent_dim):\n",
    "    # Encoder\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "\n",
    "    h = Flatten()(img)\n",
    "    h = Dense(512)(h)\n",
    "    h = LeakyReLU(alpha=0.2)(h)\n",
    "    h = Dense(512)(h)\n",
    "    h = LeakyReLU(alpha=0.2)(h)\n",
    "    mu = Dense(latent_dim)(h)\n",
    "    log_var = Dense(latent_dim)(h)\n",
    "    \n",
    "    # NOTE: original code uses keras.layer.merge() function which is deprecated\n",
    "    #     latent_repr = merge([mu, log_var],\n",
    "    #                          mode=lambda p: p[0] + K.random_normal(K.shape(p[0])) * K.exp(p[1] / 2),\n",
    "    #                          output_shape=lambda p: p[0])\n",
    "    \n",
    "    # NOW: we use Lambda layer to do the same thing\n",
    "    latent_repr = Lambda(lambda p: p[0] + K.random_normal(K.shape(p[0])) * K.exp(p[1] / 2), \n",
    "                         output_shape=lambda p: p[0])([mu, log_var])   \n",
    "    \n",
    "    return Model(img, latent_repr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder(img_shape, latent_dim):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(512, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
    "    model.add(Reshape(img_shape))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    z = Input(shape=(latent_dim,))\n",
    "    img = model(z)\n",
    "\n",
    "    return Model(z, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(latent_dim):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(512, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.summary()\n",
    "\n",
    "    encoded_repr = Input(shape=(latent_dim, ))\n",
    "    validity = model(encoded_repr)\n",
    "\n",
    "    return Model(encoded_repr, validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(epoch, decoder, latent_dim):\n",
    "    r, c = 5, 5\n",
    "\n",
    "    z = np.random.normal(size=(r*c, latent_dim))\n",
    "    gen_imgs = decoder.predict(z)\n",
    "\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, model_name):\n",
    "    model_path = \"saved_model/%s.json\" % model_name\n",
    "    weights_path = \"saved_model/%s_weights.hdf5\" % model_name\n",
    "    options = {\"file_arch\": model_path,\n",
    "                \"file_weight\": weights_path}\n",
    "    json_string = model.to_json()\n",
    "    open(options['file_arch'], 'w').write(json_string)\n",
    "    model.save_weights(options['file_weight'])\n",
    "\n",
    "def save_model(G, D):\n",
    "    save(G, \"aae_generator\")\n",
    "    save(D, \"aae_discriminator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, adversarial_autoencoder, D, latent_dim, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "    # Load the dataset\n",
    "    (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "    # Rescale -1 to 1\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "    # Adversarial ground truths\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        # Select a random batch of images\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        imgs = X_train[idx]\n",
    "\n",
    "        latent_fake = encoder.predict(imgs)\n",
    "        latent_real = np.random.normal(size=(batch_size, latent_dim))\n",
    "\n",
    "        # Train the discriminator\n",
    "        d_loss_real = D.train_on_batch(latent_real, valid)\n",
    "        d_loss_fake = D.train_on_batch(latent_fake, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Adversarial autoencoder\n",
    "        # ---------------------\n",
    "\n",
    "        # Train the combined stack\n",
    "        g_loss = adversarial_autoencoder.train_on_batch(imgs, [imgs, valid])\n",
    "\n",
    "        # Plot the progress\n",
    "        print (\"%d [D loss: %f, acc: %.2f%%] [G loss: %f, mse: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0], g_loss[1]))\n",
    "\n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % sample_interval == 0:\n",
    "            sample_images(epoch, decoder, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "latent_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(0.0002, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               5632      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 137,217\n",
      "Trainable params: 137,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Build and compile the discriminator\n",
    "D = build_discriminator(latent_dim)\n",
    "D.compile(loss='binary_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 512)               5632      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 784)               402192    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 670,480\n",
      "Trainable params: 670,480\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the encoder / decoder\n",
    "encoder = build_encoder(img_shape, latent_dim)\n",
    "decoder = build_decoder(img_shape, latent_dim)\n",
    "\n",
    "img = Input(shape=img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generator takes the image, encodes it and reconstructs it\n",
    "# from the encoding\n",
    "encoded_repr = encoder(img)\n",
    "reconstructed_img = decoder(encoded_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the adversarial_autoencoder model we will only train the generator\n",
    "D.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The discriminator determines validity of the encoding\n",
    "validity = D(encoded_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The adversarial_autoencoder model  (stacked generator and discriminator)\n",
    "adversarial_autoencoder = Model(img, [reconstructed_img, validity])\n",
    "adversarial_autoencoder.compile(loss=['mse', 'binary_crossentropy'],\n",
    "    loss_weights=[0.999, 0.001],\n",
    "    optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.351715, acc: 81.25%] [G loss: 0.176907, mse: 0.171583]\n",
      "1 [D loss: 0.263372, acc: 87.50%] [G loss: 0.166061, mse: 0.154519]\n",
      "2 [D loss: 0.272404, acc: 89.06%] [G loss: 0.157300, mse: 0.151216]\n",
      "3 [D loss: 0.435980, acc: 76.56%] [G loss: 0.166682, mse: 0.160853]\n",
      "4 [D loss: 0.322865, acc: 82.81%] [G loss: 0.176703, mse: 0.169069]\n",
      "5 [D loss: 0.430299, acc: 79.69%] [G loss: 0.170225, mse: 0.165230]\n",
      "6 [D loss: 0.192809, acc: 89.06%] [G loss: 0.178563, mse: 0.157949]\n",
      "7 [D loss: 0.246403, acc: 89.06%] [G loss: 0.191749, mse: 0.176326]\n",
      "8 [D loss: 0.316771, acc: 82.81%] [G loss: 0.156234, mse: 0.148636]\n",
      "9 [D loss: 0.504943, acc: 78.12%] [G loss: 0.175928, mse: 0.171551]\n",
      "10 [D loss: 0.270260, acc: 87.50%] [G loss: 0.157584, mse: 0.146232]\n",
      "11 [D loss: 0.338696, acc: 76.56%] [G loss: 0.164910, mse: 0.155430]\n",
      "12 [D loss: 0.149349, acc: 95.31%] [G loss: 0.161164, mse: 0.155375]\n",
      "13 [D loss: 0.247623, acc: 87.50%] [G loss: 0.177295, mse: 0.173077]\n",
      "14 [D loss: 0.294870, acc: 82.81%] [G loss: 0.149279, mse: 0.143195]\n",
      "15 [D loss: 0.258200, acc: 82.81%] [G loss: 0.146710, mse: 0.140104]\n",
      "16 [D loss: 0.262412, acc: 87.50%] [G loss: 0.159747, mse: 0.154697]\n",
      "17 [D loss: 0.363604, acc: 84.38%] [G loss: 0.150796, mse: 0.145847]\n",
      "18 [D loss: 0.190517, acc: 90.62%] [G loss: 0.163052, mse: 0.156689]\n",
      "19 [D loss: 0.320852, acc: 85.94%] [G loss: 0.138840, mse: 0.132414]\n",
      "20 [D loss: 0.348773, acc: 79.69%] [G loss: 0.126615, mse: 0.121581]\n",
      "21 [D loss: 0.171090, acc: 93.75%] [G loss: 0.143460, mse: 0.138155]\n",
      "22 [D loss: 0.217062, acc: 87.50%] [G loss: 0.160353, mse: 0.154210]\n",
      "23 [D loss: 0.320227, acc: 81.25%] [G loss: 0.125660, mse: 0.119943]\n",
      "24 [D loss: 0.356809, acc: 82.81%] [G loss: 0.145824, mse: 0.141543]\n",
      "25 [D loss: 0.326708, acc: 79.69%] [G loss: 0.152404, mse: 0.147179]\n",
      "26 [D loss: 0.340604, acc: 84.38%] [G loss: 0.141164, mse: 0.134802]\n",
      "27 [D loss: 0.323532, acc: 81.25%] [G loss: 0.141171, mse: 0.136099]\n",
      "28 [D loss: 0.313325, acc: 87.50%] [G loss: 0.141745, mse: 0.136693]\n",
      "29 [D loss: 0.438010, acc: 79.69%] [G loss: 0.162706, mse: 0.157796]\n",
      "30 [D loss: 0.244532, acc: 82.81%] [G loss: 0.136838, mse: 0.129812]\n",
      "31 [D loss: 0.319115, acc: 84.38%] [G loss: 0.135935, mse: 0.130457]\n",
      "32 [D loss: 0.208667, acc: 92.19%] [G loss: 0.122081, mse: 0.116547]\n",
      "33 [D loss: 0.306014, acc: 85.94%] [G loss: 0.170771, mse: 0.165528]\n",
      "34 [D loss: 0.282870, acc: 89.06%] [G loss: 0.135458, mse: 0.130000]\n",
      "35 [D loss: 0.311612, acc: 87.50%] [G loss: 0.131582, mse: 0.125821]\n",
      "36 [D loss: 0.292809, acc: 82.81%] [G loss: 0.134856, mse: 0.129605]\n",
      "37 [D loss: 0.215930, acc: 89.06%] [G loss: 0.148052, mse: 0.142507]\n",
      "38 [D loss: 0.235787, acc: 90.62%] [G loss: 0.154907, mse: 0.150497]\n",
      "39 [D loss: 0.311036, acc: 85.94%] [G loss: 0.137777, mse: 0.132498]\n",
      "40 [D loss: 0.232810, acc: 89.06%] [G loss: 0.126241, mse: 0.119576]\n",
      "41 [D loss: 0.268515, acc: 85.94%] [G loss: 0.144071, mse: 0.139520]\n",
      "42 [D loss: 0.212115, acc: 93.75%] [G loss: 0.140961, mse: 0.136087]\n",
      "43 [D loss: 0.236556, acc: 92.19%] [G loss: 0.128389, mse: 0.123772]\n",
      "44 [D loss: 0.273749, acc: 89.06%] [G loss: 0.145466, mse: 0.139610]\n",
      "45 [D loss: 0.181966, acc: 95.31%] [G loss: 0.147691, mse: 0.142755]\n",
      "46 [D loss: 0.283569, acc: 92.19%] [G loss: 0.143375, mse: 0.139286]\n",
      "47 [D loss: 0.207079, acc: 95.31%] [G loss: 0.142732, mse: 0.136382]\n",
      "48 [D loss: 0.214229, acc: 92.19%] [G loss: 0.154369, mse: 0.149577]\n",
      "49 [D loss: 0.284292, acc: 89.06%] [G loss: 0.148227, mse: 0.143874]\n",
      "50 [D loss: 0.252548, acc: 92.19%] [G loss: 0.137340, mse: 0.131018]\n",
      "51 [D loss: 0.219693, acc: 92.19%] [G loss: 0.155336, mse: 0.150326]\n",
      "52 [D loss: 0.239048, acc: 93.75%] [G loss: 0.145085, mse: 0.140173]\n",
      "53 [D loss: 0.223517, acc: 87.50%] [G loss: 0.144285, mse: 0.138554]\n",
      "54 [D loss: 0.213992, acc: 93.75%] [G loss: 0.123238, mse: 0.117796]\n",
      "55 [D loss: 0.204661, acc: 93.75%] [G loss: 0.132545, mse: 0.127868]\n",
      "56 [D loss: 0.258450, acc: 90.62%] [G loss: 0.135068, mse: 0.130262]\n",
      "57 [D loss: 0.188626, acc: 93.75%] [G loss: 0.136549, mse: 0.130901]\n",
      "58 [D loss: 0.198908, acc: 98.44%] [G loss: 0.140374, mse: 0.135450]\n",
      "59 [D loss: 0.181936, acc: 98.44%] [G loss: 0.129425, mse: 0.124872]\n",
      "60 [D loss: 0.205520, acc: 90.62%] [G loss: 0.122563, mse: 0.117261]\n",
      "61 [D loss: 0.183723, acc: 93.75%] [G loss: 0.139927, mse: 0.135271]\n",
      "62 [D loss: 0.215510, acc: 92.19%] [G loss: 0.142685, mse: 0.137945]\n",
      "63 [D loss: 0.147943, acc: 96.88%] [G loss: 0.137813, mse: 0.133252]\n",
      "64 [D loss: 0.191909, acc: 96.88%] [G loss: 0.134105, mse: 0.129429]\n",
      "65 [D loss: 0.195356, acc: 96.88%] [G loss: 0.109667, mse: 0.104618]\n",
      "66 [D loss: 0.183205, acc: 96.88%] [G loss: 0.130351, mse: 0.125342]\n",
      "67 [D loss: 0.195669, acc: 92.19%] [G loss: 0.133704, mse: 0.129294]\n",
      "68 [D loss: 0.193685, acc: 96.88%] [G loss: 0.137206, mse: 0.132514]\n",
      "69 [D loss: 0.246894, acc: 96.88%] [G loss: 0.128443, mse: 0.123837]\n",
      "70 [D loss: 0.172790, acc: 95.31%] [G loss: 0.136959, mse: 0.130639]\n",
      "71 [D loss: 0.128176, acc: 100.00%] [G loss: 0.129872, mse: 0.123808]\n",
      "72 [D loss: 0.219127, acc: 96.88%] [G loss: 0.140826, mse: 0.136320]\n",
      "73 [D loss: 0.176951, acc: 95.31%] [G loss: 0.154288, mse: 0.148170]\n",
      "74 [D loss: 0.156699, acc: 98.44%] [G loss: 0.143572, mse: 0.137876]\n",
      "75 [D loss: 0.167570, acc: 96.88%] [G loss: 0.133742, mse: 0.129051]\n",
      "76 [D loss: 0.173534, acc: 96.88%] [G loss: 0.125305, mse: 0.120002]\n",
      "77 [D loss: 0.173888, acc: 96.88%] [G loss: 0.135108, mse: 0.130048]\n",
      "78 [D loss: 0.177649, acc: 100.00%] [G loss: 0.139909, mse: 0.134993]\n",
      "79 [D loss: 0.178853, acc: 98.44%] [G loss: 0.130367, mse: 0.124719]\n",
      "80 [D loss: 0.201160, acc: 96.88%] [G loss: 0.135490, mse: 0.130361]\n",
      "81 [D loss: 0.131722, acc: 100.00%] [G loss: 0.125744, mse: 0.120022]\n",
      "82 [D loss: 0.121060, acc: 98.44%] [G loss: 0.141199, mse: 0.136295]\n",
      "83 [D loss: 0.147838, acc: 100.00%] [G loss: 0.134811, mse: 0.130368]\n",
      "84 [D loss: 0.136950, acc: 100.00%] [G loss: 0.140138, mse: 0.135758]\n",
      "85 [D loss: 0.156124, acc: 98.44%] [G loss: 0.131038, mse: 0.125776]\n",
      "86 [D loss: 0.138563, acc: 100.00%] [G loss: 0.143925, mse: 0.139213]\n",
      "87 [D loss: 0.141983, acc: 98.44%] [G loss: 0.121195, mse: 0.116459]\n",
      "88 [D loss: 0.117161, acc: 98.44%] [G loss: 0.137554, mse: 0.133206]\n",
      "89 [D loss: 0.151870, acc: 96.88%] [G loss: 0.118969, mse: 0.114089]\n",
      "90 [D loss: 0.151731, acc: 96.88%] [G loss: 0.134452, mse: 0.129971]\n",
      "91 [D loss: 0.129743, acc: 100.00%] [G loss: 0.142831, mse: 0.137353]\n",
      "92 [D loss: 0.137587, acc: 98.44%] [G loss: 0.140048, mse: 0.135084]\n",
      "93 [D loss: 0.126857, acc: 96.88%] [G loss: 0.125323, mse: 0.120065]\n",
      "94 [D loss: 0.127890, acc: 100.00%] [G loss: 0.129135, mse: 0.123942]\n",
      "95 [D loss: 0.127815, acc: 100.00%] [G loss: 0.127488, mse: 0.122709]\n",
      "96 [D loss: 0.105925, acc: 100.00%] [G loss: 0.123767, mse: 0.118961]\n",
      "97 [D loss: 0.124632, acc: 96.88%] [G loss: 0.126891, mse: 0.121553]\n",
      "98 [D loss: 0.137528, acc: 98.44%] [G loss: 0.122316, mse: 0.117417]\n",
      "99 [D loss: 0.153112, acc: 98.44%] [G loss: 0.138938, mse: 0.133884]\n",
      "100 [D loss: 0.135834, acc: 100.00%] [G loss: 0.129047, mse: 0.123846]\n",
      "101 [D loss: 0.126384, acc: 100.00%] [G loss: 0.128745, mse: 0.123758]\n",
      "102 [D loss: 0.151069, acc: 98.44%] [G loss: 0.136681, mse: 0.132388]\n",
      "103 [D loss: 0.108871, acc: 100.00%] [G loss: 0.128625, mse: 0.123958]\n",
      "104 [D loss: 0.103381, acc: 100.00%] [G loss: 0.132574, mse: 0.127602]\n",
      "105 [D loss: 0.107308, acc: 100.00%] [G loss: 0.132526, mse: 0.127574]\n",
      "106 [D loss: 0.107016, acc: 100.00%] [G loss: 0.128315, mse: 0.123525]\n",
      "107 [D loss: 0.109129, acc: 100.00%] [G loss: 0.132479, mse: 0.126740]\n",
      "108 [D loss: 0.092253, acc: 100.00%] [G loss: 0.117890, mse: 0.112504]\n",
      "109 [D loss: 0.084707, acc: 100.00%] [G loss: 0.113315, mse: 0.108406]\n",
      "110 [D loss: 0.127893, acc: 98.44%] [G loss: 0.131236, mse: 0.126058]\n",
      "111 [D loss: 0.091316, acc: 100.00%] [G loss: 0.149057, mse: 0.144548]\n",
      "112 [D loss: 0.097651, acc: 100.00%] [G loss: 0.114645, mse: 0.109522]\n",
      "113 [D loss: 0.081524, acc: 100.00%] [G loss: 0.123165, mse: 0.117832]\n",
      "114 [D loss: 0.102232, acc: 98.44%] [G loss: 0.135810, mse: 0.130437]\n",
      "115 [D loss: 0.084403, acc: 100.00%] [G loss: 0.125737, mse: 0.120565]\n",
      "116 [D loss: 0.093332, acc: 100.00%] [G loss: 0.118323, mse: 0.113458]\n",
      "117 [D loss: 0.084393, acc: 100.00%] [G loss: 0.130364, mse: 0.124768]\n",
      "118 [D loss: 0.089524, acc: 98.44%] [G loss: 0.131829, mse: 0.126620]\n",
      "119 [D loss: 0.115454, acc: 98.44%] [G loss: 0.140640, mse: 0.135949]\n",
      "120 [D loss: 0.070131, acc: 100.00%] [G loss: 0.102535, mse: 0.096694]\n",
      "121 [D loss: 0.089479, acc: 100.00%] [G loss: 0.134271, mse: 0.129260]\n",
      "122 [D loss: 0.076363, acc: 100.00%] [G loss: 0.128773, mse: 0.123700]\n",
      "123 [D loss: 0.084123, acc: 100.00%] [G loss: 0.137947, mse: 0.133036]\n",
      "124 [D loss: 0.062890, acc: 100.00%] [G loss: 0.122731, mse: 0.117675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125 [D loss: 0.078893, acc: 100.00%] [G loss: 0.124468, mse: 0.119391]\n",
      "126 [D loss: 0.067672, acc: 100.00%] [G loss: 0.113078, mse: 0.107785]\n",
      "127 [D loss: 0.090189, acc: 98.44%] [G loss: 0.127000, mse: 0.122012]\n",
      "128 [D loss: 0.086529, acc: 100.00%] [G loss: 0.146677, mse: 0.142333]\n",
      "129 [D loss: 0.064016, acc: 100.00%] [G loss: 0.120246, mse: 0.114912]\n",
      "130 [D loss: 0.074298, acc: 100.00%] [G loss: 0.130564, mse: 0.125363]\n",
      "131 [D loss: 0.080659, acc: 100.00%] [G loss: 0.126577, mse: 0.121532]\n",
      "132 [D loss: 0.079828, acc: 100.00%] [G loss: 0.126292, mse: 0.121870]\n",
      "133 [D loss: 0.079487, acc: 100.00%] [G loss: 0.123485, mse: 0.118601]\n",
      "134 [D loss: 0.070987, acc: 100.00%] [G loss: 0.123697, mse: 0.118457]\n",
      "135 [D loss: 0.075759, acc: 100.00%] [G loss: 0.129174, mse: 0.124573]\n",
      "136 [D loss: 0.061345, acc: 100.00%] [G loss: 0.120010, mse: 0.114424]\n",
      "137 [D loss: 0.087516, acc: 98.44%] [G loss: 0.119403, mse: 0.114338]\n",
      "138 [D loss: 0.081143, acc: 100.00%] [G loss: 0.125573, mse: 0.120844]\n",
      "139 [D loss: 0.051918, acc: 100.00%] [G loss: 0.114156, mse: 0.109422]\n",
      "140 [D loss: 0.072423, acc: 100.00%] [G loss: 0.135299, mse: 0.130223]\n",
      "141 [D loss: 0.086309, acc: 98.44%] [G loss: 0.134622, mse: 0.129809]\n",
      "142 [D loss: 0.059129, acc: 100.00%] [G loss: 0.117531, mse: 0.112199]\n",
      "143 [D loss: 0.080436, acc: 100.00%] [G loss: 0.116238, mse: 0.110791]\n",
      "144 [D loss: 0.066438, acc: 100.00%] [G loss: 0.120836, mse: 0.115326]\n",
      "145 [D loss: 0.053594, acc: 100.00%] [G loss: 0.116509, mse: 0.110898]\n",
      "146 [D loss: 0.066866, acc: 100.00%] [G loss: 0.124961, mse: 0.119262]\n",
      "147 [D loss: 0.071743, acc: 100.00%] [G loss: 0.126332, mse: 0.121104]\n",
      "148 [D loss: 0.049676, acc: 100.00%] [G loss: 0.107200, mse: 0.101910]\n",
      "149 [D loss: 0.050825, acc: 100.00%] [G loss: 0.115775, mse: 0.110292]\n",
      "150 [D loss: 0.062782, acc: 100.00%] [G loss: 0.127900, mse: 0.123129]\n",
      "151 [D loss: 0.065774, acc: 100.00%] [G loss: 0.113271, mse: 0.108066]\n",
      "152 [D loss: 0.056388, acc: 100.00%] [G loss: 0.130203, mse: 0.125111]\n",
      "153 [D loss: 0.062348, acc: 100.00%] [G loss: 0.123006, mse: 0.117783]\n",
      "154 [D loss: 0.064147, acc: 100.00%] [G loss: 0.134271, mse: 0.129402]\n",
      "155 [D loss: 0.061027, acc: 100.00%] [G loss: 0.117801, mse: 0.112544]\n",
      "156 [D loss: 0.046644, acc: 100.00%] [G loss: 0.123512, mse: 0.117790]\n",
      "157 [D loss: 0.041396, acc: 100.00%] [G loss: 0.119645, mse: 0.113840]\n",
      "158 [D loss: 0.052496, acc: 100.00%] [G loss: 0.114470, mse: 0.109070]\n",
      "159 [D loss: 0.053480, acc: 100.00%] [G loss: 0.105554, mse: 0.099544]\n",
      "160 [D loss: 0.052660, acc: 100.00%] [G loss: 0.133744, mse: 0.128686]\n",
      "161 [D loss: 0.051892, acc: 100.00%] [G loss: 0.112000, mse: 0.106561]\n",
      "162 [D loss: 0.050093, acc: 100.00%] [G loss: 0.135842, mse: 0.130352]\n",
      "163 [D loss: 0.047966, acc: 100.00%] [G loss: 0.121471, mse: 0.116550]\n",
      "164 [D loss: 0.075925, acc: 96.88%] [G loss: 0.112722, mse: 0.107340]\n",
      "165 [D loss: 0.048000, acc: 100.00%] [G loss: 0.123928, mse: 0.118269]\n",
      "166 [D loss: 0.051135, acc: 100.00%] [G loss: 0.119449, mse: 0.114277]\n",
      "167 [D loss: 0.054525, acc: 100.00%] [G loss: 0.118482, mse: 0.112965]\n",
      "168 [D loss: 0.044730, acc: 100.00%] [G loss: 0.124225, mse: 0.118600]\n",
      "169 [D loss: 0.039709, acc: 100.00%] [G loss: 0.125970, mse: 0.120339]\n",
      "170 [D loss: 0.068176, acc: 98.44%] [G loss: 0.125637, mse: 0.120401]\n",
      "171 [D loss: 0.045797, acc: 100.00%] [G loss: 0.123713, mse: 0.117780]\n",
      "172 [D loss: 0.037779, acc: 100.00%] [G loss: 0.121897, mse: 0.116495]\n",
      "173 [D loss: 0.056556, acc: 100.00%] [G loss: 0.123682, mse: 0.118677]\n",
      "174 [D loss: 0.054763, acc: 100.00%] [G loss: 0.118203, mse: 0.112504]\n",
      "175 [D loss: 0.039151, acc: 100.00%] [G loss: 0.113270, mse: 0.108145]\n",
      "176 [D loss: 0.035959, acc: 100.00%] [G loss: 0.106117, mse: 0.100532]\n",
      "177 [D loss: 0.040769, acc: 100.00%] [G loss: 0.117084, mse: 0.111441]\n",
      "178 [D loss: 0.047786, acc: 100.00%] [G loss: 0.111315, mse: 0.105832]\n",
      "179 [D loss: 0.041961, acc: 100.00%] [G loss: 0.122740, mse: 0.117530]\n",
      "180 [D loss: 0.045971, acc: 100.00%] [G loss: 0.121678, mse: 0.116330]\n",
      "181 [D loss: 0.037249, acc: 100.00%] [G loss: 0.118867, mse: 0.113272]\n",
      "182 [D loss: 0.036231, acc: 100.00%] [G loss: 0.113439, mse: 0.107814]\n",
      "183 [D loss: 0.039822, acc: 100.00%] [G loss: 0.118024, mse: 0.112850]\n",
      "184 [D loss: 0.047973, acc: 100.00%] [G loss: 0.119986, mse: 0.114646]\n",
      "185 [D loss: 0.032368, acc: 100.00%] [G loss: 0.111891, mse: 0.105968]\n",
      "186 [D loss: 0.031371, acc: 100.00%] [G loss: 0.117727, mse: 0.112168]\n",
      "187 [D loss: 0.045555, acc: 100.00%] [G loss: 0.134120, mse: 0.129289]\n",
      "188 [D loss: 0.033919, acc: 100.00%] [G loss: 0.129184, mse: 0.123716]\n",
      "189 [D loss: 0.037445, acc: 100.00%] [G loss: 0.116471, mse: 0.111132]\n",
      "190 [D loss: 0.031653, acc: 100.00%] [G loss: 0.130172, mse: 0.124567]\n",
      "191 [D loss: 0.038226, acc: 100.00%] [G loss: 0.102283, mse: 0.096790]\n",
      "192 [D loss: 0.030583, acc: 100.00%] [G loss: 0.129000, mse: 0.123722]\n",
      "193 [D loss: 0.037588, acc: 100.00%] [G loss: 0.127402, mse: 0.122011]\n",
      "194 [D loss: 0.034990, acc: 100.00%] [G loss: 0.130650, mse: 0.125594]\n",
      "195 [D loss: 0.032879, acc: 100.00%] [G loss: 0.106191, mse: 0.100784]\n",
      "196 [D loss: 0.041584, acc: 100.00%] [G loss: 0.137525, mse: 0.132374]\n",
      "197 [D loss: 0.042234, acc: 100.00%] [G loss: 0.124124, mse: 0.119126]\n",
      "198 [D loss: 0.041585, acc: 100.00%] [G loss: 0.124799, mse: 0.119390]\n",
      "199 [D loss: 0.032483, acc: 100.00%] [G loss: 0.120418, mse: 0.114454]\n",
      "200 [D loss: 0.034797, acc: 100.00%] [G loss: 0.119224, mse: 0.113835]\n",
      "201 [D loss: 0.040868, acc: 100.00%] [G loss: 0.125921, mse: 0.120681]\n",
      "202 [D loss: 0.032741, acc: 100.00%] [G loss: 0.130213, mse: 0.124399]\n",
      "203 [D loss: 0.027763, acc: 100.00%] [G loss: 0.128117, mse: 0.122265]\n",
      "204 [D loss: 0.031065, acc: 100.00%] [G loss: 0.113843, mse: 0.108460]\n",
      "205 [D loss: 0.026517, acc: 100.00%] [G loss: 0.118857, mse: 0.113233]\n",
      "206 [D loss: 0.038708, acc: 100.00%] [G loss: 0.116435, mse: 0.110600]\n",
      "207 [D loss: 0.039376, acc: 100.00%] [G loss: 0.115393, mse: 0.109507]\n",
      "208 [D loss: 0.039553, acc: 100.00%] [G loss: 0.123079, mse: 0.116294]\n",
      "209 [D loss: 0.036076, acc: 100.00%] [G loss: 0.117603, mse: 0.111222]\n",
      "210 [D loss: 0.031085, acc: 100.00%] [G loss: 0.130582, mse: 0.124519]\n",
      "211 [D loss: 0.042741, acc: 100.00%] [G loss: 0.113850, mse: 0.108490]\n",
      "212 [D loss: 0.033050, acc: 100.00%] [G loss: 0.114064, mse: 0.108104]\n",
      "213 [D loss: 0.032285, acc: 100.00%] [G loss: 0.127217, mse: 0.121307]\n",
      "214 [D loss: 0.033046, acc: 100.00%] [G loss: 0.129871, mse: 0.123717]\n",
      "215 [D loss: 0.030378, acc: 100.00%] [G loss: 0.117714, mse: 0.112333]\n",
      "216 [D loss: 0.029546, acc: 100.00%] [G loss: 0.104189, mse: 0.098187]\n",
      "217 [D loss: 0.028487, acc: 100.00%] [G loss: 0.106720, mse: 0.100813]\n",
      "218 [D loss: 0.033358, acc: 100.00%] [G loss: 0.119998, mse: 0.114559]\n",
      "219 [D loss: 0.029028, acc: 100.00%] [G loss: 0.122068, mse: 0.116404]\n",
      "220 [D loss: 0.036556, acc: 100.00%] [G loss: 0.121515, mse: 0.115831]\n",
      "221 [D loss: 0.030843, acc: 100.00%] [G loss: 0.119718, mse: 0.113925]\n",
      "222 [D loss: 0.022889, acc: 100.00%] [G loss: 0.114493, mse: 0.108601]\n",
      "223 [D loss: 0.032657, acc: 100.00%] [G loss: 0.120558, mse: 0.114841]\n",
      "224 [D loss: 0.025104, acc: 100.00%] [G loss: 0.124008, mse: 0.117867]\n",
      "225 [D loss: 0.033417, acc: 100.00%] [G loss: 0.120532, mse: 0.114617]\n",
      "226 [D loss: 0.030490, acc: 100.00%] [G loss: 0.109642, mse: 0.103353]\n",
      "227 [D loss: 0.029224, acc: 100.00%] [G loss: 0.135349, mse: 0.129829]\n",
      "228 [D loss: 0.023007, acc: 100.00%] [G loss: 0.120538, mse: 0.114583]\n",
      "229 [D loss: 0.030380, acc: 100.00%] [G loss: 0.108056, mse: 0.102288]\n",
      "230 [D loss: 0.029102, acc: 100.00%] [G loss: 0.113528, mse: 0.107286]\n",
      "231 [D loss: 0.022459, acc: 100.00%] [G loss: 0.127085, mse: 0.121517]\n",
      "232 [D loss: 0.026760, acc: 100.00%] [G loss: 0.108549, mse: 0.102854]\n",
      "233 [D loss: 0.026426, acc: 100.00%] [G loss: 0.113163, mse: 0.107368]\n",
      "234 [D loss: 0.027480, acc: 100.00%] [G loss: 0.112865, mse: 0.107000]\n",
      "235 [D loss: 0.026745, acc: 100.00%] [G loss: 0.120042, mse: 0.114438]\n",
      "236 [D loss: 0.022977, acc: 100.00%] [G loss: 0.126133, mse: 0.120419]\n",
      "237 [D loss: 0.036025, acc: 100.00%] [G loss: 0.125087, mse: 0.119598]\n",
      "238 [D loss: 0.026099, acc: 100.00%] [G loss: 0.116471, mse: 0.110238]\n",
      "239 [D loss: 0.027804, acc: 100.00%] [G loss: 0.116938, mse: 0.111166]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240 [D loss: 0.018518, acc: 100.00%] [G loss: 0.115086, mse: 0.109027]\n",
      "241 [D loss: 0.025122, acc: 100.00%] [G loss: 0.104766, mse: 0.099108]\n",
      "242 [D loss: 0.034832, acc: 100.00%] [G loss: 0.128552, mse: 0.123130]\n",
      "243 [D loss: 0.030869, acc: 100.00%] [G loss: 0.117769, mse: 0.112382]\n",
      "244 [D loss: 0.022173, acc: 100.00%] [G loss: 0.114354, mse: 0.108657]\n",
      "245 [D loss: 0.032696, acc: 100.00%] [G loss: 0.116319, mse: 0.109877]\n",
      "246 [D loss: 0.026905, acc: 100.00%] [G loss: 0.119171, mse: 0.113402]\n",
      "247 [D loss: 0.032313, acc: 100.00%] [G loss: 0.115802, mse: 0.110127]\n",
      "248 [D loss: 0.025061, acc: 100.00%] [G loss: 0.120485, mse: 0.114594]\n",
      "249 [D loss: 0.026181, acc: 100.00%] [G loss: 0.120425, mse: 0.114294]\n",
      "250 [D loss: 0.020481, acc: 100.00%] [G loss: 0.105706, mse: 0.099417]\n",
      "251 [D loss: 0.028986, acc: 100.00%] [G loss: 0.121073, mse: 0.115080]\n",
      "252 [D loss: 0.020552, acc: 100.00%] [G loss: 0.121307, mse: 0.115152]\n",
      "253 [D loss: 0.035622, acc: 100.00%] [G loss: 0.142085, mse: 0.136524]\n",
      "254 [D loss: 0.020702, acc: 100.00%] [G loss: 0.111970, mse: 0.105157]\n",
      "255 [D loss: 0.027202, acc: 100.00%] [G loss: 0.130785, mse: 0.124588]\n",
      "256 [D loss: 0.021786, acc: 100.00%] [G loss: 0.122551, mse: 0.116578]\n",
      "257 [D loss: 0.030049, acc: 100.00%] [G loss: 0.128842, mse: 0.123277]\n",
      "258 [D loss: 0.026567, acc: 100.00%] [G loss: 0.135214, mse: 0.129014]\n",
      "259 [D loss: 0.025422, acc: 100.00%] [G loss: 0.102132, mse: 0.096094]\n",
      "260 [D loss: 0.022282, acc: 100.00%] [G loss: 0.101274, mse: 0.095110]\n",
      "261 [D loss: 0.022434, acc: 100.00%] [G loss: 0.115847, mse: 0.110046]\n",
      "262 [D loss: 0.025155, acc: 100.00%] [G loss: 0.122152, mse: 0.115904]\n",
      "263 [D loss: 0.018735, acc: 100.00%] [G loss: 0.113632, mse: 0.107158]\n",
      "264 [D loss: 0.019139, acc: 100.00%] [G loss: 0.132536, mse: 0.126858]\n",
      "265 [D loss: 0.024267, acc: 100.00%] [G loss: 0.126073, mse: 0.120303]\n",
      "266 [D loss: 0.034364, acc: 98.44%] [G loss: 0.113001, mse: 0.107111]\n",
      "267 [D loss: 0.020094, acc: 100.00%] [G loss: 0.101132, mse: 0.094924]\n",
      "268 [D loss: 0.026846, acc: 100.00%] [G loss: 0.129931, mse: 0.123657]\n",
      "269 [D loss: 0.021905, acc: 100.00%] [G loss: 0.111519, mse: 0.105277]\n",
      "270 [D loss: 0.018738, acc: 100.00%] [G loss: 0.123563, mse: 0.117210]\n",
      "271 [D loss: 0.026904, acc: 100.00%] [G loss: 0.129315, mse: 0.123447]\n",
      "272 [D loss: 0.039941, acc: 98.44%] [G loss: 0.111044, mse: 0.105047]\n",
      "273 [D loss: 0.022793, acc: 100.00%] [G loss: 0.123120, mse: 0.117381]\n",
      "274 [D loss: 0.014884, acc: 100.00%] [G loss: 0.107151, mse: 0.100667]\n",
      "275 [D loss: 0.018068, acc: 100.00%] [G loss: 0.112620, mse: 0.105740]\n",
      "276 [D loss: 0.026454, acc: 100.00%] [G loss: 0.122487, mse: 0.116945]\n",
      "277 [D loss: 0.033370, acc: 98.44%] [G loss: 0.122766, mse: 0.115762]\n",
      "278 [D loss: 0.021517, acc: 100.00%] [G loss: 0.122926, mse: 0.116701]\n",
      "279 [D loss: 0.017897, acc: 100.00%] [G loss: 0.110539, mse: 0.103840]\n",
      "280 [D loss: 0.021420, acc: 100.00%] [G loss: 0.102417, mse: 0.095906]\n",
      "281 [D loss: 0.019814, acc: 100.00%] [G loss: 0.122045, mse: 0.116343]\n",
      "282 [D loss: 0.015655, acc: 100.00%] [G loss: 0.096338, mse: 0.089898]\n",
      "283 [D loss: 0.018994, acc: 100.00%] [G loss: 0.111455, mse: 0.105299]\n",
      "284 [D loss: 0.019541, acc: 100.00%] [G loss: 0.119092, mse: 0.112958]\n",
      "285 [D loss: 0.031652, acc: 100.00%] [G loss: 0.115544, mse: 0.109872]\n",
      "286 [D loss: 0.016161, acc: 100.00%] [G loss: 0.108674, mse: 0.102127]\n",
      "287 [D loss: 0.020089, acc: 100.00%] [G loss: 0.106755, mse: 0.100396]\n",
      "288 [D loss: 0.020538, acc: 100.00%] [G loss: 0.109679, mse: 0.103785]\n",
      "289 [D loss: 0.020253, acc: 100.00%] [G loss: 0.111645, mse: 0.104996]\n",
      "290 [D loss: 0.019543, acc: 100.00%] [G loss: 0.102553, mse: 0.096486]\n",
      "291 [D loss: 0.026993, acc: 100.00%] [G loss: 0.104665, mse: 0.098388]\n",
      "292 [D loss: 0.023144, acc: 100.00%] [G loss: 0.124878, mse: 0.119140]\n",
      "293 [D loss: 0.023710, acc: 100.00%] [G loss: 0.125947, mse: 0.120314]\n",
      "294 [D loss: 0.017604, acc: 100.00%] [G loss: 0.114114, mse: 0.107660]\n",
      "295 [D loss: 0.015466, acc: 100.00%] [G loss: 0.115645, mse: 0.109584]\n",
      "296 [D loss: 0.019099, acc: 100.00%] [G loss: 0.132765, mse: 0.127002]\n",
      "297 [D loss: 0.019696, acc: 100.00%] [G loss: 0.109469, mse: 0.103295]\n",
      "298 [D loss: 0.018167, acc: 100.00%] [G loss: 0.104398, mse: 0.098126]\n",
      "299 [D loss: 0.023492, acc: 100.00%] [G loss: 0.134066, mse: 0.128725]\n",
      "300 [D loss: 0.019786, acc: 100.00%] [G loss: 0.109525, mse: 0.103445]\n",
      "301 [D loss: 0.021279, acc: 100.00%] [G loss: 0.123685, mse: 0.118263]\n",
      "302 [D loss: 0.014944, acc: 100.00%] [G loss: 0.100756, mse: 0.093931]\n",
      "303 [D loss: 0.015746, acc: 100.00%] [G loss: 0.121093, mse: 0.114759]\n",
      "304 [D loss: 0.016134, acc: 100.00%] [G loss: 0.111444, mse: 0.105019]\n",
      "305 [D loss: 0.015620, acc: 100.00%] [G loss: 0.101830, mse: 0.095231]\n",
      "306 [D loss: 0.016384, acc: 100.00%] [G loss: 0.116882, mse: 0.110447]\n",
      "307 [D loss: 0.015263, acc: 100.00%] [G loss: 0.106195, mse: 0.099551]\n",
      "308 [D loss: 0.019505, acc: 100.00%] [G loss: 0.127418, mse: 0.121248]\n",
      "309 [D loss: 0.018891, acc: 100.00%] [G loss: 0.106192, mse: 0.100046]\n",
      "310 [D loss: 0.016338, acc: 100.00%] [G loss: 0.119223, mse: 0.112999]\n",
      "311 [D loss: 0.015694, acc: 100.00%] [G loss: 0.101086, mse: 0.095078]\n",
      "312 [D loss: 0.027280, acc: 100.00%] [G loss: 0.123286, mse: 0.117380]\n",
      "313 [D loss: 0.018425, acc: 100.00%] [G loss: 0.105023, mse: 0.098253]\n",
      "314 [D loss: 0.015685, acc: 100.00%] [G loss: 0.103626, mse: 0.097010]\n",
      "315 [D loss: 0.025993, acc: 100.00%] [G loss: 0.121117, mse: 0.115556]\n",
      "316 [D loss: 0.014598, acc: 100.00%] [G loss: 0.127889, mse: 0.121562]\n",
      "317 [D loss: 0.019651, acc: 100.00%] [G loss: 0.106927, mse: 0.100942]\n",
      "318 [D loss: 0.019021, acc: 100.00%] [G loss: 0.117942, mse: 0.111967]\n",
      "319 [D loss: 0.017783, acc: 100.00%] [G loss: 0.120329, mse: 0.113992]\n",
      "320 [D loss: 0.025352, acc: 100.00%] [G loss: 0.130198, mse: 0.123894]\n",
      "321 [D loss: 0.015938, acc: 100.00%] [G loss: 0.110270, mse: 0.103946]\n",
      "322 [D loss: 0.019560, acc: 100.00%] [G loss: 0.108133, mse: 0.101943]\n",
      "323 [D loss: 0.029864, acc: 98.44%] [G loss: 0.104517, mse: 0.098464]\n",
      "324 [D loss: 0.013375, acc: 100.00%] [G loss: 0.123116, mse: 0.116575]\n",
      "325 [D loss: 0.022045, acc: 100.00%] [G loss: 0.114237, mse: 0.107676]\n",
      "326 [D loss: 0.013604, acc: 100.00%] [G loss: 0.103246, mse: 0.096607]\n",
      "327 [D loss: 0.013749, acc: 100.00%] [G loss: 0.107572, mse: 0.101554]\n",
      "328 [D loss: 0.021679, acc: 100.00%] [G loss: 0.116186, mse: 0.109739]\n",
      "329 [D loss: 0.010765, acc: 100.00%] [G loss: 0.110518, mse: 0.103341]\n",
      "330 [D loss: 0.027528, acc: 100.00%] [G loss: 0.119856, mse: 0.113157]\n",
      "331 [D loss: 0.011397, acc: 100.00%] [G loss: 0.114578, mse: 0.107509]\n",
      "332 [D loss: 0.025780, acc: 100.00%] [G loss: 0.122777, mse: 0.115664]\n",
      "333 [D loss: 0.023729, acc: 100.00%] [G loss: 0.098459, mse: 0.092236]\n",
      "334 [D loss: 0.019300, acc: 100.00%] [G loss: 0.114277, mse: 0.107725]\n",
      "335 [D loss: 0.035483, acc: 98.44%] [G loss: 0.112102, mse: 0.105614]\n",
      "336 [D loss: 0.014286, acc: 100.00%] [G loss: 0.100647, mse: 0.093499]\n",
      "337 [D loss: 0.017456, acc: 100.00%] [G loss: 0.111216, mse: 0.104976]\n",
      "338 [D loss: 0.018708, acc: 100.00%] [G loss: 0.101995, mse: 0.094801]\n",
      "339 [D loss: 0.015445, acc: 100.00%] [G loss: 0.117996, mse: 0.110982]\n",
      "340 [D loss: 0.014003, acc: 100.00%] [G loss: 0.112097, mse: 0.105481]\n",
      "341 [D loss: 0.018923, acc: 100.00%] [G loss: 0.109283, mse: 0.102319]\n",
      "342 [D loss: 0.022644, acc: 100.00%] [G loss: 0.112059, mse: 0.105911]\n",
      "343 [D loss: 0.027570, acc: 100.00%] [G loss: 0.101870, mse: 0.096013]\n",
      "344 [D loss: 0.028982, acc: 100.00%] [G loss: 0.123046, mse: 0.116820]\n",
      "345 [D loss: 0.014118, acc: 100.00%] [G loss: 0.108940, mse: 0.102587]\n",
      "346 [D loss: 0.020648, acc: 100.00%] [G loss: 0.112648, mse: 0.105681]\n",
      "347 [D loss: 0.018127, acc: 100.00%] [G loss: 0.113976, mse: 0.107923]\n",
      "348 [D loss: 0.014609, acc: 100.00%] [G loss: 0.106503, mse: 0.099553]\n",
      "349 [D loss: 0.019398, acc: 100.00%] [G loss: 0.128682, mse: 0.122532]\n",
      "350 [D loss: 0.016104, acc: 100.00%] [G loss: 0.102293, mse: 0.095980]\n",
      "351 [D loss: 0.032634, acc: 98.44%] [G loss: 0.126064, mse: 0.120034]\n",
      "352 [D loss: 0.017483, acc: 100.00%] [G loss: 0.129823, mse: 0.123581]\n",
      "353 [D loss: 0.015908, acc: 100.00%] [G loss: 0.120893, mse: 0.114568]\n",
      "354 [D loss: 0.022405, acc: 100.00%] [G loss: 0.094008, mse: 0.086602]\n",
      "355 [D loss: 0.014174, acc: 100.00%] [G loss: 0.097019, mse: 0.090693]\n",
      "356 [D loss: 0.014219, acc: 100.00%] [G loss: 0.105294, mse: 0.098314]\n",
      "357 [D loss: 0.020234, acc: 100.00%] [G loss: 0.130772, mse: 0.124236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358 [D loss: 0.013547, acc: 100.00%] [G loss: 0.104706, mse: 0.097720]\n",
      "359 [D loss: 0.012419, acc: 100.00%] [G loss: 0.112736, mse: 0.106112]\n",
      "360 [D loss: 0.010971, acc: 100.00%] [G loss: 0.095877, mse: 0.089163]\n",
      "361 [D loss: 0.016836, acc: 100.00%] [G loss: 0.094811, mse: 0.088040]\n",
      "362 [D loss: 0.017313, acc: 100.00%] [G loss: 0.112725, mse: 0.106657]\n",
      "363 [D loss: 0.025845, acc: 98.44%] [G loss: 0.105281, mse: 0.098952]\n",
      "364 [D loss: 0.013006, acc: 100.00%] [G loss: 0.111063, mse: 0.104764]\n",
      "365 [D loss: 0.022767, acc: 100.00%] [G loss: 0.096979, mse: 0.090652]\n",
      "366 [D loss: 0.018398, acc: 100.00%] [G loss: 0.121998, mse: 0.115709]\n",
      "367 [D loss: 0.012137, acc: 100.00%] [G loss: 0.121859, mse: 0.115275]\n",
      "368 [D loss: 0.018903, acc: 100.00%] [G loss: 0.120657, mse: 0.114266]\n",
      "369 [D loss: 0.017797, acc: 100.00%] [G loss: 0.112377, mse: 0.106114]\n",
      "370 [D loss: 0.011167, acc: 100.00%] [G loss: 0.094846, mse: 0.087808]\n",
      "371 [D loss: 0.012801, acc: 100.00%] [G loss: 0.105578, mse: 0.098810]\n",
      "372 [D loss: 0.015258, acc: 100.00%] [G loss: 0.104988, mse: 0.098440]\n",
      "373 [D loss: 0.014697, acc: 100.00%] [G loss: 0.105086, mse: 0.098584]\n",
      "374 [D loss: 0.023976, acc: 100.00%] [G loss: 0.115658, mse: 0.109308]\n",
      "375 [D loss: 0.011471, acc: 100.00%] [G loss: 0.106143, mse: 0.099293]\n",
      "376 [D loss: 0.018904, acc: 100.00%] [G loss: 0.100978, mse: 0.094296]\n",
      "377 [D loss: 0.012341, acc: 100.00%] [G loss: 0.106580, mse: 0.100049]\n",
      "378 [D loss: 0.011226, acc: 100.00%] [G loss: 0.105642, mse: 0.098627]\n",
      "379 [D loss: 0.016147, acc: 100.00%] [G loss: 0.115681, mse: 0.109529]\n",
      "380 [D loss: 0.015455, acc: 100.00%] [G loss: 0.104608, mse: 0.098348]\n",
      "381 [D loss: 0.022155, acc: 100.00%] [G loss: 0.130843, mse: 0.125055]\n",
      "382 [D loss: 0.012982, acc: 100.00%] [G loss: 0.128656, mse: 0.122118]\n",
      "383 [D loss: 0.019862, acc: 100.00%] [G loss: 0.107445, mse: 0.101603]\n",
      "384 [D loss: 0.010234, acc: 100.00%] [G loss: 0.116517, mse: 0.110141]\n",
      "385 [D loss: 0.026378, acc: 100.00%] [G loss: 0.108764, mse: 0.102276]\n",
      "386 [D loss: 0.013166, acc: 100.00%] [G loss: 0.104084, mse: 0.097272]\n",
      "387 [D loss: 0.016801, acc: 100.00%] [G loss: 0.116468, mse: 0.110561]\n",
      "388 [D loss: 0.017005, acc: 100.00%] [G loss: 0.120322, mse: 0.114319]\n",
      "389 [D loss: 0.020871, acc: 100.00%] [G loss: 0.130905, mse: 0.125058]\n",
      "390 [D loss: 0.018925, acc: 100.00%] [G loss: 0.108426, mse: 0.102265]\n",
      "391 [D loss: 0.016123, acc: 100.00%] [G loss: 0.101656, mse: 0.095865]\n",
      "392 [D loss: 0.018588, acc: 100.00%] [G loss: 0.128125, mse: 0.122610]\n",
      "393 [D loss: 0.014347, acc: 100.00%] [G loss: 0.113819, mse: 0.107375]\n",
      "394 [D loss: 0.017061, acc: 100.00%] [G loss: 0.114309, mse: 0.107604]\n",
      "395 [D loss: 0.013628, acc: 100.00%] [G loss: 0.110709, mse: 0.103868]\n",
      "396 [D loss: 0.016875, acc: 100.00%] [G loss: 0.101759, mse: 0.095118]\n",
      "397 [D loss: 0.016371, acc: 100.00%] [G loss: 0.123760, mse: 0.116580]\n",
      "398 [D loss: 0.020165, acc: 100.00%] [G loss: 0.105844, mse: 0.099667]\n",
      "399 [D loss: 0.013468, acc: 100.00%] [G loss: 0.124063, mse: 0.116734]\n",
      "400 [D loss: 0.013779, acc: 100.00%] [G loss: 0.094357, mse: 0.087264]\n",
      "401 [D loss: 0.014425, acc: 100.00%] [G loss: 0.108545, mse: 0.101669]\n",
      "402 [D loss: 0.017236, acc: 100.00%] [G loss: 0.113561, mse: 0.106450]\n",
      "403 [D loss: 0.021936, acc: 100.00%] [G loss: 0.115091, mse: 0.108487]\n",
      "404 [D loss: 0.016010, acc: 100.00%] [G loss: 0.113792, mse: 0.107383]\n",
      "405 [D loss: 0.016123, acc: 100.00%] [G loss: 0.117431, mse: 0.110729]\n",
      "406 [D loss: 0.033782, acc: 98.44%] [G loss: 0.123219, mse: 0.116868]\n",
      "407 [D loss: 0.023933, acc: 100.00%] [G loss: 0.104901, mse: 0.098088]\n",
      "408 [D loss: 0.022890, acc: 100.00%] [G loss: 0.122261, mse: 0.116179]\n",
      "409 [D loss: 0.022390, acc: 100.00%] [G loss: 0.099395, mse: 0.092345]\n",
      "410 [D loss: 0.013294, acc: 100.00%] [G loss: 0.112803, mse: 0.105919]\n",
      "411 [D loss: 0.025872, acc: 100.00%] [G loss: 0.106508, mse: 0.099692]\n",
      "412 [D loss: 0.012016, acc: 100.00%] [G loss: 0.120618, mse: 0.113310]\n",
      "413 [D loss: 0.011929, acc: 100.00%] [G loss: 0.102733, mse: 0.096715]\n",
      "414 [D loss: 0.020752, acc: 100.00%] [G loss: 0.127616, mse: 0.120905]\n",
      "415 [D loss: 0.017225, acc: 100.00%] [G loss: 0.098192, mse: 0.090926]\n",
      "416 [D loss: 0.020131, acc: 100.00%] [G loss: 0.124699, mse: 0.117944]\n",
      "417 [D loss: 0.024163, acc: 100.00%] [G loss: 0.125104, mse: 0.118768]\n",
      "418 [D loss: 0.040219, acc: 98.44%] [G loss: 0.132883, mse: 0.126067]\n",
      "419 [D loss: 0.011117, acc: 100.00%] [G loss: 0.097842, mse: 0.090273]\n",
      "420 [D loss: 0.025746, acc: 100.00%] [G loss: 0.115428, mse: 0.108874]\n",
      "421 [D loss: 0.045974, acc: 96.88%] [G loss: 0.107878, mse: 0.101390]\n",
      "422 [D loss: 0.020065, acc: 100.00%] [G loss: 0.104082, mse: 0.096404]\n",
      "423 [D loss: 0.016805, acc: 100.00%] [G loss: 0.108119, mse: 0.100778]\n",
      "424 [D loss: 0.015548, acc: 100.00%] [G loss: 0.109207, mse: 0.102342]\n",
      "425 [D loss: 0.018491, acc: 100.00%] [G loss: 0.104827, mse: 0.097528]\n",
      "426 [D loss: 0.013910, acc: 100.00%] [G loss: 0.099846, mse: 0.093001]\n",
      "427 [D loss: 0.015507, acc: 100.00%] [G loss: 0.104220, mse: 0.097488]\n",
      "428 [D loss: 0.045234, acc: 98.44%] [G loss: 0.122297, mse: 0.115845]\n",
      "429 [D loss: 0.009281, acc: 100.00%] [G loss: 0.130888, mse: 0.123607]\n",
      "430 [D loss: 0.010646, acc: 100.00%] [G loss: 0.120266, mse: 0.113865]\n",
      "431 [D loss: 0.028414, acc: 100.00%] [G loss: 0.114922, mse: 0.108840]\n",
      "432 [D loss: 0.017355, acc: 100.00%] [G loss: 0.094180, mse: 0.087085]\n",
      "433 [D loss: 0.016484, acc: 100.00%] [G loss: 0.114045, mse: 0.107676]\n",
      "434 [D loss: 0.021145, acc: 100.00%] [G loss: 0.105398, mse: 0.097899]\n",
      "435 [D loss: 0.012787, acc: 100.00%] [G loss: 0.114537, mse: 0.107600]\n",
      "436 [D loss: 0.021917, acc: 100.00%] [G loss: 0.100873, mse: 0.093781]\n",
      "437 [D loss: 0.018163, acc: 100.00%] [G loss: 0.101030, mse: 0.093719]\n",
      "438 [D loss: 0.029354, acc: 98.44%] [G loss: 0.117223, mse: 0.110375]\n",
      "439 [D loss: 0.017190, acc: 100.00%] [G loss: 0.115614, mse: 0.109406]\n",
      "440 [D loss: 0.013245, acc: 100.00%] [G loss: 0.100208, mse: 0.092845]\n",
      "441 [D loss: 0.016417, acc: 100.00%] [G loss: 0.110533, mse: 0.103513]\n",
      "442 [D loss: 0.016757, acc: 100.00%] [G loss: 0.109405, mse: 0.103189]\n",
      "443 [D loss: 0.033721, acc: 98.44%] [G loss: 0.130774, mse: 0.124550]\n",
      "444 [D loss: 0.026143, acc: 100.00%] [G loss: 0.123739, mse: 0.117365]\n",
      "445 [D loss: 0.029438, acc: 98.44%] [G loss: 0.117324, mse: 0.110400]\n",
      "446 [D loss: 0.021948, acc: 100.00%] [G loss: 0.112438, mse: 0.105695]\n",
      "447 [D loss: 0.013869, acc: 100.00%] [G loss: 0.108965, mse: 0.102308]\n",
      "448 [D loss: 0.021420, acc: 100.00%] [G loss: 0.101951, mse: 0.094162]\n",
      "449 [D loss: 0.031693, acc: 98.44%] [G loss: 0.105555, mse: 0.098650]\n",
      "450 [D loss: 0.034410, acc: 98.44%] [G loss: 0.109891, mse: 0.103776]\n",
      "451 [D loss: 0.019094, acc: 100.00%] [G loss: 0.109863, mse: 0.103258]\n",
      "452 [D loss: 0.050354, acc: 98.44%] [G loss: 0.114929, mse: 0.107699]\n",
      "453 [D loss: 0.027592, acc: 98.44%] [G loss: 0.119810, mse: 0.112556]\n",
      "454 [D loss: 0.023380, acc: 100.00%] [G loss: 0.113848, mse: 0.106410]\n",
      "455 [D loss: 0.022220, acc: 100.00%] [G loss: 0.113401, mse: 0.106629]\n",
      "456 [D loss: 0.015077, acc: 100.00%] [G loss: 0.114605, mse: 0.107458]\n",
      "457 [D loss: 0.021328, acc: 100.00%] [G loss: 0.116955, mse: 0.110089]\n",
      "458 [D loss: 0.012535, acc: 100.00%] [G loss: 0.111889, mse: 0.105387]\n",
      "459 [D loss: 0.014328, acc: 100.00%] [G loss: 0.120092, mse: 0.113365]\n",
      "460 [D loss: 0.021991, acc: 100.00%] [G loss: 0.128421, mse: 0.121828]\n",
      "461 [D loss: 0.012603, acc: 100.00%] [G loss: 0.096923, mse: 0.090125]\n",
      "462 [D loss: 0.030663, acc: 100.00%] [G loss: 0.115311, mse: 0.109722]\n",
      "463 [D loss: 0.023255, acc: 100.00%] [G loss: 0.113873, mse: 0.107051]\n",
      "464 [D loss: 0.022312, acc: 100.00%] [G loss: 0.113070, mse: 0.106485]\n",
      "465 [D loss: 0.028321, acc: 100.00%] [G loss: 0.124357, mse: 0.117408]\n",
      "466 [D loss: 0.019959, acc: 100.00%] [G loss: 0.117710, mse: 0.110742]\n",
      "467 [D loss: 0.033823, acc: 98.44%] [G loss: 0.119022, mse: 0.112989]\n",
      "468 [D loss: 0.019425, acc: 100.00%] [G loss: 0.111676, mse: 0.104760]\n",
      "469 [D loss: 0.016172, acc: 100.00%] [G loss: 0.126445, mse: 0.120099]\n",
      "470 [D loss: 0.013352, acc: 100.00%] [G loss: 0.105911, mse: 0.098798]\n",
      "471 [D loss: 0.029064, acc: 100.00%] [G loss: 0.084765, mse: 0.077708]\n",
      "472 [D loss: 0.026177, acc: 100.00%] [G loss: 0.099044, mse: 0.092119]\n",
      "473 [D loss: 0.028433, acc: 100.00%] [G loss: 0.120579, mse: 0.113528]\n",
      "474 [D loss: 0.015361, acc: 100.00%] [G loss: 0.109179, mse: 0.102075]\n",
      "475 [D loss: 0.019614, acc: 100.00%] [G loss: 0.119226, mse: 0.112741]\n",
      "476 [D loss: 0.012425, acc: 100.00%] [G loss: 0.101852, mse: 0.095031]\n",
      "477 [D loss: 0.052331, acc: 98.44%] [G loss: 0.108770, mse: 0.102545]\n",
      "478 [D loss: 0.014309, acc: 100.00%] [G loss: 0.101717, mse: 0.094602]\n",
      "479 [D loss: 0.063552, acc: 98.44%] [G loss: 0.103756, mse: 0.096603]\n",
      "480 [D loss: 0.016363, acc: 100.00%] [G loss: 0.117059, mse: 0.109611]\n",
      "481 [D loss: 0.020351, acc: 100.00%] [G loss: 0.102029, mse: 0.095083]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "482 [D loss: 0.014453, acc: 100.00%] [G loss: 0.106115, mse: 0.099078]\n",
      "483 [D loss: 0.017408, acc: 100.00%] [G loss: 0.123042, mse: 0.115921]\n",
      "484 [D loss: 0.024188, acc: 100.00%] [G loss: 0.105952, mse: 0.098349]\n",
      "485 [D loss: 0.021645, acc: 100.00%] [G loss: 0.120169, mse: 0.113380]\n",
      "486 [D loss: 0.010673, acc: 100.00%] [G loss: 0.117437, mse: 0.110914]\n",
      "487 [D loss: 0.021769, acc: 100.00%] [G loss: 0.102592, mse: 0.096176]\n",
      "488 [D loss: 0.019578, acc: 100.00%] [G loss: 0.114288, mse: 0.107272]\n",
      "489 [D loss: 0.014874, acc: 100.00%] [G loss: 0.115966, mse: 0.109071]\n",
      "490 [D loss: 0.029983, acc: 100.00%] [G loss: 0.099159, mse: 0.092082]\n",
      "491 [D loss: 0.011759, acc: 100.00%] [G loss: 0.108632, mse: 0.102065]\n",
      "492 [D loss: 0.023598, acc: 100.00%] [G loss: 0.111347, mse: 0.104702]\n",
      "493 [D loss: 0.011421, acc: 100.00%] [G loss: 0.099322, mse: 0.091462]\n",
      "494 [D loss: 0.045625, acc: 98.44%] [G loss: 0.116977, mse: 0.111262]\n",
      "495 [D loss: 0.029454, acc: 100.00%] [G loss: 0.114118, mse: 0.106620]\n",
      "496 [D loss: 0.023907, acc: 100.00%] [G loss: 0.112192, mse: 0.104949]\n",
      "497 [D loss: 0.013680, acc: 100.00%] [G loss: 0.111607, mse: 0.104544]\n",
      "498 [D loss: 0.031973, acc: 98.44%] [G loss: 0.105154, mse: 0.098687]\n",
      "499 [D loss: 0.013947, acc: 100.00%] [G loss: 0.099943, mse: 0.093330]\n",
      "500 [D loss: 0.027947, acc: 100.00%] [G loss: 0.108608, mse: 0.101615]\n",
      "501 [D loss: 0.028642, acc: 100.00%] [G loss: 0.101451, mse: 0.095007]\n",
      "502 [D loss: 0.007549, acc: 100.00%] [G loss: 0.118859, mse: 0.111934]\n",
      "503 [D loss: 0.034932, acc: 98.44%] [G loss: 0.109930, mse: 0.103657]\n",
      "504 [D loss: 0.017048, acc: 100.00%] [G loss: 0.126063, mse: 0.120108]\n",
      "505 [D loss: 0.018711, acc: 100.00%] [G loss: 0.105685, mse: 0.097864]\n",
      "506 [D loss: 0.018712, acc: 100.00%] [G loss: 0.111033, mse: 0.103992]\n",
      "507 [D loss: 0.042676, acc: 98.44%] [G loss: 0.104982, mse: 0.097877]\n",
      "508 [D loss: 0.016493, acc: 100.00%] [G loss: 0.117778, mse: 0.111028]\n",
      "509 [D loss: 0.012540, acc: 100.00%] [G loss: 0.110102, mse: 0.103672]\n",
      "510 [D loss: 0.025749, acc: 98.44%] [G loss: 0.129705, mse: 0.122597]\n",
      "511 [D loss: 0.029899, acc: 100.00%] [G loss: 0.128488, mse: 0.122576]\n",
      "512 [D loss: 0.035920, acc: 98.44%] [G loss: 0.110166, mse: 0.103088]\n",
      "513 [D loss: 0.067386, acc: 96.88%] [G loss: 0.112354, mse: 0.105634]\n",
      "514 [D loss: 0.021604, acc: 100.00%] [G loss: 0.106153, mse: 0.099116]\n",
      "515 [D loss: 0.023503, acc: 100.00%] [G loss: 0.108996, mse: 0.102175]\n",
      "516 [D loss: 0.012616, acc: 100.00%] [G loss: 0.092779, mse: 0.085644]\n",
      "517 [D loss: 0.026833, acc: 100.00%] [G loss: 0.101483, mse: 0.095258]\n",
      "518 [D loss: 0.019637, acc: 100.00%] [G loss: 0.102858, mse: 0.095372]\n",
      "519 [D loss: 0.020863, acc: 100.00%] [G loss: 0.107920, mse: 0.100663]\n",
      "520 [D loss: 0.027113, acc: 100.00%] [G loss: 0.100056, mse: 0.092848]\n",
      "521 [D loss: 0.021578, acc: 100.00%] [G loss: 0.114657, mse: 0.107452]\n",
      "522 [D loss: 0.014913, acc: 100.00%] [G loss: 0.103475, mse: 0.097047]\n",
      "523 [D loss: 0.038105, acc: 98.44%] [G loss: 0.109422, mse: 0.102522]\n",
      "524 [D loss: 0.026099, acc: 100.00%] [G loss: 0.111868, mse: 0.104799]\n",
      "525 [D loss: 0.027992, acc: 100.00%] [G loss: 0.122168, mse: 0.115674]\n",
      "526 [D loss: 0.021373, acc: 100.00%] [G loss: 0.109283, mse: 0.103208]\n",
      "527 [D loss: 0.032284, acc: 100.00%] [G loss: 0.113859, mse: 0.108259]\n",
      "528 [D loss: 0.051767, acc: 98.44%] [G loss: 0.099601, mse: 0.092885]\n",
      "529 [D loss: 0.017811, acc: 100.00%] [G loss: 0.094692, mse: 0.087378]\n",
      "530 [D loss: 0.013053, acc: 100.00%] [G loss: 0.113033, mse: 0.106123]\n",
      "531 [D loss: 0.020995, acc: 100.00%] [G loss: 0.112351, mse: 0.105403]\n",
      "532 [D loss: 0.019197, acc: 100.00%] [G loss: 0.108126, mse: 0.101613]\n",
      "533 [D loss: 0.024856, acc: 100.00%] [G loss: 0.106243, mse: 0.100212]\n",
      "534 [D loss: 0.012869, acc: 100.00%] [G loss: 0.122441, mse: 0.114549]\n",
      "535 [D loss: 0.024703, acc: 100.00%] [G loss: 0.096827, mse: 0.089669]\n",
      "536 [D loss: 0.039100, acc: 98.44%] [G loss: 0.123894, mse: 0.117469]\n",
      "537 [D loss: 0.023486, acc: 100.00%] [G loss: 0.112933, mse: 0.106502]\n",
      "538 [D loss: 0.034297, acc: 98.44%] [G loss: 0.114928, mse: 0.107623]\n",
      "539 [D loss: 0.021285, acc: 100.00%] [G loss: 0.109087, mse: 0.101088]\n",
      "540 [D loss: 0.028509, acc: 100.00%] [G loss: 0.102467, mse: 0.096086]\n",
      "541 [D loss: 0.015389, acc: 100.00%] [G loss: 0.109331, mse: 0.102726]\n",
      "542 [D loss: 0.012188, acc: 100.00%] [G loss: 0.101461, mse: 0.093746]\n",
      "543 [D loss: 0.015173, acc: 100.00%] [G loss: 0.108913, mse: 0.101848]\n",
      "544 [D loss: 0.026443, acc: 100.00%] [G loss: 0.109010, mse: 0.102109]\n",
      "545 [D loss: 0.050712, acc: 96.88%] [G loss: 0.105820, mse: 0.098241]\n",
      "546 [D loss: 0.041968, acc: 98.44%] [G loss: 0.107478, mse: 0.100873]\n",
      "547 [D loss: 0.017801, acc: 100.00%] [G loss: 0.097063, mse: 0.090132]\n",
      "548 [D loss: 0.078385, acc: 95.31%] [G loss: 0.112181, mse: 0.105153]\n",
      "549 [D loss: 0.027520, acc: 100.00%] [G loss: 0.102079, mse: 0.094485]\n",
      "550 [D loss: 0.071938, acc: 98.44%] [G loss: 0.113685, mse: 0.107344]\n",
      "551 [D loss: 0.024768, acc: 100.00%] [G loss: 0.111006, mse: 0.103815]\n",
      "552 [D loss: 0.082607, acc: 98.44%] [G loss: 0.110750, mse: 0.104093]\n",
      "553 [D loss: 0.017359, acc: 100.00%] [G loss: 0.099651, mse: 0.092028]\n",
      "554 [D loss: 0.018828, acc: 100.00%] [G loss: 0.113565, mse: 0.106138]\n",
      "555 [D loss: 0.058594, acc: 98.44%] [G loss: 0.096507, mse: 0.088869]\n",
      "556 [D loss: 0.023980, acc: 98.44%] [G loss: 0.109002, mse: 0.101298]\n",
      "557 [D loss: 0.033264, acc: 100.00%] [G loss: 0.101676, mse: 0.094071]\n",
      "558 [D loss: 0.053830, acc: 98.44%] [G loss: 0.113858, mse: 0.107797]\n",
      "559 [D loss: 0.017036, acc: 100.00%] [G loss: 0.109498, mse: 0.101494]\n",
      "560 [D loss: 0.032765, acc: 100.00%] [G loss: 0.095883, mse: 0.088407]\n",
      "561 [D loss: 0.021046, acc: 100.00%] [G loss: 0.107194, mse: 0.099669]\n",
      "562 [D loss: 0.023744, acc: 100.00%] [G loss: 0.119822, mse: 0.112809]\n",
      "563 [D loss: 0.030907, acc: 100.00%] [G loss: 0.118191, mse: 0.110882]\n",
      "564 [D loss: 0.041858, acc: 100.00%] [G loss: 0.108129, mse: 0.101935]\n",
      "565 [D loss: 0.023919, acc: 100.00%] [G loss: 0.101894, mse: 0.094782]\n",
      "566 [D loss: 0.024959, acc: 100.00%] [G loss: 0.104218, mse: 0.097660]\n",
      "567 [D loss: 0.020258, acc: 100.00%] [G loss: 0.116057, mse: 0.109671]\n",
      "568 [D loss: 0.031086, acc: 98.44%] [G loss: 0.098953, mse: 0.091788]\n",
      "569 [D loss: 0.008119, acc: 100.00%] [G loss: 0.109114, mse: 0.101624]\n",
      "570 [D loss: 0.042429, acc: 98.44%] [G loss: 0.111517, mse: 0.104698]\n",
      "571 [D loss: 0.018953, acc: 100.00%] [G loss: 0.105683, mse: 0.098943]\n",
      "572 [D loss: 0.026390, acc: 100.00%] [G loss: 0.095786, mse: 0.089449]\n",
      "573 [D loss: 0.022698, acc: 100.00%] [G loss: 0.103118, mse: 0.095990]\n",
      "574 [D loss: 0.015251, acc: 100.00%] [G loss: 0.106371, mse: 0.098597]\n",
      "575 [D loss: 0.020482, acc: 100.00%] [G loss: 0.110819, mse: 0.104069]\n",
      "576 [D loss: 0.024972, acc: 100.00%] [G loss: 0.107674, mse: 0.101095]\n",
      "577 [D loss: 0.022852, acc: 98.44%] [G loss: 0.109301, mse: 0.102151]\n",
      "578 [D loss: 0.031520, acc: 100.00%] [G loss: 0.128287, mse: 0.122828]\n",
      "579 [D loss: 0.021463, acc: 100.00%] [G loss: 0.101797, mse: 0.095367]\n",
      "580 [D loss: 0.038114, acc: 98.44%] [G loss: 0.101012, mse: 0.094800]\n",
      "581 [D loss: 0.032400, acc: 100.00%] [G loss: 0.116860, mse: 0.110972]\n",
      "582 [D loss: 0.055955, acc: 96.88%] [G loss: 0.117643, mse: 0.110888]\n",
      "583 [D loss: 0.039054, acc: 100.00%] [G loss: 0.113147, mse: 0.106675]\n",
      "584 [D loss: 0.024804, acc: 98.44%] [G loss: 0.103669, mse: 0.096159]\n",
      "585 [D loss: 0.044461, acc: 98.44%] [G loss: 0.118828, mse: 0.111012]\n",
      "586 [D loss: 0.051437, acc: 98.44%] [G loss: 0.112671, mse: 0.106147]\n",
      "587 [D loss: 0.051274, acc: 100.00%] [G loss: 0.113626, mse: 0.106709]\n",
      "588 [D loss: 0.038074, acc: 100.00%] [G loss: 0.116487, mse: 0.109824]\n",
      "589 [D loss: 0.056978, acc: 98.44%] [G loss: 0.101342, mse: 0.093716]\n",
      "590 [D loss: 0.021554, acc: 100.00%] [G loss: 0.107945, mse: 0.101080]\n",
      "591 [D loss: 0.041084, acc: 100.00%] [G loss: 0.107369, mse: 0.100337]\n",
      "592 [D loss: 0.076558, acc: 96.88%] [G loss: 0.095226, mse: 0.087848]\n",
      "593 [D loss: 0.023923, acc: 100.00%] [G loss: 0.106639, mse: 0.100394]\n",
      "594 [D loss: 0.016356, acc: 100.00%] [G loss: 0.108514, mse: 0.102014]\n",
      "595 [D loss: 0.038666, acc: 100.00%] [G loss: 0.116306, mse: 0.110852]\n",
      "596 [D loss: 0.021241, acc: 100.00%] [G loss: 0.095279, mse: 0.088787]\n",
      "597 [D loss: 0.025278, acc: 98.44%] [G loss: 0.111454, mse: 0.104947]\n",
      "598 [D loss: 0.050490, acc: 98.44%] [G loss: 0.108534, mse: 0.101319]\n",
      "599 [D loss: 0.051114, acc: 98.44%] [G loss: 0.109244, mse: 0.102566]\n",
      "600 [D loss: 0.013729, acc: 100.00%] [G loss: 0.099368, mse: 0.091909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601 [D loss: 0.026721, acc: 100.00%] [G loss: 0.118015, mse: 0.111801]\n",
      "602 [D loss: 0.045063, acc: 98.44%] [G loss: 0.104354, mse: 0.098065]\n",
      "603 [D loss: 0.068724, acc: 96.88%] [G loss: 0.103370, mse: 0.096646]\n",
      "604 [D loss: 0.036800, acc: 100.00%] [G loss: 0.103441, mse: 0.096869]\n",
      "605 [D loss: 0.047354, acc: 98.44%] [G loss: 0.108461, mse: 0.101604]\n",
      "606 [D loss: 0.060362, acc: 96.88%] [G loss: 0.098541, mse: 0.092734]\n",
      "607 [D loss: 0.024737, acc: 100.00%] [G loss: 0.110494, mse: 0.104628]\n",
      "608 [D loss: 0.038110, acc: 98.44%] [G loss: 0.100483, mse: 0.092559]\n",
      "609 [D loss: 0.068513, acc: 96.88%] [G loss: 0.102059, mse: 0.095283]\n",
      "610 [D loss: 0.052487, acc: 98.44%] [G loss: 0.122259, mse: 0.115723]\n",
      "611 [D loss: 0.032153, acc: 100.00%] [G loss: 0.088070, mse: 0.081539]\n",
      "612 [D loss: 0.044761, acc: 98.44%] [G loss: 0.119534, mse: 0.112782]\n",
      "613 [D loss: 0.051422, acc: 96.88%] [G loss: 0.104644, mse: 0.096974]\n",
      "614 [D loss: 0.070089, acc: 96.88%] [G loss: 0.111162, mse: 0.103965]\n",
      "615 [D loss: 0.019738, acc: 100.00%] [G loss: 0.099487, mse: 0.092023]\n",
      "616 [D loss: 0.035861, acc: 100.00%] [G loss: 0.105034, mse: 0.097782]\n",
      "617 [D loss: 0.047334, acc: 98.44%] [G loss: 0.115388, mse: 0.107885]\n",
      "618 [D loss: 0.054336, acc: 100.00%] [G loss: 0.105762, mse: 0.099903]\n",
      "619 [D loss: 0.052589, acc: 98.44%] [G loss: 0.098122, mse: 0.091688]\n",
      "620 [D loss: 0.034243, acc: 100.00%] [G loss: 0.110183, mse: 0.103010]\n",
      "621 [D loss: 0.033451, acc: 98.44%] [G loss: 0.101716, mse: 0.094867]\n",
      "622 [D loss: 0.065089, acc: 96.88%] [G loss: 0.097139, mse: 0.091659]\n",
      "623 [D loss: 0.052522, acc: 96.88%] [G loss: 0.100007, mse: 0.093812]\n",
      "624 [D loss: 0.038620, acc: 100.00%] [G loss: 0.093271, mse: 0.086969]\n",
      "625 [D loss: 0.056320, acc: 98.44%] [G loss: 0.114380, mse: 0.108560]\n",
      "626 [D loss: 0.028389, acc: 100.00%] [G loss: 0.104443, mse: 0.097140]\n",
      "627 [D loss: 0.037189, acc: 98.44%] [G loss: 0.114368, mse: 0.106572]\n",
      "628 [D loss: 0.026083, acc: 100.00%] [G loss: 0.101972, mse: 0.094574]\n",
      "629 [D loss: 0.030316, acc: 100.00%] [G loss: 0.103958, mse: 0.097027]\n",
      "630 [D loss: 0.029372, acc: 100.00%] [G loss: 0.115246, mse: 0.108486]\n",
      "631 [D loss: 0.030475, acc: 100.00%] [G loss: 0.108204, mse: 0.101736]\n",
      "632 [D loss: 0.092610, acc: 93.75%] [G loss: 0.110039, mse: 0.103482]\n",
      "633 [D loss: 0.045359, acc: 100.00%] [G loss: 0.105961, mse: 0.099544]\n",
      "634 [D loss: 0.057553, acc: 98.44%] [G loss: 0.110418, mse: 0.104140]\n",
      "635 [D loss: 0.031074, acc: 100.00%] [G loss: 0.096550, mse: 0.089946]\n",
      "636 [D loss: 0.045832, acc: 100.00%] [G loss: 0.119801, mse: 0.114521]\n",
      "637 [D loss: 0.039753, acc: 96.88%] [G loss: 0.115396, mse: 0.108290]\n",
      "638 [D loss: 0.029278, acc: 100.00%] [G loss: 0.117858, mse: 0.111278]\n",
      "639 [D loss: 0.088170, acc: 96.88%] [G loss: 0.100841, mse: 0.094332]\n",
      "640 [D loss: 0.022199, acc: 100.00%] [G loss: 0.103743, mse: 0.096789]\n",
      "641 [D loss: 0.033085, acc: 98.44%] [G loss: 0.102640, mse: 0.096482]\n",
      "642 [D loss: 0.033632, acc: 100.00%] [G loss: 0.104550, mse: 0.097361]\n",
      "643 [D loss: 0.047043, acc: 98.44%] [G loss: 0.095659, mse: 0.088626]\n",
      "644 [D loss: 0.046855, acc: 98.44%] [G loss: 0.104443, mse: 0.097330]\n",
      "645 [D loss: 0.056094, acc: 96.88%] [G loss: 0.108283, mse: 0.101999]\n",
      "646 [D loss: 0.066347, acc: 98.44%] [G loss: 0.110265, mse: 0.103248]\n",
      "647 [D loss: 0.132635, acc: 93.75%] [G loss: 0.107698, mse: 0.102148]\n",
      "648 [D loss: 0.034770, acc: 98.44%] [G loss: 0.115955, mse: 0.110048]\n",
      "649 [D loss: 0.059730, acc: 96.88%] [G loss: 0.101790, mse: 0.095882]\n",
      "650 [D loss: 0.043189, acc: 98.44%] [G loss: 0.107488, mse: 0.101223]\n",
      "651 [D loss: 0.070078, acc: 96.88%] [G loss: 0.100608, mse: 0.093108]\n",
      "652 [D loss: 0.025178, acc: 100.00%] [G loss: 0.089943, mse: 0.082647]\n",
      "653 [D loss: 0.067385, acc: 98.44%] [G loss: 0.099717, mse: 0.093710]\n",
      "654 [D loss: 0.083001, acc: 98.44%] [G loss: 0.109226, mse: 0.102767]\n",
      "655 [D loss: 0.120494, acc: 95.31%] [G loss: 0.109747, mse: 0.103756]\n",
      "656 [D loss: 0.031488, acc: 100.00%] [G loss: 0.104013, mse: 0.097806]\n",
      "657 [D loss: 0.035361, acc: 100.00%] [G loss: 0.105567, mse: 0.099203]\n",
      "658 [D loss: 0.051418, acc: 98.44%] [G loss: 0.100309, mse: 0.093816]\n",
      "659 [D loss: 0.027034, acc: 100.00%] [G loss: 0.116371, mse: 0.109393]\n",
      "660 [D loss: 0.040659, acc: 98.44%] [G loss: 0.105208, mse: 0.098880]\n",
      "661 [D loss: 0.076553, acc: 95.31%] [G loss: 0.106076, mse: 0.099887]\n",
      "662 [D loss: 0.029511, acc: 100.00%] [G loss: 0.114428, mse: 0.108912]\n",
      "663 [D loss: 0.032196, acc: 100.00%] [G loss: 0.101738, mse: 0.095072]\n",
      "664 [D loss: 0.106485, acc: 95.31%] [G loss: 0.095636, mse: 0.089416]\n",
      "665 [D loss: 0.078493, acc: 98.44%] [G loss: 0.105793, mse: 0.098803]\n",
      "666 [D loss: 0.026101, acc: 100.00%] [G loss: 0.105599, mse: 0.099156]\n",
      "667 [D loss: 0.043022, acc: 98.44%] [G loss: 0.089438, mse: 0.082745]\n",
      "668 [D loss: 0.047711, acc: 98.44%] [G loss: 0.097449, mse: 0.091281]\n",
      "669 [D loss: 0.030989, acc: 100.00%] [G loss: 0.100608, mse: 0.093428]\n",
      "670 [D loss: 0.041834, acc: 98.44%] [G loss: 0.106961, mse: 0.101360]\n",
      "671 [D loss: 0.034010, acc: 100.00%] [G loss: 0.107451, mse: 0.101501]\n",
      "672 [D loss: 0.038289, acc: 100.00%] [G loss: 0.115146, mse: 0.108653]\n",
      "673 [D loss: 0.038689, acc: 100.00%] [G loss: 0.109063, mse: 0.102692]\n",
      "674 [D loss: 0.069054, acc: 98.44%] [G loss: 0.119365, mse: 0.113775]\n",
      "675 [D loss: 0.099834, acc: 96.88%] [G loss: 0.103773, mse: 0.097256]\n",
      "676 [D loss: 0.083380, acc: 96.88%] [G loss: 0.105359, mse: 0.099602]\n",
      "677 [D loss: 0.043093, acc: 98.44%] [G loss: 0.089425, mse: 0.082234]\n",
      "678 [D loss: 0.030213, acc: 100.00%] [G loss: 0.089079, mse: 0.081903]\n",
      "679 [D loss: 0.034488, acc: 100.00%] [G loss: 0.101195, mse: 0.094554]\n",
      "680 [D loss: 0.032782, acc: 100.00%] [G loss: 0.115262, mse: 0.109117]\n",
      "681 [D loss: 0.032946, acc: 98.44%] [G loss: 0.116172, mse: 0.109529]\n",
      "682 [D loss: 0.060862, acc: 98.44%] [G loss: 0.105522, mse: 0.098339]\n",
      "683 [D loss: 0.069742, acc: 96.88%] [G loss: 0.104484, mse: 0.098180]\n",
      "684 [D loss: 0.058205, acc: 98.44%] [G loss: 0.107531, mse: 0.101554]\n",
      "685 [D loss: 0.063218, acc: 96.88%] [G loss: 0.112969, mse: 0.106342]\n",
      "686 [D loss: 0.016753, acc: 100.00%] [G loss: 0.103570, mse: 0.097035]\n",
      "687 [D loss: 0.074075, acc: 96.88%] [G loss: 0.108004, mse: 0.101653]\n",
      "688 [D loss: 0.026363, acc: 100.00%] [G loss: 0.099048, mse: 0.091868]\n",
      "689 [D loss: 0.092906, acc: 96.88%] [G loss: 0.108722, mse: 0.102109]\n",
      "690 [D loss: 0.052319, acc: 96.88%] [G loss: 0.102483, mse: 0.096297]\n",
      "691 [D loss: 0.108084, acc: 98.44%] [G loss: 0.102123, mse: 0.096073]\n",
      "692 [D loss: 0.078351, acc: 96.88%] [G loss: 0.090345, mse: 0.083544]\n",
      "693 [D loss: 0.047589, acc: 98.44%] [G loss: 0.089607, mse: 0.082676]\n",
      "694 [D loss: 0.032057, acc: 98.44%] [G loss: 0.102958, mse: 0.096167]\n",
      "695 [D loss: 0.050891, acc: 98.44%] [G loss: 0.106327, mse: 0.099478]\n",
      "696 [D loss: 0.075886, acc: 96.88%] [G loss: 0.103984, mse: 0.097135]\n",
      "697 [D loss: 0.109257, acc: 92.19%] [G loss: 0.118727, mse: 0.112509]\n",
      "698 [D loss: 0.129911, acc: 95.31%] [G loss: 0.110049, mse: 0.104098]\n",
      "699 [D loss: 0.046300, acc: 98.44%] [G loss: 0.101050, mse: 0.094946]\n",
      "700 [D loss: 0.078611, acc: 98.44%] [G loss: 0.099351, mse: 0.092669]\n",
      "701 [D loss: 0.138163, acc: 96.88%] [G loss: 0.109879, mse: 0.103786]\n",
      "702 [D loss: 0.058944, acc: 98.44%] [G loss: 0.104005, mse: 0.097411]\n",
      "703 [D loss: 0.203853, acc: 95.31%] [G loss: 0.121608, mse: 0.115927]\n",
      "704 [D loss: 0.075665, acc: 98.44%] [G loss: 0.101224, mse: 0.095043]\n",
      "705 [D loss: 0.063856, acc: 98.44%] [G loss: 0.104308, mse: 0.097222]\n",
      "706 [D loss: 0.054804, acc: 98.44%] [G loss: 0.096816, mse: 0.089978]\n",
      "707 [D loss: 0.080726, acc: 96.88%] [G loss: 0.092552, mse: 0.085824]\n",
      "708 [D loss: 0.035662, acc: 98.44%] [G loss: 0.091031, mse: 0.084496]\n",
      "709 [D loss: 0.070683, acc: 96.88%] [G loss: 0.092446, mse: 0.086250]\n",
      "710 [D loss: 0.040297, acc: 98.44%] [G loss: 0.098075, mse: 0.091514]\n",
      "711 [D loss: 0.183647, acc: 93.75%] [G loss: 0.106967, mse: 0.100155]\n",
      "712 [D loss: 0.067158, acc: 98.44%] [G loss: 0.116722, mse: 0.110371]\n",
      "713 [D loss: 0.109916, acc: 98.44%] [G loss: 0.097657, mse: 0.091272]\n",
      "714 [D loss: 0.104754, acc: 96.88%] [G loss: 0.110995, mse: 0.104374]\n",
      "715 [D loss: 0.046731, acc: 98.44%] [G loss: 0.094081, mse: 0.087375]\n",
      "716 [D loss: 0.078721, acc: 98.44%] [G loss: 0.113476, mse: 0.107150]\n",
      "717 [D loss: 0.044418, acc: 98.44%] [G loss: 0.093860, mse: 0.086562]\n",
      "718 [D loss: 0.134858, acc: 92.19%] [G loss: 0.102857, mse: 0.097346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "719 [D loss: 0.128089, acc: 96.88%] [G loss: 0.098214, mse: 0.091940]\n",
      "720 [D loss: 0.072628, acc: 95.31%] [G loss: 0.106792, mse: 0.100327]\n",
      "721 [D loss: 0.112002, acc: 95.31%] [G loss: 0.116255, mse: 0.110443]\n",
      "722 [D loss: 0.110819, acc: 95.31%] [G loss: 0.108948, mse: 0.103447]\n",
      "723 [D loss: 0.073229, acc: 96.88%] [G loss: 0.102462, mse: 0.097477]\n",
      "724 [D loss: 0.074840, acc: 98.44%] [G loss: 0.112307, mse: 0.106448]\n",
      "725 [D loss: 0.074387, acc: 96.88%] [G loss: 0.094471, mse: 0.088433]\n",
      "726 [D loss: 0.043125, acc: 98.44%] [G loss: 0.108081, mse: 0.102212]\n",
      "727 [D loss: 0.126157, acc: 95.31%] [G loss: 0.089015, mse: 0.082935]\n",
      "728 [D loss: 0.152542, acc: 93.75%] [G loss: 0.113341, mse: 0.107769]\n",
      "729 [D loss: 0.047233, acc: 96.88%] [G loss: 0.092518, mse: 0.085587]\n",
      "730 [D loss: 0.094723, acc: 98.44%] [G loss: 0.098709, mse: 0.092194]\n",
      "731 [D loss: 0.152384, acc: 95.31%] [G loss: 0.111962, mse: 0.105137]\n",
      "732 [D loss: 0.167645, acc: 93.75%] [G loss: 0.110764, mse: 0.104151]\n",
      "733 [D loss: 0.086556, acc: 96.88%] [G loss: 0.099098, mse: 0.092505]\n",
      "734 [D loss: 0.133073, acc: 98.44%] [G loss: 0.098958, mse: 0.091812]\n",
      "735 [D loss: 0.175207, acc: 92.19%] [G loss: 0.106825, mse: 0.100871]\n",
      "736 [D loss: 0.139010, acc: 92.19%] [G loss: 0.111246, mse: 0.105064]\n",
      "737 [D loss: 0.088194, acc: 98.44%] [G loss: 0.113768, mse: 0.107768]\n",
      "738 [D loss: 0.079718, acc: 95.31%] [G loss: 0.098129, mse: 0.091656]\n",
      "739 [D loss: 0.096278, acc: 96.88%] [G loss: 0.107437, mse: 0.101114]\n",
      "740 [D loss: 0.037262, acc: 98.44%] [G loss: 0.104300, mse: 0.098336]\n",
      "741 [D loss: 0.159117, acc: 93.75%] [G loss: 0.108767, mse: 0.102958]\n",
      "742 [D loss: 0.194193, acc: 90.62%] [G loss: 0.090673, mse: 0.084496]\n",
      "743 [D loss: 0.107646, acc: 93.75%] [G loss: 0.126020, mse: 0.120109]\n",
      "744 [D loss: 0.080251, acc: 98.44%] [G loss: 0.098816, mse: 0.092363]\n",
      "745 [D loss: 0.086116, acc: 98.44%] [G loss: 0.112867, mse: 0.107068]\n",
      "746 [D loss: 0.097110, acc: 96.88%] [G loss: 0.095500, mse: 0.089790]\n",
      "747 [D loss: 0.066825, acc: 96.88%] [G loss: 0.099800, mse: 0.093900]\n",
      "748 [D loss: 0.078328, acc: 98.44%] [G loss: 0.094731, mse: 0.088673]\n",
      "749 [D loss: 0.055727, acc: 98.44%] [G loss: 0.104412, mse: 0.098973]\n",
      "750 [D loss: 0.040855, acc: 98.44%] [G loss: 0.107574, mse: 0.101664]\n",
      "751 [D loss: 0.107721, acc: 96.88%] [G loss: 0.101460, mse: 0.095886]\n",
      "752 [D loss: 0.073522, acc: 98.44%] [G loss: 0.104218, mse: 0.098196]\n",
      "753 [D loss: 0.064623, acc: 95.31%] [G loss: 0.097358, mse: 0.089946]\n",
      "754 [D loss: 0.152734, acc: 96.88%] [G loss: 0.100983, mse: 0.094337]\n",
      "755 [D loss: 0.126952, acc: 96.88%] [G loss: 0.111249, mse: 0.105173]\n",
      "756 [D loss: 0.089385, acc: 96.88%] [G loss: 0.094092, mse: 0.087263]\n",
      "757 [D loss: 0.061413, acc: 96.88%] [G loss: 0.096005, mse: 0.089493]\n",
      "758 [D loss: 0.095003, acc: 96.88%] [G loss: 0.112699, mse: 0.107318]\n",
      "759 [D loss: 0.074649, acc: 96.88%] [G loss: 0.086130, mse: 0.080565]\n",
      "760 [D loss: 0.056052, acc: 98.44%] [G loss: 0.105826, mse: 0.099705]\n",
      "761 [D loss: 0.113141, acc: 98.44%] [G loss: 0.108083, mse: 0.103316]\n",
      "762 [D loss: 0.120680, acc: 93.75%] [G loss: 0.114859, mse: 0.109271]\n",
      "763 [D loss: 0.113504, acc: 96.88%] [G loss: 0.103455, mse: 0.097751]\n",
      "764 [D loss: 0.051876, acc: 98.44%] [G loss: 0.099704, mse: 0.093379]\n",
      "765 [D loss: 0.076143, acc: 96.88%] [G loss: 0.103329, mse: 0.097798]\n",
      "766 [D loss: 0.087429, acc: 96.88%] [G loss: 0.095338, mse: 0.089571]\n",
      "767 [D loss: 0.066205, acc: 98.44%] [G loss: 0.095836, mse: 0.089654]\n",
      "768 [D loss: 0.127765, acc: 93.75%] [G loss: 0.102662, mse: 0.096762]\n",
      "769 [D loss: 0.099945, acc: 95.31%] [G loss: 0.088452, mse: 0.083156]\n",
      "770 [D loss: 0.168667, acc: 93.75%] [G loss: 0.113664, mse: 0.108389]\n",
      "771 [D loss: 0.103325, acc: 96.88%] [G loss: 0.099865, mse: 0.094172]\n",
      "772 [D loss: 0.143755, acc: 95.31%] [G loss: 0.101681, mse: 0.095910]\n",
      "773 [D loss: 0.064778, acc: 96.88%] [G loss: 0.083021, mse: 0.076717]\n",
      "774 [D loss: 0.095902, acc: 95.31%] [G loss: 0.100805, mse: 0.094259]\n",
      "775 [D loss: 0.063468, acc: 98.44%] [G loss: 0.094536, mse: 0.088159]\n",
      "776 [D loss: 0.069621, acc: 100.00%] [G loss: 0.103863, mse: 0.098734]\n",
      "777 [D loss: 0.163625, acc: 96.88%] [G loss: 0.103313, mse: 0.097722]\n",
      "778 [D loss: 0.133775, acc: 93.75%] [G loss: 0.113483, mse: 0.106941]\n",
      "779 [D loss: 0.192612, acc: 92.19%] [G loss: 0.112908, mse: 0.107710]\n",
      "780 [D loss: 0.107047, acc: 95.31%] [G loss: 0.100522, mse: 0.094468]\n",
      "781 [D loss: 0.061316, acc: 98.44%] [G loss: 0.101690, mse: 0.095703]\n",
      "782 [D loss: 0.206601, acc: 95.31%] [G loss: 0.103114, mse: 0.097030]\n",
      "783 [D loss: 0.136310, acc: 92.19%] [G loss: 0.108509, mse: 0.103554]\n",
      "784 [D loss: 0.111518, acc: 98.44%] [G loss: 0.092348, mse: 0.087269]\n",
      "785 [D loss: 0.096942, acc: 98.44%] [G loss: 0.100365, mse: 0.095464]\n",
      "786 [D loss: 0.114810, acc: 96.88%] [G loss: 0.104347, mse: 0.098420]\n",
      "787 [D loss: 0.177940, acc: 93.75%] [G loss: 0.112980, mse: 0.107460]\n",
      "788 [D loss: 0.194033, acc: 90.62%] [G loss: 0.106225, mse: 0.101097]\n",
      "789 [D loss: 0.098797, acc: 96.88%] [G loss: 0.106741, mse: 0.100975]\n",
      "790 [D loss: 0.131703, acc: 95.31%] [G loss: 0.110497, mse: 0.104947]\n",
      "791 [D loss: 0.040082, acc: 100.00%] [G loss: 0.104839, mse: 0.099401]\n",
      "792 [D loss: 0.195973, acc: 90.62%] [G loss: 0.099738, mse: 0.093795]\n",
      "793 [D loss: 0.135232, acc: 95.31%] [G loss: 0.108441, mse: 0.102833]\n",
      "794 [D loss: 0.084107, acc: 98.44%] [G loss: 0.107257, mse: 0.101546]\n",
      "795 [D loss: 0.148212, acc: 93.75%] [G loss: 0.105785, mse: 0.100418]\n",
      "796 [D loss: 0.109491, acc: 98.44%] [G loss: 0.102605, mse: 0.097394]\n",
      "797 [D loss: 0.139128, acc: 96.88%] [G loss: 0.096809, mse: 0.090033]\n",
      "798 [D loss: 0.222418, acc: 93.75%] [G loss: 0.111739, mse: 0.106022]\n",
      "799 [D loss: 0.242546, acc: 90.62%] [G loss: 0.103765, mse: 0.098665]\n",
      "800 [D loss: 0.065678, acc: 96.88%] [G loss: 0.105078, mse: 0.099407]\n",
      "801 [D loss: 0.147040, acc: 95.31%] [G loss: 0.113197, mse: 0.106822]\n",
      "802 [D loss: 0.184244, acc: 89.06%] [G loss: 0.096600, mse: 0.090805]\n",
      "803 [D loss: 0.133799, acc: 95.31%] [G loss: 0.091580, mse: 0.086845]\n",
      "804 [D loss: 0.073311, acc: 98.44%] [G loss: 0.107921, mse: 0.102096]\n",
      "805 [D loss: 0.056736, acc: 98.44%] [G loss: 0.119834, mse: 0.113747]\n",
      "806 [D loss: 0.060838, acc: 96.88%] [G loss: 0.098531, mse: 0.092203]\n",
      "807 [D loss: 0.168284, acc: 95.31%] [G loss: 0.106528, mse: 0.100907]\n",
      "808 [D loss: 0.100817, acc: 96.88%] [G loss: 0.095971, mse: 0.090151]\n",
      "809 [D loss: 0.171009, acc: 93.75%] [G loss: 0.108489, mse: 0.103747]\n",
      "810 [D loss: 0.111286, acc: 93.75%] [G loss: 0.107363, mse: 0.102145]\n",
      "811 [D loss: 0.125987, acc: 93.75%] [G loss: 0.093200, mse: 0.087619]\n",
      "812 [D loss: 0.124512, acc: 93.75%] [G loss: 0.113461, mse: 0.108253]\n",
      "813 [D loss: 0.063665, acc: 98.44%] [G loss: 0.091797, mse: 0.084886]\n",
      "814 [D loss: 0.081315, acc: 95.31%] [G loss: 0.092316, mse: 0.086820]\n",
      "815 [D loss: 0.168899, acc: 90.62%] [G loss: 0.108458, mse: 0.103374]\n",
      "816 [D loss: 0.214030, acc: 92.19%] [G loss: 0.097216, mse: 0.092049]\n",
      "817 [D loss: 0.094859, acc: 95.31%] [G loss: 0.114769, mse: 0.109408]\n",
      "818 [D loss: 0.092549, acc: 93.75%] [G loss: 0.103773, mse: 0.098192]\n",
      "819 [D loss: 0.068298, acc: 98.44%] [G loss: 0.104408, mse: 0.099101]\n",
      "820 [D loss: 0.235960, acc: 87.50%] [G loss: 0.105777, mse: 0.100995]\n",
      "821 [D loss: 0.077396, acc: 96.88%] [G loss: 0.103609, mse: 0.098404]\n",
      "822 [D loss: 0.169094, acc: 93.75%] [G loss: 0.117882, mse: 0.112803]\n",
      "823 [D loss: 0.304950, acc: 85.94%] [G loss: 0.097260, mse: 0.093312]\n",
      "824 [D loss: 0.173044, acc: 93.75%] [G loss: 0.103486, mse: 0.098451]\n",
      "825 [D loss: 0.122786, acc: 93.75%] [G loss: 0.106006, mse: 0.100539]\n",
      "826 [D loss: 0.148062, acc: 93.75%] [G loss: 0.101118, mse: 0.096072]\n",
      "827 [D loss: 0.208930, acc: 95.31%] [G loss: 0.100651, mse: 0.095831]\n",
      "828 [D loss: 0.178163, acc: 92.19%] [G loss: 0.096145, mse: 0.091130]\n",
      "829 [D loss: 0.162755, acc: 92.19%] [G loss: 0.103399, mse: 0.098196]\n",
      "830 [D loss: 0.086115, acc: 95.31%] [G loss: 0.095024, mse: 0.089753]\n",
      "831 [D loss: 0.218321, acc: 90.62%] [G loss: 0.089099, mse: 0.084002]\n",
      "832 [D loss: 0.291391, acc: 89.06%] [G loss: 0.104151, mse: 0.099463]\n",
      "833 [D loss: 0.107852, acc: 96.88%] [G loss: 0.099489, mse: 0.093647]\n",
      "834 [D loss: 0.213597, acc: 92.19%] [G loss: 0.090499, mse: 0.085938]\n",
      "835 [D loss: 0.138019, acc: 96.88%] [G loss: 0.089291, mse: 0.084183]\n",
      "836 [D loss: 0.106309, acc: 95.31%] [G loss: 0.104060, mse: 0.099348]\n",
      "837 [D loss: 0.126826, acc: 96.88%] [G loss: 0.096821, mse: 0.092341]\n",
      "838 [D loss: 0.106078, acc: 98.44%] [G loss: 0.098786, mse: 0.093254]\n",
      "839 [D loss: 0.216518, acc: 92.19%] [G loss: 0.110053, mse: 0.105511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "840 [D loss: 0.144183, acc: 95.31%] [G loss: 0.100755, mse: 0.095000]\n",
      "841 [D loss: 0.048777, acc: 100.00%] [G loss: 0.112902, mse: 0.107919]\n",
      "842 [D loss: 0.289642, acc: 90.62%] [G loss: 0.119040, mse: 0.113810]\n",
      "843 [D loss: 0.224495, acc: 90.62%] [G loss: 0.101544, mse: 0.096336]\n",
      "844 [D loss: 0.168311, acc: 90.62%] [G loss: 0.099713, mse: 0.094521]\n",
      "845 [D loss: 0.158252, acc: 93.75%] [G loss: 0.110824, mse: 0.104816]\n",
      "846 [D loss: 0.228684, acc: 90.62%] [G loss: 0.101344, mse: 0.096345]\n",
      "847 [D loss: 0.085127, acc: 96.88%] [G loss: 0.089214, mse: 0.084049]\n",
      "848 [D loss: 0.153904, acc: 95.31%] [G loss: 0.106541, mse: 0.101485]\n",
      "849 [D loss: 0.165429, acc: 93.75%] [G loss: 0.104652, mse: 0.099284]\n",
      "850 [D loss: 0.159009, acc: 95.31%] [G loss: 0.095175, mse: 0.089749]\n",
      "851 [D loss: 0.346986, acc: 85.94%] [G loss: 0.104720, mse: 0.099970]\n",
      "852 [D loss: 0.124363, acc: 95.31%] [G loss: 0.119714, mse: 0.114628]\n",
      "853 [D loss: 0.162540, acc: 92.19%] [G loss: 0.097108, mse: 0.091529]\n",
      "854 [D loss: 0.263213, acc: 92.19%] [G loss: 0.103759, mse: 0.098014]\n",
      "855 [D loss: 0.205155, acc: 92.19%] [G loss: 0.098687, mse: 0.094419]\n",
      "856 [D loss: 0.200910, acc: 90.62%] [G loss: 0.097844, mse: 0.092873]\n",
      "857 [D loss: 0.132630, acc: 95.31%] [G loss: 0.088254, mse: 0.083301]\n",
      "858 [D loss: 0.156714, acc: 93.75%] [G loss: 0.091548, mse: 0.087674]\n",
      "859 [D loss: 0.304108, acc: 89.06%] [G loss: 0.098518, mse: 0.094352]\n",
      "860 [D loss: 0.165126, acc: 92.19%] [G loss: 0.109439, mse: 0.104691]\n",
      "861 [D loss: 0.336469, acc: 85.94%] [G loss: 0.102626, mse: 0.098691]\n",
      "862 [D loss: 0.134077, acc: 95.31%] [G loss: 0.095415, mse: 0.089867]\n",
      "863 [D loss: 0.142241, acc: 95.31%] [G loss: 0.098809, mse: 0.093878]\n",
      "864 [D loss: 0.119229, acc: 95.31%] [G loss: 0.098456, mse: 0.092625]\n",
      "865 [D loss: 0.130199, acc: 95.31%] [G loss: 0.099568, mse: 0.094667]\n",
      "866 [D loss: 0.068819, acc: 98.44%] [G loss: 0.113324, mse: 0.108107]\n",
      "867 [D loss: 0.110625, acc: 93.75%] [G loss: 0.091660, mse: 0.086267]\n",
      "868 [D loss: 0.096835, acc: 96.88%] [G loss: 0.096164, mse: 0.090633]\n",
      "869 [D loss: 0.103652, acc: 96.88%] [G loss: 0.097021, mse: 0.091732]\n",
      "870 [D loss: 0.217921, acc: 90.62%] [G loss: 0.109357, mse: 0.105384]\n",
      "871 [D loss: 0.075282, acc: 98.44%] [G loss: 0.092466, mse: 0.087402]\n",
      "872 [D loss: 0.119262, acc: 96.88%] [G loss: 0.102226, mse: 0.097033]\n",
      "873 [D loss: 0.317813, acc: 90.62%] [G loss: 0.118607, mse: 0.114958]\n",
      "874 [D loss: 0.173173, acc: 95.31%] [G loss: 0.112680, mse: 0.107944]\n",
      "875 [D loss: 0.231196, acc: 93.75%] [G loss: 0.113753, mse: 0.109741]\n",
      "876 [D loss: 0.127536, acc: 90.62%] [G loss: 0.093784, mse: 0.088584]\n",
      "877 [D loss: 0.137894, acc: 95.31%] [G loss: 0.108558, mse: 0.101967]\n",
      "878 [D loss: 0.263677, acc: 92.19%] [G loss: 0.104574, mse: 0.099967]\n",
      "879 [D loss: 0.142139, acc: 93.75%] [G loss: 0.099496, mse: 0.093881]\n",
      "880 [D loss: 0.165001, acc: 95.31%] [G loss: 0.105654, mse: 0.099120]\n",
      "881 [D loss: 0.097860, acc: 96.88%] [G loss: 0.099881, mse: 0.094814]\n",
      "882 [D loss: 0.184193, acc: 90.62%] [G loss: 0.091885, mse: 0.086729]\n",
      "883 [D loss: 0.227390, acc: 92.19%] [G loss: 0.102811, mse: 0.097616]\n",
      "884 [D loss: 0.219661, acc: 89.06%] [G loss: 0.087255, mse: 0.081269]\n",
      "885 [D loss: 0.251355, acc: 90.62%] [G loss: 0.089245, mse: 0.084776]\n",
      "886 [D loss: 0.106292, acc: 96.88%] [G loss: 0.087865, mse: 0.082287]\n",
      "887 [D loss: 0.280849, acc: 87.50%] [G loss: 0.097567, mse: 0.092412]\n",
      "888 [D loss: 0.106493, acc: 98.44%] [G loss: 0.107224, mse: 0.101815]\n",
      "889 [D loss: 0.184309, acc: 92.19%] [G loss: 0.103846, mse: 0.098414]\n",
      "890 [D loss: 0.294337, acc: 89.06%] [G loss: 0.122756, mse: 0.117619]\n",
      "891 [D loss: 0.114252, acc: 92.19%] [G loss: 0.096546, mse: 0.091483]\n",
      "892 [D loss: 0.220116, acc: 93.75%] [G loss: 0.095718, mse: 0.090223]\n",
      "893 [D loss: 0.204956, acc: 92.19%] [G loss: 0.108509, mse: 0.103843]\n",
      "894 [D loss: 0.231290, acc: 92.19%] [G loss: 0.098390, mse: 0.094064]\n",
      "895 [D loss: 0.289554, acc: 92.19%] [G loss: 0.102304, mse: 0.098017]\n",
      "896 [D loss: 0.258495, acc: 89.06%] [G loss: 0.102056, mse: 0.098213]\n",
      "897 [D loss: 0.362646, acc: 85.94%] [G loss: 0.127084, mse: 0.123790]\n",
      "898 [D loss: 0.107759, acc: 95.31%] [G loss: 0.090541, mse: 0.085329]\n",
      "899 [D loss: 0.253983, acc: 90.62%] [G loss: 0.096058, mse: 0.091645]\n",
      "900 [D loss: 0.112884, acc: 96.88%] [G loss: 0.096657, mse: 0.091544]\n",
      "901 [D loss: 0.169480, acc: 95.31%] [G loss: 0.092829, mse: 0.088221]\n",
      "902 [D loss: 0.183048, acc: 93.75%] [G loss: 0.109403, mse: 0.104580]\n",
      "903 [D loss: 0.248449, acc: 87.50%] [G loss: 0.101851, mse: 0.097984]\n",
      "904 [D loss: 0.103097, acc: 96.88%] [G loss: 0.114132, mse: 0.109136]\n",
      "905 [D loss: 0.241407, acc: 89.06%] [G loss: 0.102624, mse: 0.097341]\n",
      "906 [D loss: 0.184755, acc: 92.19%] [G loss: 0.113935, mse: 0.109539]\n",
      "907 [D loss: 0.117912, acc: 93.75%] [G loss: 0.101514, mse: 0.096559]\n",
      "908 [D loss: 0.131760, acc: 95.31%] [G loss: 0.100653, mse: 0.095496]\n",
      "909 [D loss: 0.192839, acc: 95.31%] [G loss: 0.096559, mse: 0.091979]\n",
      "910 [D loss: 0.137475, acc: 95.31%] [G loss: 0.107337, mse: 0.101848]\n",
      "911 [D loss: 0.242586, acc: 89.06%] [G loss: 0.091012, mse: 0.086539]\n",
      "912 [D loss: 0.319994, acc: 85.94%] [G loss: 0.097257, mse: 0.092162]\n",
      "913 [D loss: 0.235183, acc: 84.38%] [G loss: 0.101419, mse: 0.097142]\n",
      "914 [D loss: 0.266771, acc: 89.06%] [G loss: 0.100022, mse: 0.095116]\n",
      "915 [D loss: 0.342151, acc: 89.06%] [G loss: 0.101993, mse: 0.097675]\n",
      "916 [D loss: 0.396886, acc: 82.81%] [G loss: 0.101880, mse: 0.097866]\n",
      "917 [D loss: 0.133587, acc: 96.88%] [G loss: 0.103647, mse: 0.097785]\n",
      "918 [D loss: 0.241423, acc: 92.19%] [G loss: 0.099688, mse: 0.094916]\n",
      "919 [D loss: 0.094954, acc: 96.88%] [G loss: 0.087803, mse: 0.082790]\n",
      "920 [D loss: 0.228485, acc: 84.38%] [G loss: 0.087580, mse: 0.082976]\n",
      "921 [D loss: 0.235677, acc: 89.06%] [G loss: 0.110041, mse: 0.106209]\n",
      "922 [D loss: 0.182636, acc: 93.75%] [G loss: 0.096255, mse: 0.091805]\n",
      "923 [D loss: 0.141056, acc: 93.75%] [G loss: 0.100258, mse: 0.095911]\n",
      "924 [D loss: 0.203012, acc: 89.06%] [G loss: 0.099027, mse: 0.094599]\n",
      "925 [D loss: 0.195526, acc: 92.19%] [G loss: 0.087426, mse: 0.082985]\n",
      "926 [D loss: 0.246113, acc: 90.62%] [G loss: 0.102523, mse: 0.097636]\n",
      "927 [D loss: 0.131358, acc: 93.75%] [G loss: 0.107747, mse: 0.102728]\n",
      "928 [D loss: 0.236207, acc: 90.62%] [G loss: 0.112744, mse: 0.108344]\n",
      "929 [D loss: 0.205293, acc: 93.75%] [G loss: 0.109196, mse: 0.103971]\n",
      "930 [D loss: 0.449728, acc: 79.69%] [G loss: 0.109569, mse: 0.104944]\n",
      "931 [D loss: 0.154301, acc: 92.19%] [G loss: 0.100461, mse: 0.096233]\n",
      "932 [D loss: 0.332230, acc: 90.62%] [G loss: 0.097663, mse: 0.093545]\n",
      "933 [D loss: 0.201526, acc: 92.19%] [G loss: 0.117352, mse: 0.113076]\n",
      "934 [D loss: 0.113941, acc: 98.44%] [G loss: 0.103895, mse: 0.099138]\n",
      "935 [D loss: 0.243061, acc: 92.19%] [G loss: 0.093896, mse: 0.089475]\n",
      "936 [D loss: 0.136609, acc: 95.31%] [G loss: 0.099874, mse: 0.094921]\n",
      "937 [D loss: 0.169918, acc: 92.19%] [G loss: 0.102824, mse: 0.098338]\n",
      "938 [D loss: 0.273462, acc: 89.06%] [G loss: 0.103420, mse: 0.099364]\n",
      "939 [D loss: 0.272948, acc: 89.06%] [G loss: 0.101796, mse: 0.098026]\n",
      "940 [D loss: 0.165694, acc: 90.62%] [G loss: 0.098422, mse: 0.094085]\n",
      "941 [D loss: 0.138016, acc: 92.19%] [G loss: 0.097419, mse: 0.092688]\n",
      "942 [D loss: 0.081026, acc: 98.44%] [G loss: 0.096911, mse: 0.091916]\n",
      "943 [D loss: 0.166390, acc: 95.31%] [G loss: 0.101137, mse: 0.096135]\n",
      "944 [D loss: 0.177293, acc: 95.31%] [G loss: 0.094125, mse: 0.089636]\n",
      "945 [D loss: 0.265869, acc: 89.06%] [G loss: 0.111185, mse: 0.107495]\n",
      "946 [D loss: 0.256403, acc: 90.62%] [G loss: 0.114220, mse: 0.110597]\n",
      "947 [D loss: 0.317150, acc: 87.50%] [G loss: 0.097901, mse: 0.093878]\n",
      "948 [D loss: 0.187628, acc: 95.31%] [G loss: 0.086748, mse: 0.081129]\n",
      "949 [D loss: 0.189428, acc: 92.19%] [G loss: 0.096991, mse: 0.092418]\n",
      "950 [D loss: 0.129286, acc: 96.88%] [G loss: 0.088911, mse: 0.084265]\n",
      "951 [D loss: 0.192608, acc: 93.75%] [G loss: 0.107109, mse: 0.102830]\n",
      "952 [D loss: 0.302289, acc: 90.62%] [G loss: 0.107615, mse: 0.102419]\n",
      "953 [D loss: 0.139675, acc: 96.88%] [G loss: 0.092633, mse: 0.088176]\n",
      "954 [D loss: 0.243566, acc: 90.62%] [G loss: 0.101140, mse: 0.097662]\n",
      "955 [D loss: 0.091044, acc: 98.44%] [G loss: 0.097797, mse: 0.093020]\n",
      "956 [D loss: 0.142754, acc: 93.75%] [G loss: 0.117820, mse: 0.112976]\n",
      "957 [D loss: 0.259884, acc: 84.38%] [G loss: 0.078965, mse: 0.075300]\n",
      "958 [D loss: 0.255574, acc: 87.50%] [G loss: 0.112229, mse: 0.107535]\n",
      "959 [D loss: 0.325878, acc: 87.50%] [G loss: 0.112574, mse: 0.108689]\n",
      "960 [D loss: 0.337012, acc: 89.06%] [G loss: 0.111742, mse: 0.108153]\n",
      "961 [D loss: 0.213205, acc: 90.62%] [G loss: 0.102503, mse: 0.097670]\n",
      "962 [D loss: 0.110808, acc: 95.31%] [G loss: 0.092198, mse: 0.086204]\n",
      "963 [D loss: 0.201869, acc: 92.19%] [G loss: 0.097543, mse: 0.093013]\n",
      "964 [D loss: 0.199751, acc: 93.75%] [G loss: 0.104047, mse: 0.099922]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "965 [D loss: 0.287981, acc: 84.38%] [G loss: 0.104563, mse: 0.100536]\n",
      "966 [D loss: 0.114932, acc: 96.88%] [G loss: 0.094930, mse: 0.090177]\n",
      "967 [D loss: 0.232710, acc: 87.50%] [G loss: 0.097663, mse: 0.094404]\n",
      "968 [D loss: 0.214479, acc: 87.50%] [G loss: 0.086445, mse: 0.082429]\n",
      "969 [D loss: 0.388241, acc: 84.38%] [G loss: 0.111653, mse: 0.107958]\n",
      "970 [D loss: 0.139281, acc: 93.75%] [G loss: 0.083510, mse: 0.079179]\n",
      "971 [D loss: 0.245045, acc: 89.06%] [G loss: 0.091596, mse: 0.088020]\n",
      "972 [D loss: 0.277751, acc: 87.50%] [G loss: 0.103205, mse: 0.098628]\n",
      "973 [D loss: 0.133986, acc: 96.88%] [G loss: 0.118131, mse: 0.113874]\n",
      "974 [D loss: 0.358937, acc: 87.50%] [G loss: 0.100741, mse: 0.096808]\n",
      "975 [D loss: 0.145501, acc: 95.31%] [G loss: 0.109642, mse: 0.104319]\n",
      "976 [D loss: 0.197487, acc: 89.06%] [G loss: 0.097715, mse: 0.093530]\n",
      "977 [D loss: 0.224348, acc: 89.06%] [G loss: 0.107030, mse: 0.102822]\n",
      "978 [D loss: 0.331534, acc: 84.38%] [G loss: 0.104776, mse: 0.101221]\n",
      "979 [D loss: 0.143449, acc: 92.19%] [G loss: 0.108843, mse: 0.104813]\n",
      "980 [D loss: 0.262369, acc: 89.06%] [G loss: 0.101123, mse: 0.097405]\n",
      "981 [D loss: 0.285230, acc: 93.75%] [G loss: 0.099652, mse: 0.096017]\n",
      "982 [D loss: 0.218636, acc: 89.06%] [G loss: 0.117014, mse: 0.113026]\n",
      "983 [D loss: 0.290956, acc: 93.75%] [G loss: 0.110611, mse: 0.106444]\n",
      "984 [D loss: 0.169359, acc: 93.75%] [G loss: 0.099531, mse: 0.094671]\n",
      "985 [D loss: 0.436287, acc: 85.94%] [G loss: 0.094738, mse: 0.090617]\n",
      "986 [D loss: 0.137213, acc: 92.19%] [G loss: 0.089068, mse: 0.084086]\n",
      "987 [D loss: 0.323389, acc: 87.50%] [G loss: 0.098478, mse: 0.093880]\n",
      "988 [D loss: 0.136190, acc: 93.75%] [G loss: 0.099020, mse: 0.094935]\n",
      "989 [D loss: 0.168287, acc: 90.62%] [G loss: 0.091253, mse: 0.087014]\n",
      "990 [D loss: 0.275108, acc: 89.06%] [G loss: 0.088917, mse: 0.084470]\n",
      "991 [D loss: 0.317028, acc: 81.25%] [G loss: 0.101667, mse: 0.097477]\n",
      "992 [D loss: 0.336043, acc: 84.38%] [G loss: 0.090654, mse: 0.086652]\n",
      "993 [D loss: 0.224700, acc: 90.62%] [G loss: 0.094957, mse: 0.091554]\n",
      "994 [D loss: 0.303495, acc: 89.06%] [G loss: 0.095053, mse: 0.090783]\n",
      "995 [D loss: 0.298412, acc: 87.50%] [G loss: 0.103787, mse: 0.099598]\n",
      "996 [D loss: 0.273326, acc: 85.94%] [G loss: 0.102676, mse: 0.099633]\n",
      "997 [D loss: 0.269441, acc: 92.19%] [G loss: 0.105731, mse: 0.102120]\n",
      "998 [D loss: 0.242874, acc: 90.62%] [G loss: 0.102213, mse: 0.097373]\n",
      "999 [D loss: 0.111749, acc: 98.44%] [G loss: 0.087220, mse: 0.083327]\n"
     ]
    }
   ],
   "source": [
    "epochs=1000\n",
    "# epochs=20000\n",
    "train(encoder, decoder, adversarial_autoencoder, D, \n",
    "      latent_dim, epochs=epochs, batch_size=32, sample_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
