{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wasserstein GAN - Improved\n",
    "\n",
    "ref.:\n",
    "GULRAJANI, Ishaan et al.  \n",
    "Improved training of wasserstein gans.  \n",
    "In: Advances in neural information processing systems. 2017. p. 5767-5777.\n",
    "\n",
    "![new objective](wgan_gp_objective.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers.merge import _Merge\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import RMSprop\n",
    "from functools import partial\n",
    "\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWeightedAverage(_Merge):\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "    def _merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((32, 1, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(latent_dim, channels):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=latent_dim))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128, kernel_size=4, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(64, kernel_size=4, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv2D(channels, kernel_size=4, padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "    img = model(noise)\n",
    "\n",
    "    return Model(noise, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_critic(img_shape):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=img_shape, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "    validity = model(img)\n",
    "\n",
    "    return Model(img, validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty_loss(y_true, y_pred, averaged_samples):\n",
    "    \"\"\"\n",
    "    Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "    \"\"\"\n",
    "    gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "    # compute the euclidean norm by squaring ...\n",
    "    gradients_sqr = K.square(gradients)\n",
    "    #   ... summing over the rows ...\n",
    "    gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                              axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "    #   ... and sqrt\n",
    "    gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "    # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "    gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "    # return the mean as loss over all the batch samples\n",
    "    return K.mean(gradient_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_loss(y_true, y_pred):\n",
    "    \"\"\" same loss as in WGAN \"\"\"\n",
    "    return K.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(G, latent_dim, epoch):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
    "    gen_imgs = G.predict(noise)\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(G, critic_model, generator_model, \n",
    "          n_critic, latent_dim,\n",
    "          epochs, batch_size, sample_interval=50):\n",
    "\n",
    "    # Load the dataset\n",
    "    (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "    # Rescale -1 to 1\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "    # Adversarial ground truths\n",
    "    valid = -np.ones((batch_size, 1))\n",
    "    fake =  np.ones((batch_size, 1))\n",
    "    dummy = np.zeros((batch_size, 1)) # Dummy gt for gradient penalty\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for _ in range(n_critic):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "            # Sample generator input\n",
    "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "            # Train the critic\n",
    "            d_loss = critic_model.train_on_batch([imgs, noise],\n",
    "                                                 [valid, fake, dummy])\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "\n",
    "        g_loss = generator_model.train_on_batch(noise, valid)\n",
    "\n",
    "        # Plot the progress\n",
    "        print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, d_loss[0], g_loss))\n",
    "\n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % sample_interval == 0 or epoch == epochs - 1:\n",
    "            sample_images(G, latent_dim, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('images'):\n",
    "    os.makedirs('images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following parameter and optimizer set as recommended in paper\n",
    "n_critic = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# create optimizer\n",
    "optimizer = RMSprop(lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 64)        131136    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 1)         1025      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,028,673\n",
      "Trainable params: 1,028,289\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the generator model\n",
    "G = build_generator(latent_dim, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 16)        160       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 32)          4640      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 100,097\n",
      "Trainable params: 99,649\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the critic model\n",
    "C = build_critic(img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# Construct Computational Graph\n",
    "#       for the Critic\n",
    "#-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze generator's layers while training critic\n",
    "G.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image input (real sample)\n",
    "real_img = Input(shape=img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise input\n",
    "z_disc = Input(shape=(latent_dim,))\n",
    "\n",
    "# Generate image based of noise (fake sample)\n",
    "fake_img = G(z_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator determines validity of the real and fake images\n",
    "fake = C(fake_img)\n",
    "valid = C(real_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct weighted average between real and fake images\n",
    "interpolated_img = RandomWeightedAverage()([real_img, fake_img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine validity of weighted sample\n",
    "validity_interpolated = C(interpolated_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Python partial to provide loss function with additional\n",
    "# 'averaged_samples' argument\n",
    "partial_gp_loss = partial(gradient_penalty_loss,\n",
    "                          averaged_samples=interpolated_img)\n",
    "\n",
    "partial_gp_loss.__name__ = 'gradient_penalty' # Keras requires function names\n",
    "\n",
    "critic_model = Model(inputs=[real_img, z_disc],\n",
    "                     outputs=[valid, fake, validity_interpolated])\n",
    "\n",
    "critic_model.compile(loss=[wasserstein_loss,\n",
    "                           wasserstein_loss,\n",
    "                           partial_gp_loss],\n",
    "                     optimizer=optimizer,\n",
    "                     loss_weights=[1, 1, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# Construct Computational Graph\n",
    "#         for Generator\n",
    "#-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the generator we freeze the critic's layers\n",
    "C.trainable = False\n",
    "G.trainable = True  # this is the default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampled noise for input to generator\n",
    "z_gen = Input(shape=(latent_dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images based of noise\n",
    "img = G(z_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator determines validity\n",
    "valid = C(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines generator model\n",
    "generator_model = Model(z_gen, valid)\n",
    "generator_model.compile(loss=wasserstein_loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 25.070177] [G loss: 0.297224]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 15.671294] [G loss: 0.142512]\n",
      "2 [D loss: 15.337543] [G loss: 0.230281]\n",
      "3 [D loss: 12.432339] [G loss: 0.056703]\n",
      "4 [D loss: 10.851179] [G loss: 0.147348]\n",
      "5 [D loss: 8.952878] [G loss: 0.442589]\n",
      "6 [D loss: 7.261176] [G loss: -0.013684]\n",
      "7 [D loss: 6.972205] [G loss: 0.325016]\n",
      "8 [D loss: 5.035443] [G loss: 0.185003]\n",
      "9 [D loss: 5.259032] [G loss: 0.004149]\n",
      "10 [D loss: 4.227239] [G loss: 0.058557]\n",
      "11 [D loss: 3.524243] [G loss: 0.075562]\n",
      "12 [D loss: 3.562151] [G loss: 0.075373]\n",
      "13 [D loss: 1.219465] [G loss: 0.086477]\n",
      "14 [D loss: 1.879110] [G loss: -0.013041]\n",
      "15 [D loss: 1.456353] [G loss: 0.004118]\n",
      "16 [D loss: 1.266511] [G loss: -0.044474]\n",
      "17 [D loss: 0.833396] [G loss: -0.189196]\n",
      "18 [D loss: 0.918401] [G loss: -0.054217]\n",
      "19 [D loss: 0.098476] [G loss: -0.063081]\n",
      "20 [D loss: 0.282075] [G loss: -0.028315]\n",
      "21 [D loss: 0.161475] [G loss: -0.138403]\n",
      "22 [D loss: -0.262619] [G loss: -0.491620]\n",
      "23 [D loss: -0.266399] [G loss: -0.451789]\n",
      "24 [D loss: -0.206631] [G loss: -0.407562]\n",
      "25 [D loss: -0.554200] [G loss: -0.513265]\n",
      "26 [D loss: -0.001416] [G loss: -0.477432]\n",
      "27 [D loss: -0.386792] [G loss: -0.564311]\n",
      "28 [D loss: -0.389767] [G loss: -0.592039]\n",
      "29 [D loss: -0.535728] [G loss: -0.624239]\n",
      "30 [D loss: -0.732063] [G loss: -0.725365]\n",
      "31 [D loss: -0.777627] [G loss: -0.560101]\n",
      "32 [D loss: -0.686989] [G loss: -0.901833]\n",
      "33 [D loss: -1.106806] [G loss: -0.875886]\n",
      "34 [D loss: -1.073751] [G loss: -0.779835]\n",
      "35 [D loss: -1.119061] [G loss: -0.848128]\n",
      "36 [D loss: -1.001735] [G loss: -0.944344]\n",
      "37 [D loss: -1.090034] [G loss: -1.240726]\n",
      "38 [D loss: -1.390110] [G loss: -0.889298]\n",
      "39 [D loss: -1.142889] [G loss: -1.267280]\n",
      "40 [D loss: -1.069970] [G loss: -1.202893]\n",
      "41 [D loss: -1.696560] [G loss: -1.232475]\n",
      "42 [D loss: -0.817946] [G loss: -1.487631]\n",
      "43 [D loss: -1.109610] [G loss: -1.487747]\n",
      "44 [D loss: -0.902185] [G loss: -1.863937]\n",
      "45 [D loss: -1.042187] [G loss: -1.668095]\n",
      "46 [D loss: -1.067435] [G loss: -1.604807]\n",
      "47 [D loss: -0.986146] [G loss: -1.412992]\n",
      "48 [D loss: -1.279378] [G loss: -1.488737]\n",
      "49 [D loss: -0.766385] [G loss: -1.876685]\n",
      "50 [D loss: -0.876564] [G loss: -1.764488]\n",
      "51 [D loss: -0.981649] [G loss: -2.069877]\n",
      "52 [D loss: -1.021934] [G loss: -2.036800]\n",
      "53 [D loss: -1.408833] [G loss: -1.929405]\n",
      "54 [D loss: -1.061918] [G loss: -2.361048]\n",
      "55 [D loss: -0.741099] [G loss: -2.426511]\n",
      "56 [D loss: -1.278593] [G loss: -2.325201]\n",
      "57 [D loss: -0.558210] [G loss: -2.457148]\n",
      "58 [D loss: -0.520872] [G loss: -2.451554]\n",
      "59 [D loss: -0.920217] [G loss: -2.504654]\n",
      "60 [D loss: -0.946360] [G loss: -2.753081]\n",
      "61 [D loss: -0.320158] [G loss: -2.819373]\n",
      "62 [D loss: -0.816941] [G loss: -2.799486]\n",
      "63 [D loss: -0.747646] [G loss: -2.940184]\n",
      "64 [D loss: -0.399154] [G loss: -3.141222]\n",
      "65 [D loss: -0.812684] [G loss: -2.965510]\n",
      "66 [D loss: -0.385940] [G loss: -3.010318]\n",
      "67 [D loss: -0.498866] [G loss: -2.789148]\n",
      "68 [D loss: -0.406339] [G loss: -3.174393]\n",
      "69 [D loss: -0.295409] [G loss: -2.879430]\n",
      "70 [D loss: -0.076857] [G loss: -3.123940]\n",
      "71 [D loss: -0.517318] [G loss: -2.924675]\n",
      "72 [D loss: 0.073025] [G loss: -3.369738]\n",
      "73 [D loss: -0.100644] [G loss: -3.298262]\n",
      "74 [D loss: -0.760617] [G loss: -3.505100]\n",
      "75 [D loss: -0.331788] [G loss: -3.401697]\n",
      "76 [D loss: -0.837633] [G loss: -2.992279]\n",
      "77 [D loss: -0.716409] [G loss: -3.102402]\n",
      "78 [D loss: -0.534149] [G loss: -3.440134]\n",
      "79 [D loss: -0.454453] [G loss: -3.476396]\n",
      "80 [D loss: -0.441445] [G loss: -3.328845]\n",
      "81 [D loss: -0.537797] [G loss: -3.044704]\n",
      "82 [D loss: -0.417576] [G loss: -3.221294]\n",
      "83 [D loss: -0.676270] [G loss: -3.551393]\n",
      "84 [D loss: -0.466822] [G loss: -3.284372]\n",
      "85 [D loss: -0.227881] [G loss: -3.160799]\n",
      "86 [D loss: -0.497008] [G loss: -3.105584]\n",
      "87 [D loss: -0.294864] [G loss: -3.375686]\n",
      "88 [D loss: -0.251434] [G loss: -3.162066]\n",
      "89 [D loss: -0.656951] [G loss: -3.040349]\n",
      "90 [D loss: 0.006204] [G loss: -3.363696]\n",
      "91 [D loss: -0.522213] [G loss: -3.286977]\n",
      "92 [D loss: -0.470905] [G loss: -2.729548]\n",
      "93 [D loss: -0.570134] [G loss: -2.917490]\n",
      "94 [D loss: -0.287203] [G loss: -3.037196]\n",
      "95 [D loss: -0.472581] [G loss: -2.889572]\n",
      "96 [D loss: -0.670858] [G loss: -2.870620]\n",
      "97 [D loss: -0.573126] [G loss: -2.891487]\n",
      "98 [D loss: -0.297690] [G loss: -2.794147]\n",
      "99 [D loss: -0.080689] [G loss: -2.799009]\n",
      "100 [D loss: -0.179960] [G loss: -2.837023]\n",
      "101 [D loss: -0.234624] [G loss: -2.406060]\n",
      "102 [D loss: -0.317829] [G loss: -2.348829]\n",
      "103 [D loss: -0.458314] [G loss: -2.453546]\n",
      "104 [D loss: -0.457783] [G loss: -2.248569]\n",
      "105 [D loss: -0.483339] [G loss: -2.520058]\n",
      "106 [D loss: -0.578985] [G loss: -2.404913]\n",
      "107 [D loss: -0.516754] [G loss: -2.378740]\n",
      "108 [D loss: -0.264944] [G loss: -2.346348]\n",
      "109 [D loss: -0.205971] [G loss: -2.278308]\n",
      "110 [D loss: -0.187744] [G loss: -2.469878]\n",
      "111 [D loss: -0.394332] [G loss: -2.024429]\n",
      "112 [D loss: -0.768547] [G loss: -1.979805]\n",
      "113 [D loss: -0.224861] [G loss: -1.985562]\n",
      "114 [D loss: -0.404910] [G loss: -1.862610]\n",
      "115 [D loss: -0.300151] [G loss: -1.775937]\n",
      "116 [D loss: -0.570693] [G loss: -2.111527]\n",
      "117 [D loss: -0.502947] [G loss: -1.839322]\n",
      "118 [D loss: -0.823071] [G loss: -1.443584]\n",
      "119 [D loss: -0.553467] [G loss: -1.597898]\n",
      "120 [D loss: -0.547142] [G loss: -1.742411]\n",
      "121 [D loss: -0.654853] [G loss: -1.700079]\n",
      "122 [D loss: -0.503456] [G loss: -1.666211]\n",
      "123 [D loss: -0.646529] [G loss: -1.463672]\n",
      "124 [D loss: -0.361847] [G loss: -1.402239]\n",
      "125 [D loss: -0.857639] [G loss: -1.618027]\n",
      "126 [D loss: -0.765662] [G loss: -1.279636]\n",
      "127 [D loss: -0.571169] [G loss: -1.288763]\n",
      "128 [D loss: -0.867947] [G loss: -1.475717]\n",
      "129 [D loss: -0.232170] [G loss: -1.358266]\n",
      "130 [D loss: -0.226698] [G loss: -1.325804]\n",
      "131 [D loss: -0.394060] [G loss: -1.381486]\n",
      "132 [D loss: -0.695428] [G loss: -1.152539]\n",
      "133 [D loss: -0.368854] [G loss: -1.596550]\n",
      "134 [D loss: -0.286806] [G loss: -1.215351]\n",
      "135 [D loss: -0.698393] [G loss: -1.068768]\n",
      "136 [D loss: 0.052721] [G loss: -1.044202]\n",
      "137 [D loss: -0.288236] [G loss: -1.110865]\n",
      "138 [D loss: -0.717860] [G loss: -1.341444]\n",
      "139 [D loss: -0.527562] [G loss: -1.278962]\n",
      "140 [D loss: -0.061323] [G loss: -1.339512]\n",
      "141 [D loss: -0.307274] [G loss: -1.198379]\n",
      "142 [D loss: -0.631974] [G loss: -1.411343]\n",
      "143 [D loss: -0.716174] [G loss: -1.154721]\n",
      "144 [D loss: -0.798963] [G loss: -1.162143]\n",
      "145 [D loss: -0.738029] [G loss: -1.194636]\n",
      "146 [D loss: -0.553283] [G loss: -1.526354]\n",
      "147 [D loss: -0.788101] [G loss: -1.333618]\n",
      "148 [D loss: -0.328646] [G loss: -1.276018]\n",
      "149 [D loss: -0.057140] [G loss: -1.564731]\n",
      "150 [D loss: -0.458648] [G loss: -1.436762]\n",
      "151 [D loss: -0.334786] [G loss: -1.328761]\n",
      "152 [D loss: -0.559190] [G loss: -1.224076]\n",
      "153 [D loss: -0.702486] [G loss: -1.284237]\n",
      "154 [D loss: -0.289296] [G loss: -1.004591]\n",
      "155 [D loss: -0.627247] [G loss: -1.317678]\n",
      "156 [D loss: -0.670819] [G loss: -1.526887]\n",
      "157 [D loss: -0.402191] [G loss: -1.479605]\n",
      "158 [D loss: -0.393368] [G loss: -1.306712]\n",
      "159 [D loss: -0.416412] [G loss: -1.634306]\n",
      "160 [D loss: -0.485812] [G loss: -1.819059]\n",
      "161 [D loss: -1.011636] [G loss: -1.858888]\n",
      "162 [D loss: -0.138104] [G loss: -1.585411]\n",
      "163 [D loss: -0.374079] [G loss: -1.690820]\n",
      "164 [D loss: -0.673381] [G loss: -1.790972]\n",
      "165 [D loss: -0.434256] [G loss: -1.750434]\n",
      "166 [D loss: -0.479558] [G loss: -1.918026]\n",
      "167 [D loss: -0.145071] [G loss: -1.819326]\n",
      "168 [D loss: -0.409559] [G loss: -1.901745]\n",
      "169 [D loss: -0.426218] [G loss: -1.786627]\n",
      "170 [D loss: -0.533897] [G loss: -1.853959]\n",
      "171 [D loss: -0.521429] [G loss: -2.007041]\n",
      "172 [D loss: -0.662314] [G loss: -1.810603]\n",
      "173 [D loss: -0.623940] [G loss: -2.000686]\n",
      "174 [D loss: -0.448554] [G loss: -1.901991]\n",
      "175 [D loss: -0.184319] [G loss: -2.183244]\n",
      "176 [D loss: -0.716309] [G loss: -1.895703]\n",
      "177 [D loss: -0.782236] [G loss: -2.251605]\n",
      "178 [D loss: 0.063282] [G loss: -2.250019]\n",
      "179 [D loss: -0.091538] [G loss: -2.013003]\n",
      "180 [D loss: -0.857296] [G loss: -2.087917]\n",
      "181 [D loss: -0.508678] [G loss: -2.169069]\n",
      "182 [D loss: -0.120650] [G loss: -2.243894]\n",
      "183 [D loss: -0.530328] [G loss: -2.273810]\n",
      "184 [D loss: -0.326380] [G loss: -2.340534]\n",
      "185 [D loss: -0.270462] [G loss: -2.345347]\n",
      "186 [D loss: -0.454020] [G loss: -2.349087]\n",
      "187 [D loss: -0.335371] [G loss: -2.275207]\n",
      "188 [D loss: -0.425433] [G loss: -2.650778]\n",
      "189 [D loss: -0.436790] [G loss: -2.351426]\n",
      "190 [D loss: -0.495283] [G loss: -2.552995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191 [D loss: -1.064137] [G loss: -2.537212]\n",
      "192 [D loss: -0.683361] [G loss: -2.275415]\n",
      "193 [D loss: -0.389910] [G loss: -2.385437]\n",
      "194 [D loss: -0.748865] [G loss: -2.279803]\n",
      "195 [D loss: -0.814931] [G loss: -2.298585]\n",
      "196 [D loss: -0.349325] [G loss: -2.511345]\n",
      "197 [D loss: -0.440804] [G loss: -2.617544]\n",
      "198 [D loss: -0.169316] [G loss: -2.125013]\n",
      "199 [D loss: -0.246127] [G loss: -2.563831]\n",
      "200 [D loss: -0.301772] [G loss: -2.467508]\n",
      "201 [D loss: -0.017004] [G loss: -2.208903]\n",
      "202 [D loss: -0.580021] [G loss: -2.741154]\n",
      "203 [D loss: -0.481001] [G loss: -2.209312]\n",
      "204 [D loss: -0.145891] [G loss: -2.632309]\n",
      "205 [D loss: -0.099586] [G loss: -2.275704]\n",
      "206 [D loss: -0.447367] [G loss: -2.638948]\n",
      "207 [D loss: -0.253114] [G loss: -2.339246]\n",
      "208 [D loss: 0.010422] [G loss: -2.480601]\n",
      "209 [D loss: -0.445769] [G loss: -2.459414]\n",
      "210 [D loss: -0.323106] [G loss: -2.534847]\n",
      "211 [D loss: -0.299835] [G loss: -2.672637]\n",
      "212 [D loss: -0.196829] [G loss: -2.628883]\n",
      "213 [D loss: -0.516484] [G loss: -2.755405]\n",
      "214 [D loss: -0.312235] [G loss: -2.353329]\n",
      "215 [D loss: -0.379911] [G loss: -2.637338]\n",
      "216 [D loss: -0.388904] [G loss: -2.518440]\n",
      "217 [D loss: -0.365346] [G loss: -2.408177]\n",
      "218 [D loss: -0.509035] [G loss: -2.530439]\n",
      "219 [D loss: -0.646523] [G loss: -3.026291]\n",
      "220 [D loss: -0.335135] [G loss: -2.687396]\n",
      "221 [D loss: 0.162689] [G loss: -2.794325]\n",
      "222 [D loss: -0.667179] [G loss: -2.891233]\n",
      "223 [D loss: -0.558202] [G loss: -2.448424]\n",
      "224 [D loss: -0.603871] [G loss: -2.797119]\n",
      "225 [D loss: -0.402640] [G loss: -2.541808]\n",
      "226 [D loss: -0.512524] [G loss: -2.633786]\n",
      "227 [D loss: -0.814924] [G loss: -2.712926]\n",
      "228 [D loss: -0.391670] [G loss: -2.910740]\n",
      "229 [D loss: 0.074299] [G loss: -2.816163]\n",
      "230 [D loss: -0.086976] [G loss: -2.577815]\n",
      "231 [D loss: -0.154412] [G loss: -2.697284]\n",
      "232 [D loss: -0.467240] [G loss: -2.421170]\n",
      "233 [D loss: -0.816361] [G loss: -2.400041]\n",
      "234 [D loss: -0.419281] [G loss: -2.651496]\n",
      "235 [D loss: -0.641996] [G loss: -2.392639]\n",
      "236 [D loss: -0.177209] [G loss: -2.406008]\n",
      "237 [D loss: -0.015112] [G loss: -2.795547]\n",
      "238 [D loss: 0.347550] [G loss: -2.782946]\n",
      "239 [D loss: -0.325163] [G loss: -2.720174]\n",
      "240 [D loss: -0.134486] [G loss: -2.568514]\n",
      "241 [D loss: -0.211495] [G loss: -2.324424]\n",
      "242 [D loss: -0.280538] [G loss: -2.668126]\n",
      "243 [D loss: -0.275861] [G loss: -2.362526]\n",
      "244 [D loss: -0.578141] [G loss: -2.524907]\n",
      "245 [D loss: -0.176826] [G loss: -2.627517]\n",
      "246 [D loss: -0.528135] [G loss: -2.782127]\n",
      "247 [D loss: -0.499706] [G loss: -2.562051]\n",
      "248 [D loss: -0.181750] [G loss: -2.601048]\n",
      "249 [D loss: -0.290674] [G loss: -2.632999]\n",
      "250 [D loss: -0.483667] [G loss: -2.575986]\n",
      "251 [D loss: -0.491698] [G loss: -2.295223]\n",
      "252 [D loss: -0.458506] [G loss: -2.454367]\n",
      "253 [D loss: -0.852619] [G loss: -2.203713]\n",
      "254 [D loss: -0.596235] [G loss: -2.496985]\n",
      "255 [D loss: -0.389389] [G loss: -2.522251]\n",
      "256 [D loss: -0.266166] [G loss: -2.618252]\n",
      "257 [D loss: -0.439125] [G loss: -2.411325]\n",
      "258 [D loss: -0.067914] [G loss: -2.397615]\n",
      "259 [D loss: -0.051954] [G loss: -2.621198]\n",
      "260 [D loss: -0.302614] [G loss: -2.551078]\n",
      "261 [D loss: -0.572335] [G loss: -2.428612]\n",
      "262 [D loss: -0.281748] [G loss: -2.463456]\n",
      "263 [D loss: -0.259922] [G loss: -2.532809]\n",
      "264 [D loss: 0.029494] [G loss: -2.515253]\n",
      "265 [D loss: -0.267557] [G loss: -2.434808]\n",
      "266 [D loss: -0.214953] [G loss: -2.613297]\n",
      "267 [D loss: -0.026991] [G loss: -2.984880]\n",
      "268 [D loss: -0.320121] [G loss: -2.452468]\n",
      "269 [D loss: -0.261370] [G loss: -2.481351]\n",
      "270 [D loss: -0.111971] [G loss: -2.514941]\n",
      "271 [D loss: -0.510981] [G loss: -2.779427]\n",
      "272 [D loss: -0.637898] [G loss: -2.867948]\n",
      "273 [D loss: -0.371503] [G loss: -2.341904]\n",
      "274 [D loss: -0.351050] [G loss: -2.776648]\n",
      "275 [D loss: -0.437356] [G loss: -2.658059]\n",
      "276 [D loss: -0.166677] [G loss: -3.068024]\n",
      "277 [D loss: -0.220823] [G loss: -2.854202]\n",
      "278 [D loss: -0.066758] [G loss: -2.666360]\n",
      "279 [D loss: -0.293018] [G loss: -2.952440]\n",
      "280 [D loss: -0.517417] [G loss: -2.690261]\n",
      "281 [D loss: -0.706879] [G loss: -2.425940]\n",
      "282 [D loss: -0.173058] [G loss: -2.391267]\n",
      "283 [D loss: -0.514803] [G loss: -2.768528]\n",
      "284 [D loss: -0.594342] [G loss: -2.810609]\n",
      "285 [D loss: -0.465619] [G loss: -2.494701]\n",
      "286 [D loss: -0.260243] [G loss: -2.537942]\n",
      "287 [D loss: -0.204181] [G loss: -2.536787]\n",
      "288 [D loss: 0.044186] [G loss: -2.484985]\n",
      "289 [D loss: -0.137538] [G loss: -2.453605]\n",
      "290 [D loss: -0.164763] [G loss: -2.588247]\n",
      "291 [D loss: -0.186079] [G loss: -2.365888]\n",
      "292 [D loss: -0.476786] [G loss: -2.488451]\n",
      "293 [D loss: -0.636217] [G loss: -2.558306]\n",
      "294 [D loss: -0.431369] [G loss: -2.562963]\n",
      "295 [D loss: -0.079625] [G loss: -2.479422]\n",
      "296 [D loss: -0.422525] [G loss: -2.309701]\n",
      "297 [D loss: -0.359982] [G loss: -2.441264]\n",
      "298 [D loss: -0.140592] [G loss: -2.407581]\n",
      "299 [D loss: -0.637132] [G loss: -2.496150]\n",
      "300 [D loss: -0.208556] [G loss: -2.355052]\n",
      "301 [D loss: 0.207780] [G loss: -2.465309]\n",
      "302 [D loss: -0.449433] [G loss: -2.347197]\n",
      "303 [D loss: -0.529387] [G loss: -2.680422]\n",
      "304 [D loss: -0.174876] [G loss: -2.800395]\n",
      "305 [D loss: -0.549443] [G loss: -2.695447]\n",
      "306 [D loss: -0.151882] [G loss: -2.634629]\n",
      "307 [D loss: -0.129674] [G loss: -2.411424]\n",
      "308 [D loss: -0.120551] [G loss: -2.436934]\n",
      "309 [D loss: -0.290061] [G loss: -2.237526]\n",
      "310 [D loss: -0.279445] [G loss: -2.640412]\n",
      "311 [D loss: -0.636075] [G loss: -2.545136]\n",
      "312 [D loss: -0.560919] [G loss: -2.299874]\n",
      "313 [D loss: -0.210407] [G loss: -2.211755]\n",
      "314 [D loss: -0.345700] [G loss: -2.619282]\n",
      "315 [D loss: -0.445524] [G loss: -2.251350]\n",
      "316 [D loss: -0.233877] [G loss: -2.252349]\n",
      "317 [D loss: -0.740414] [G loss: -2.306063]\n",
      "318 [D loss: 0.046839] [G loss: -1.950159]\n",
      "319 [D loss: -0.202077] [G loss: -2.431832]\n",
      "320 [D loss: 0.055448] [G loss: -2.429212]\n",
      "321 [D loss: -0.143956] [G loss: -2.356472]\n",
      "322 [D loss: 0.082091] [G loss: -2.221801]\n",
      "323 [D loss: -0.359641] [G loss: -2.532777]\n",
      "324 [D loss: -0.177808] [G loss: -2.266760]\n",
      "325 [D loss: -0.673618] [G loss: -2.394892]\n",
      "326 [D loss: 0.009377] [G loss: -2.186224]\n",
      "327 [D loss: -0.752402] [G loss: -2.501909]\n",
      "328 [D loss: -0.324843] [G loss: -2.436224]\n",
      "329 [D loss: -0.489367] [G loss: -2.410019]\n",
      "330 [D loss: 0.175787] [G loss: -2.008576]\n",
      "331 [D loss: -0.199077] [G loss: -2.471766]\n",
      "332 [D loss: -0.472989] [G loss: -2.192517]\n",
      "333 [D loss: -0.404040] [G loss: -2.508022]\n",
      "334 [D loss: 0.009155] [G loss: -2.426872]\n",
      "335 [D loss: -0.597510] [G loss: -2.287604]\n",
      "336 [D loss: -0.523226] [G loss: -2.479235]\n",
      "337 [D loss: -0.434259] [G loss: -2.192521]\n",
      "338 [D loss: -0.609482] [G loss: -2.082816]\n",
      "339 [D loss: -0.062543] [G loss: -2.452321]\n",
      "340 [D loss: -0.668449] [G loss: -2.235391]\n",
      "341 [D loss: -0.288798] [G loss: -2.647621]\n",
      "342 [D loss: -0.887442] [G loss: -2.150807]\n",
      "343 [D loss: -0.730696] [G loss: -2.244064]\n",
      "344 [D loss: -0.573219] [G loss: -2.375361]\n",
      "345 [D loss: -0.439862] [G loss: -2.209304]\n",
      "346 [D loss: -0.604786] [G loss: -2.376744]\n",
      "347 [D loss: -0.787948] [G loss: -2.451829]\n",
      "348 [D loss: -0.190740] [G loss: -2.773957]\n",
      "349 [D loss: -0.189306] [G loss: -2.517705]\n",
      "350 [D loss: -0.667923] [G loss: -2.456203]\n",
      "351 [D loss: -0.491056] [G loss: -2.793021]\n",
      "352 [D loss: -0.319406] [G loss: -2.844542]\n",
      "353 [D loss: -0.200405] [G loss: -2.083082]\n",
      "354 [D loss: -0.169106] [G loss: -2.566899]\n",
      "355 [D loss: -0.606699] [G loss: -2.455972]\n",
      "356 [D loss: -0.185588] [G loss: -2.816846]\n",
      "357 [D loss: -0.499451] [G loss: -2.668290]\n",
      "358 [D loss: -0.313206] [G loss: -2.923193]\n",
      "359 [D loss: -0.464300] [G loss: -2.493393]\n",
      "360 [D loss: -0.733250] [G loss: -2.432061]\n",
      "361 [D loss: -0.129094] [G loss: -2.440331]\n",
      "362 [D loss: -0.582158] [G loss: -2.447736]\n",
      "363 [D loss: 0.076015] [G loss: -2.521101]\n",
      "364 [D loss: -0.174851] [G loss: -1.966697]\n",
      "365 [D loss: -0.495178] [G loss: -2.412304]\n",
      "366 [D loss: -0.484644] [G loss: -2.247888]\n",
      "367 [D loss: -0.461326] [G loss: -2.333626]\n",
      "368 [D loss: -0.808783] [G loss: -2.685942]\n",
      "369 [D loss: -0.476777] [G loss: -2.570402]\n",
      "370 [D loss: 0.191143] [G loss: -2.257330]\n",
      "371 [D loss: -0.107227] [G loss: -2.653156]\n",
      "372 [D loss: -0.013545] [G loss: -2.597404]\n",
      "373 [D loss: -0.267520] [G loss: -2.628840]\n",
      "374 [D loss: 0.022912] [G loss: -2.626578]\n",
      "375 [D loss: -0.213177] [G loss: -2.954788]\n",
      "376 [D loss: -0.401645] [G loss: -2.709180]\n",
      "377 [D loss: -0.113205] [G loss: -2.703230]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378 [D loss: -0.116739] [G loss: -2.648970]\n",
      "379 [D loss: 0.010750] [G loss: -2.589654]\n",
      "380 [D loss: -0.441586] [G loss: -2.460656]\n",
      "381 [D loss: -0.206813] [G loss: -2.441152]\n",
      "382 [D loss: 0.281966] [G loss: -2.474371]\n",
      "383 [D loss: -0.133488] [G loss: -2.311149]\n",
      "384 [D loss: -0.292996] [G loss: -2.403698]\n",
      "385 [D loss: -0.067222] [G loss: -2.365760]\n",
      "386 [D loss: -0.745061] [G loss: -2.979283]\n",
      "387 [D loss: -0.386559] [G loss: -2.123550]\n",
      "388 [D loss: -0.137915] [G loss: -2.242775]\n",
      "389 [D loss: 0.084619] [G loss: -2.664877]\n",
      "390 [D loss: -0.497101] [G loss: -2.509724]\n",
      "391 [D loss: -0.264946] [G loss: -2.902325]\n",
      "392 [D loss: -0.116550] [G loss: -2.418616]\n",
      "393 [D loss: -0.295383] [G loss: -2.787032]\n",
      "394 [D loss: -0.089682] [G loss: -2.645774]\n",
      "395 [D loss: -0.830465] [G loss: -2.395820]\n",
      "396 [D loss: -0.566966] [G loss: -2.665959]\n",
      "397 [D loss: -0.239933] [G loss: -2.603440]\n",
      "398 [D loss: -0.456670] [G loss: -2.405239]\n",
      "399 [D loss: -0.190577] [G loss: -2.686404]\n",
      "400 [D loss: -0.003322] [G loss: -2.455379]\n",
      "401 [D loss: -0.075840] [G loss: -2.762372]\n",
      "402 [D loss: -0.206128] [G loss: -2.386058]\n",
      "403 [D loss: -0.853710] [G loss: -2.378311]\n",
      "404 [D loss: -0.712518] [G loss: -2.421793]\n",
      "405 [D loss: -0.374451] [G loss: -2.639544]\n",
      "406 [D loss: -0.684705] [G loss: -2.153640]\n",
      "407 [D loss: -0.412314] [G loss: -2.563233]\n",
      "408 [D loss: -0.268434] [G loss: -2.653087]\n",
      "409 [D loss: -0.573119] [G loss: -2.408574]\n",
      "410 [D loss: -0.572019] [G loss: -2.175737]\n",
      "411 [D loss: -0.157346] [G loss: -2.353704]\n",
      "412 [D loss: -0.307131] [G loss: -2.345829]\n",
      "413 [D loss: -0.473763] [G loss: -2.485163]\n",
      "414 [D loss: -0.779488] [G loss: -2.631677]\n",
      "415 [D loss: 0.268739] [G loss: -2.549807]\n",
      "416 [D loss: -0.254608] [G loss: -2.681949]\n",
      "417 [D loss: -0.558169] [G loss: -2.124967]\n",
      "418 [D loss: -0.087704] [G loss: -2.337255]\n",
      "419 [D loss: -0.508292] [G loss: -2.296106]\n",
      "420 [D loss: -0.621380] [G loss: -2.381128]\n",
      "421 [D loss: -0.251295] [G loss: -2.431620]\n",
      "422 [D loss: -0.355559] [G loss: -2.291874]\n",
      "423 [D loss: -0.179096] [G loss: -2.468351]\n",
      "424 [D loss: -0.554157] [G loss: -2.711102]\n",
      "425 [D loss: -0.535445] [G loss: -2.723848]\n",
      "426 [D loss: 0.046725] [G loss: -2.469429]\n",
      "427 [D loss: -0.365869] [G loss: -2.162361]\n",
      "428 [D loss: 0.178482] [G loss: -2.321298]\n",
      "429 [D loss: -0.057934] [G loss: -2.634436]\n",
      "430 [D loss: 0.102681] [G loss: -2.419369]\n",
      "431 [D loss: -0.748099] [G loss: -2.380884]\n",
      "432 [D loss: -0.165086] [G loss: -2.398851]\n",
      "433 [D loss: -0.279540] [G loss: -2.424742]\n",
      "434 [D loss: -0.392615] [G loss: -2.224432]\n",
      "435 [D loss: -0.159476] [G loss: -2.366092]\n",
      "436 [D loss: -0.806162] [G loss: -2.319288]\n",
      "437 [D loss: -0.369369] [G loss: -2.296081]\n",
      "438 [D loss: -0.093631] [G loss: -2.340348]\n",
      "439 [D loss: 0.042619] [G loss: -2.346961]\n",
      "440 [D loss: 0.340262] [G loss: -2.347726]\n",
      "441 [D loss: -0.596939] [G loss: -2.327582]\n",
      "442 [D loss: -0.128934] [G loss: -2.221129]\n",
      "443 [D loss: -0.121632] [G loss: -2.410930]\n",
      "444 [D loss: -0.351338] [G loss: -2.218235]\n",
      "445 [D loss: -0.201676] [G loss: -2.384966]\n",
      "446 [D loss: -0.371342] [G loss: -2.112830]\n",
      "447 [D loss: -0.345676] [G loss: -2.336365]\n",
      "448 [D loss: 0.090232] [G loss: -2.325203]\n",
      "449 [D loss: -0.186690] [G loss: -2.413408]\n",
      "450 [D loss: -0.188274] [G loss: -2.290910]\n",
      "451 [D loss: -0.517535] [G loss: -2.268010]\n",
      "452 [D loss: -0.018489] [G loss: -2.537853]\n",
      "453 [D loss: -0.301761] [G loss: -2.544727]\n",
      "454 [D loss: -0.487592] [G loss: -2.317253]\n",
      "455 [D loss: 0.050615] [G loss: -2.219612]\n",
      "456 [D loss: -0.669119] [G loss: -2.145608]\n",
      "457 [D loss: 0.054492] [G loss: -2.352133]\n",
      "458 [D loss: -0.753118] [G loss: -2.411031]\n",
      "459 [D loss: -0.555108] [G loss: -2.203277]\n",
      "460 [D loss: 0.003084] [G loss: -2.641943]\n",
      "461 [D loss: -0.415881] [G loss: -1.987258]\n",
      "462 [D loss: -0.779263] [G loss: -2.272432]\n",
      "463 [D loss: -0.723992] [G loss: -2.308521]\n",
      "464 [D loss: -0.338395] [G loss: -1.954860]\n",
      "465 [D loss: -0.434838] [G loss: -2.193465]\n",
      "466 [D loss: -0.383266] [G loss: -2.209495]\n",
      "467 [D loss: -0.643489] [G loss: -2.129931]\n",
      "468 [D loss: -0.320380] [G loss: -2.705925]\n",
      "469 [D loss: -0.582980] [G loss: -2.232827]\n",
      "470 [D loss: -0.091193] [G loss: -2.014848]\n",
      "471 [D loss: -0.090299] [G loss: -2.233388]\n",
      "472 [D loss: 0.027497] [G loss: -2.118137]\n",
      "473 [D loss: -0.334949] [G loss: -2.162088]\n",
      "474 [D loss: -0.384646] [G loss: -2.483858]\n",
      "475 [D loss: -0.232688] [G loss: -2.379861]\n",
      "476 [D loss: -0.569875] [G loss: -2.168278]\n",
      "477 [D loss: -0.143577] [G loss: -2.541864]\n",
      "478 [D loss: -0.512247] [G loss: -2.397491]\n",
      "479 [D loss: -0.135585] [G loss: -2.546228]\n",
      "480 [D loss: -0.352344] [G loss: -2.201429]\n",
      "481 [D loss: -0.534598] [G loss: -2.457533]\n",
      "482 [D loss: -0.308347] [G loss: -2.180817]\n",
      "483 [D loss: -0.073443] [G loss: -2.229320]\n",
      "484 [D loss: -0.513484] [G loss: -2.465288]\n",
      "485 [D loss: -0.371033] [G loss: -2.366711]\n",
      "486 [D loss: -0.146963] [G loss: -1.883494]\n",
      "487 [D loss: 0.077963] [G loss: -2.707411]\n",
      "488 [D loss: -0.491954] [G loss: -1.885487]\n",
      "489 [D loss: -0.411208] [G loss: -2.146812]\n",
      "490 [D loss: -0.421607] [G loss: -2.357064]\n",
      "491 [D loss: -0.876108] [G loss: -2.360370]\n",
      "492 [D loss: -0.303762] [G loss: -2.347609]\n",
      "493 [D loss: -0.328149] [G loss: -2.602428]\n",
      "494 [D loss: -0.139785] [G loss: -2.406623]\n",
      "495 [D loss: -0.323465] [G loss: -2.489806]\n",
      "496 [D loss: -0.449870] [G loss: -2.361477]\n",
      "497 [D loss: -0.549906] [G loss: -2.418204]\n",
      "498 [D loss: 0.182754] [G loss: -2.773451]\n",
      "499 [D loss: 0.030177] [G loss: -2.559495]\n",
      "500 [D loss: 0.043289] [G loss: -2.447811]\n",
      "501 [D loss: -0.242749] [G loss: -2.721850]\n",
      "502 [D loss: -0.168230] [G loss: -2.537547]\n",
      "503 [D loss: 0.256513] [G loss: -2.309675]\n",
      "504 [D loss: -0.185485] [G loss: -2.606997]\n",
      "505 [D loss: -0.484103] [G loss: -2.834137]\n",
      "506 [D loss: -0.592959] [G loss: -2.091607]\n",
      "507 [D loss: 0.174656] [G loss: -2.708349]\n",
      "508 [D loss: -0.576789] [G loss: -2.198084]\n",
      "509 [D loss: -0.151978] [G loss: -2.515469]\n",
      "510 [D loss: -0.147830] [G loss: -2.572383]\n",
      "511 [D loss: -0.101286] [G loss: -2.363192]\n",
      "512 [D loss: 0.140668] [G loss: -2.445312]\n",
      "513 [D loss: -0.005996] [G loss: -2.594408]\n",
      "514 [D loss: -0.324733] [G loss: -2.355565]\n",
      "515 [D loss: -0.559352] [G loss: -2.311719]\n",
      "516 [D loss: -0.342778] [G loss: -2.248770]\n",
      "517 [D loss: -0.584387] [G loss: -2.399864]\n",
      "518 [D loss: -0.418709] [G loss: -2.305251]\n",
      "519 [D loss: -0.498982] [G loss: -2.410140]\n",
      "520 [D loss: -0.649798] [G loss: -2.657772]\n",
      "521 [D loss: -0.394431] [G loss: -2.389595]\n",
      "522 [D loss: 0.093455] [G loss: -2.274101]\n",
      "523 [D loss: -0.151875] [G loss: -2.321538]\n",
      "524 [D loss: -0.148529] [G loss: -2.236037]\n",
      "525 [D loss: 0.226708] [G loss: -2.546994]\n",
      "526 [D loss: -0.215802] [G loss: -2.329872]\n",
      "527 [D loss: -0.266470] [G loss: -2.752576]\n",
      "528 [D loss: -0.244900] [G loss: -2.314938]\n",
      "529 [D loss: -0.347359] [G loss: -2.093730]\n",
      "530 [D loss: -0.180106] [G loss: -2.376781]\n",
      "531 [D loss: -0.343178] [G loss: -2.552672]\n",
      "532 [D loss: -0.300916] [G loss: -2.302565]\n",
      "533 [D loss: -0.347982] [G loss: -2.425853]\n",
      "534 [D loss: -0.420667] [G loss: -2.283962]\n",
      "535 [D loss: -0.363147] [G loss: -2.390574]\n",
      "536 [D loss: -0.476968] [G loss: -2.375574]\n",
      "537 [D loss: -0.600150] [G loss: -2.203534]\n",
      "538 [D loss: -0.592321] [G loss: -2.413843]\n",
      "539 [D loss: -0.136438] [G loss: -2.281010]\n",
      "540 [D loss: -0.231703] [G loss: -1.813926]\n",
      "541 [D loss: -0.148042] [G loss: -2.328218]\n",
      "542 [D loss: -0.231462] [G loss: -2.142418]\n",
      "543 [D loss: -0.768018] [G loss: -2.161242]\n",
      "544 [D loss: -0.173870] [G loss: -2.215563]\n",
      "545 [D loss: 0.143123] [G loss: -2.123618]\n",
      "546 [D loss: -0.234163] [G loss: -2.297900]\n",
      "547 [D loss: -0.680588] [G loss: -2.004767]\n",
      "548 [D loss: -0.371547] [G loss: -2.096025]\n",
      "549 [D loss: -0.440270] [G loss: -1.867742]\n",
      "550 [D loss: -0.613496] [G loss: -2.199012]\n",
      "551 [D loss: -0.437606] [G loss: -1.993232]\n",
      "552 [D loss: -0.177173] [G loss: -1.848742]\n",
      "553 [D loss: -0.786662] [G loss: -2.490176]\n",
      "554 [D loss: -0.296117] [G loss: -2.152165]\n",
      "555 [D loss: -0.660129] [G loss: -1.887829]\n",
      "556 [D loss: -0.232382] [G loss: -2.111055]\n",
      "557 [D loss: -0.404912] [G loss: -1.755117]\n",
      "558 [D loss: -0.026882] [G loss: -1.789160]\n",
      "559 [D loss: -0.324744] [G loss: -1.867825]\n",
      "560 [D loss: -0.189564] [G loss: -2.327291]\n",
      "561 [D loss: -0.305685] [G loss: -1.870328]\n",
      "562 [D loss: -0.332323] [G loss: -1.935148]\n",
      "563 [D loss: -0.463751] [G loss: -1.873696]\n",
      "564 [D loss: -0.130171] [G loss: -1.814326]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565 [D loss: -0.297545] [G loss: -2.039172]\n",
      "566 [D loss: -0.113412] [G loss: -2.333783]\n",
      "567 [D loss: -0.248245] [G loss: -1.928136]\n",
      "568 [D loss: -0.206390] [G loss: -1.970325]\n",
      "569 [D loss: -0.012758] [G loss: -1.821019]\n",
      "570 [D loss: -0.846002] [G loss: -2.105019]\n",
      "571 [D loss: -0.063511] [G loss: -1.975502]\n",
      "572 [D loss: -0.610505] [G loss: -1.660715]\n",
      "573 [D loss: -0.236640] [G loss: -1.977567]\n",
      "574 [D loss: -0.347666] [G loss: -1.988389]\n",
      "575 [D loss: 0.019874] [G loss: -1.950062]\n",
      "576 [D loss: -0.383859] [G loss: -1.820282]\n",
      "577 [D loss: -0.378650] [G loss: -1.856554]\n",
      "578 [D loss: 0.213223] [G loss: -1.944825]\n",
      "579 [D loss: -0.539018] [G loss: -2.008958]\n",
      "580 [D loss: -0.522736] [G loss: -2.102289]\n",
      "581 [D loss: -0.426613] [G loss: -2.002568]\n",
      "582 [D loss: -0.589562] [G loss: -2.386572]\n",
      "583 [D loss: 0.024214] [G loss: -2.328669]\n",
      "584 [D loss: -0.349392] [G loss: -2.131880]\n",
      "585 [D loss: 0.075837] [G loss: -2.163935]\n",
      "586 [D loss: -0.381199] [G loss: -1.793773]\n",
      "587 [D loss: -0.307189] [G loss: -1.780457]\n",
      "588 [D loss: -0.170137] [G loss: -1.972926]\n",
      "589 [D loss: 0.034932] [G loss: -2.107042]\n",
      "590 [D loss: -0.450399] [G loss: -1.781068]\n",
      "591 [D loss: -0.561779] [G loss: -1.953483]\n",
      "592 [D loss: -0.361570] [G loss: -2.144330]\n",
      "593 [D loss: -0.424587] [G loss: -2.034845]\n",
      "594 [D loss: -0.344022] [G loss: -1.874337]\n",
      "595 [D loss: -0.266775] [G loss: -1.983540]\n",
      "596 [D loss: -0.609142] [G loss: -2.129534]\n",
      "597 [D loss: -0.074352] [G loss: -2.198777]\n",
      "598 [D loss: -0.023144] [G loss: -1.748929]\n",
      "599 [D loss: -0.326569] [G loss: -2.268968]\n",
      "600 [D loss: 0.191470] [G loss: -1.938358]\n",
      "601 [D loss: -0.583656] [G loss: -1.944908]\n",
      "602 [D loss: -0.272180] [G loss: -2.018301]\n",
      "603 [D loss: -0.382443] [G loss: -1.915183]\n",
      "604 [D loss: -0.186430] [G loss: -1.965167]\n",
      "605 [D loss: 0.079487] [G loss: -1.826574]\n",
      "606 [D loss: -0.219464] [G loss: -1.951196]\n",
      "607 [D loss: -0.255939] [G loss: -1.502327]\n",
      "608 [D loss: -0.123950] [G loss: -1.960918]\n",
      "609 [D loss: -0.597806] [G loss: -2.022983]\n",
      "610 [D loss: 0.221239] [G loss: -1.899321]\n",
      "611 [D loss: -0.229716] [G loss: -1.584244]\n",
      "612 [D loss: -0.064183] [G loss: -1.883204]\n",
      "613 [D loss: -0.424013] [G loss: -1.672691]\n",
      "614 [D loss: -0.312706] [G loss: -1.613472]\n",
      "615 [D loss: -0.115201] [G loss: -1.907668]\n",
      "616 [D loss: -0.306433] [G loss: -1.854421]\n",
      "617 [D loss: -0.050328] [G loss: -1.722121]\n",
      "618 [D loss: -0.672457] [G loss: -1.779915]\n",
      "619 [D loss: -0.004072] [G loss: -1.931314]\n",
      "620 [D loss: -0.423566] [G loss: -1.296290]\n",
      "621 [D loss: -0.490865] [G loss: -1.752212]\n",
      "622 [D loss: -0.055193] [G loss: -1.704353]\n",
      "623 [D loss: -0.282788] [G loss: -1.960953]\n",
      "624 [D loss: 0.081683] [G loss: -2.210885]\n",
      "625 [D loss: -0.941437] [G loss: -1.654058]\n",
      "626 [D loss: -0.458496] [G loss: -1.531473]\n",
      "627 [D loss: -0.401040] [G loss: -1.392893]\n",
      "628 [D loss: -0.194421] [G loss: -2.005177]\n",
      "629 [D loss: -0.296591] [G loss: -1.663718]\n",
      "630 [D loss: -0.182135] [G loss: -1.378773]\n",
      "631 [D loss: -0.492057] [G loss: -1.679363]\n",
      "632 [D loss: -0.396252] [G loss: -2.061579]\n",
      "633 [D loss: -0.351735] [G loss: -1.897248]\n",
      "634 [D loss: -0.439729] [G loss: -1.688144]\n",
      "635 [D loss: 0.080313] [G loss: -1.993926]\n",
      "636 [D loss: -0.047020] [G loss: -1.924597]\n",
      "637 [D loss: -0.447029] [G loss: -2.055810]\n",
      "638 [D loss: -0.250409] [G loss: -1.657474]\n",
      "639 [D loss: -0.138258] [G loss: -1.566582]\n",
      "640 [D loss: -0.213650] [G loss: -1.693870]\n",
      "641 [D loss: -0.434357] [G loss: -1.467347]\n",
      "642 [D loss: -0.324230] [G loss: -1.794248]\n",
      "643 [D loss: -0.261264] [G loss: -1.664737]\n",
      "644 [D loss: -0.180331] [G loss: -2.205965]\n",
      "645 [D loss: -0.445680] [G loss: -1.778390]\n",
      "646 [D loss: -0.400152] [G loss: -1.467818]\n",
      "647 [D loss: -0.478419] [G loss: -1.922174]\n",
      "648 [D loss: -0.390013] [G loss: -1.709661]\n",
      "649 [D loss: 0.022564] [G loss: -1.520592]\n",
      "650 [D loss: -0.349591] [G loss: -1.737566]\n",
      "651 [D loss: -0.578643] [G loss: -1.725468]\n",
      "652 [D loss: -0.661615] [G loss: -1.802966]\n",
      "653 [D loss: 0.167222] [G loss: -1.783785]\n",
      "654 [D loss: -0.513032] [G loss: -1.529362]\n",
      "655 [D loss: -0.532999] [G loss: -1.575681]\n",
      "656 [D loss: -0.205624] [G loss: -1.894091]\n",
      "657 [D loss: -0.109453] [G loss: -1.676613]\n",
      "658 [D loss: -0.402241] [G loss: -1.684778]\n",
      "659 [D loss: -0.645566] [G loss: -1.719680]\n",
      "660 [D loss: 0.238876] [G loss: -2.012413]\n",
      "661 [D loss: -0.326277] [G loss: -1.831853]\n",
      "662 [D loss: -0.614222] [G loss: -1.583232]\n",
      "663 [D loss: -0.082187] [G loss: -1.769018]\n",
      "664 [D loss: -0.271219] [G loss: -1.898077]\n",
      "665 [D loss: -0.349581] [G loss: -1.829729]\n",
      "666 [D loss: -0.373711] [G loss: -2.479503]\n",
      "667 [D loss: -0.741093] [G loss: -1.975060]\n",
      "668 [D loss: -0.311647] [G loss: -1.800027]\n",
      "669 [D loss: -0.524607] [G loss: -1.757720]\n",
      "670 [D loss: -0.528400] [G loss: -1.778035]\n",
      "671 [D loss: 0.054641] [G loss: -1.852197]\n",
      "672 [D loss: -0.442983] [G loss: -1.663061]\n",
      "673 [D loss: -0.293846] [G loss: -1.690039]\n",
      "674 [D loss: -0.070044] [G loss: -1.716547]\n",
      "675 [D loss: -0.124251] [G loss: -1.896079]\n",
      "676 [D loss: -0.289984] [G loss: -1.854422]\n",
      "677 [D loss: -0.331093] [G loss: -1.861586]\n",
      "678 [D loss: -0.760567] [G loss: -1.782438]\n",
      "679 [D loss: -0.713700] [G loss: -1.879426]\n",
      "680 [D loss: 0.184381] [G loss: -1.846846]\n",
      "681 [D loss: -0.514891] [G loss: -2.087910]\n",
      "682 [D loss: -0.681416] [G loss: -2.198169]\n",
      "683 [D loss: -0.471749] [G loss: -2.053531]\n",
      "684 [D loss: -0.988408] [G loss: -1.763158]\n",
      "685 [D loss: -0.511226] [G loss: -2.144315]\n",
      "686 [D loss: -0.536981] [G loss: -1.892893]\n",
      "687 [D loss: -0.313330] [G loss: -1.913191]\n",
      "688 [D loss: -0.307466] [G loss: -2.071864]\n",
      "689 [D loss: 0.222958] [G loss: -2.540676]\n",
      "690 [D loss: -0.389651] [G loss: -1.998690]\n",
      "691 [D loss: -0.259158] [G loss: -2.010922]\n",
      "692 [D loss: -0.121693] [G loss: -2.083990]\n",
      "693 [D loss: -0.217802] [G loss: -1.768178]\n",
      "694 [D loss: -0.060528] [G loss: -2.031970]\n",
      "695 [D loss: 0.112599] [G loss: -1.975419]\n",
      "696 [D loss: -0.326410] [G loss: -2.237654]\n",
      "697 [D loss: -0.827212] [G loss: -1.574480]\n",
      "698 [D loss: 0.394382] [G loss: -2.058988]\n",
      "699 [D loss: -0.363241] [G loss: -2.165106]\n",
      "700 [D loss: -0.716991] [G loss: -2.288502]\n",
      "701 [D loss: -0.440599] [G loss: -2.097457]\n",
      "702 [D loss: -0.832565] [G loss: -2.148857]\n",
      "703 [D loss: -0.716641] [G loss: -2.112979]\n",
      "704 [D loss: -0.450790] [G loss: -2.235780]\n",
      "705 [D loss: -0.302432] [G loss: -1.597218]\n",
      "706 [D loss: -0.235651] [G loss: -2.070434]\n",
      "707 [D loss: -0.730663] [G loss: -1.696485]\n",
      "708 [D loss: -0.693271] [G loss: -2.285269]\n",
      "709 [D loss: -0.339262] [G loss: -2.139023]\n",
      "710 [D loss: -0.342782] [G loss: -2.320825]\n",
      "711 [D loss: -0.580589] [G loss: -2.110123]\n",
      "712 [D loss: -0.495892] [G loss: -2.544015]\n",
      "713 [D loss: -0.540277] [G loss: -1.908365]\n",
      "714 [D loss: -0.036432] [G loss: -2.408241]\n",
      "715 [D loss: -0.000346] [G loss: -1.984291]\n",
      "716 [D loss: -0.432453] [G loss: -1.887731]\n",
      "717 [D loss: -0.469015] [G loss: -1.809911]\n",
      "718 [D loss: -0.470624] [G loss: -1.926717]\n",
      "719 [D loss: -0.508732] [G loss: -1.957166]\n",
      "720 [D loss: -0.761117] [G loss: -1.880200]\n",
      "721 [D loss: -0.402630] [G loss: -1.898980]\n",
      "722 [D loss: -0.573163] [G loss: -2.117380]\n",
      "723 [D loss: -0.609656] [G loss: -2.059923]\n",
      "724 [D loss: -0.424195] [G loss: -2.483554]\n",
      "725 [D loss: 0.152628] [G loss: -1.703742]\n",
      "726 [D loss: -0.226892] [G loss: -2.038592]\n",
      "727 [D loss: -0.222870] [G loss: -2.197087]\n",
      "728 [D loss: -0.567695] [G loss: -1.870167]\n",
      "729 [D loss: 0.042743] [G loss: -2.080987]\n",
      "730 [D loss: -0.029458] [G loss: -2.113120]\n",
      "731 [D loss: -0.251817] [G loss: -1.841311]\n",
      "732 [D loss: 0.087261] [G loss: -2.141005]\n",
      "733 [D loss: -0.473503] [G loss: -2.051779]\n",
      "734 [D loss: -0.665467] [G loss: -2.024746]\n",
      "735 [D loss: -0.153295] [G loss: -2.055276]\n",
      "736 [D loss: -0.661340] [G loss: -1.602887]\n",
      "737 [D loss: -0.088544] [G loss: -1.701128]\n",
      "738 [D loss: -0.242968] [G loss: -2.260131]\n",
      "739 [D loss: -0.696295] [G loss: -1.864829]\n",
      "740 [D loss: -0.805316] [G loss: -1.666304]\n",
      "741 [D loss: -0.287697] [G loss: -2.184783]\n",
      "742 [D loss: 0.089986] [G loss: -1.652228]\n",
      "743 [D loss: -0.253897] [G loss: -1.686106]\n",
      "744 [D loss: -0.873147] [G loss: -1.954756]\n",
      "745 [D loss: -0.612839] [G loss: -1.941142]\n",
      "746 [D loss: -0.810987] [G loss: -1.617342]\n",
      "747 [D loss: 0.076809] [G loss: -1.648166]\n",
      "748 [D loss: -0.212278] [G loss: -1.722162]\n",
      "749 [D loss: -0.507094] [G loss: -2.007535]\n",
      "750 [D loss: -0.091830] [G loss: -1.473837]\n",
      "751 [D loss: -0.201481] [G loss: -2.101535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "752 [D loss: -1.178323] [G loss: -1.816783]\n",
      "753 [D loss: -0.934460] [G loss: -1.955165]\n",
      "754 [D loss: -0.054417] [G loss: -1.774947]\n",
      "755 [D loss: -1.108276] [G loss: -1.511630]\n",
      "756 [D loss: -0.133919] [G loss: -1.576594]\n",
      "757 [D loss: -0.886952] [G loss: -2.077778]\n",
      "758 [D loss: -0.239481] [G loss: -1.233009]\n",
      "759 [D loss: -0.465669] [G loss: -1.725977]\n",
      "760 [D loss: -0.373216] [G loss: -1.939457]\n",
      "761 [D loss: -0.955613] [G loss: -1.587133]\n",
      "762 [D loss: -0.172634] [G loss: -1.825816]\n",
      "763 [D loss: -0.171664] [G loss: -2.080106]\n",
      "764 [D loss: -0.312583] [G loss: -1.789814]\n",
      "765 [D loss: -0.087180] [G loss: -1.606876]\n",
      "766 [D loss: -0.622036] [G loss: -1.540080]\n",
      "767 [D loss: -0.182765] [G loss: -2.016765]\n",
      "768 [D loss: 0.049768] [G loss: -1.949809]\n",
      "769 [D loss: -0.102701] [G loss: -1.811347]\n",
      "770 [D loss: -0.862979] [G loss: -1.765056]\n",
      "771 [D loss: -0.153048] [G loss: -2.138021]\n",
      "772 [D loss: -0.980550] [G loss: -1.665675]\n",
      "773 [D loss: -0.211273] [G loss: -1.793531]\n",
      "774 [D loss: -0.725310] [G loss: -1.871071]\n",
      "775 [D loss: -0.605046] [G loss: -1.838801]\n",
      "776 [D loss: -0.487640] [G loss: -1.855689]\n",
      "777 [D loss: -0.092491] [G loss: -1.707359]\n",
      "778 [D loss: -0.077505] [G loss: -1.985008]\n",
      "779 [D loss: -0.397928] [G loss: -2.009441]\n",
      "780 [D loss: -0.468708] [G loss: -1.859712]\n",
      "781 [D loss: -0.410727] [G loss: -1.810103]\n",
      "782 [D loss: -0.361116] [G loss: -1.863340]\n",
      "783 [D loss: -0.329520] [G loss: -1.657387]\n",
      "784 [D loss: -0.669398] [G loss: -2.017718]\n",
      "785 [D loss: -0.439430] [G loss: -1.976480]\n",
      "786 [D loss: -0.156481] [G loss: -1.735795]\n",
      "787 [D loss: -0.483266] [G loss: -1.878818]\n",
      "788 [D loss: -0.647898] [G loss: -2.156613]\n",
      "789 [D loss: -0.432934] [G loss: -1.942900]\n",
      "790 [D loss: -0.631956] [G loss: -1.821055]\n",
      "791 [D loss: -0.129581] [G loss: -2.114766]\n",
      "792 [D loss: -0.540005] [G loss: -2.332489]\n",
      "793 [D loss: 0.029534] [G loss: -2.055682]\n",
      "794 [D loss: -0.332995] [G loss: -2.154494]\n",
      "795 [D loss: -0.264760] [G loss: -1.561331]\n",
      "796 [D loss: -0.395422] [G loss: -1.998855]\n",
      "797 [D loss: -0.141503] [G loss: -1.739901]\n",
      "798 [D loss: -0.363246] [G loss: -2.010946]\n",
      "799 [D loss: -0.147311] [G loss: -1.687124]\n",
      "800 [D loss: -0.224077] [G loss: -2.004551]\n",
      "801 [D loss: 0.001785] [G loss: -1.905746]\n",
      "802 [D loss: 0.172536] [G loss: -2.153552]\n",
      "803 [D loss: -0.382092] [G loss: -2.052182]\n",
      "804 [D loss: -0.238352] [G loss: -1.933372]\n",
      "805 [D loss: -0.818923] [G loss: -1.477032]\n",
      "806 [D loss: -0.431501] [G loss: -2.276451]\n",
      "807 [D loss: -0.765160] [G loss: -1.632561]\n",
      "808 [D loss: 0.185789] [G loss: -2.399297]\n",
      "809 [D loss: -0.313832] [G loss: -1.745643]\n",
      "810 [D loss: -0.114348] [G loss: -2.050771]\n",
      "811 [D loss: -0.585426] [G loss: -1.778215]\n",
      "812 [D loss: -0.362072] [G loss: -1.597920]\n",
      "813 [D loss: -0.556881] [G loss: -2.384878]\n",
      "814 [D loss: -0.609365] [G loss: -2.228810]\n",
      "815 [D loss: 0.210415] [G loss: -1.508409]\n",
      "816 [D loss: -0.011992] [G loss: -2.029037]\n",
      "817 [D loss: -0.476999] [G loss: -1.879073]\n",
      "818 [D loss: -0.541234] [G loss: -1.882999]\n",
      "819 [D loss: 0.006487] [G loss: -1.936823]\n",
      "820 [D loss: -0.159131] [G loss: -1.814545]\n",
      "821 [D loss: -0.489612] [G loss: -2.009562]\n",
      "822 [D loss: -0.890942] [G loss: -2.085776]\n",
      "823 [D loss: -0.358289] [G loss: -2.319200]\n",
      "824 [D loss: -0.286918] [G loss: -2.039331]\n",
      "825 [D loss: -0.487314] [G loss: -1.708594]\n",
      "826 [D loss: -0.467807] [G loss: -1.767682]\n",
      "827 [D loss: -0.734506] [G loss: -1.673507]\n",
      "828 [D loss: -0.467784] [G loss: -1.935886]\n",
      "829 [D loss: -0.529678] [G loss: -2.016685]\n",
      "830 [D loss: -0.845121] [G loss: -1.900612]\n",
      "831 [D loss: -0.060115] [G loss: -1.848313]\n",
      "832 [D loss: -0.207451] [G loss: -1.751549]\n",
      "833 [D loss: -0.334509] [G loss: -2.362559]\n",
      "834 [D loss: -0.067580] [G loss: -1.898876]\n",
      "835 [D loss: -0.517199] [G loss: -1.448738]\n",
      "836 [D loss: -0.666563] [G loss: -1.741751]\n",
      "837 [D loss: -0.199584] [G loss: -1.751438]\n",
      "838 [D loss: -0.476046] [G loss: -1.989716]\n",
      "839 [D loss: -0.670439] [G loss: -1.654525]\n",
      "840 [D loss: -0.343873] [G loss: -1.811658]\n",
      "841 [D loss: -0.220285] [G loss: -1.875852]\n",
      "842 [D loss: -0.635759] [G loss: -1.557120]\n",
      "843 [D loss: -0.083085] [G loss: -1.452204]\n",
      "844 [D loss: -0.704277] [G loss: -1.321595]\n",
      "845 [D loss: -0.507626] [G loss: -1.697037]\n",
      "846 [D loss: -0.627039] [G loss: -1.859341]\n",
      "847 [D loss: -0.990031] [G loss: -1.707777]\n",
      "848 [D loss: -0.413957] [G loss: -1.903416]\n",
      "849 [D loss: 0.167216] [G loss: -1.702722]\n",
      "850 [D loss: -0.487304] [G loss: -1.658032]\n",
      "851 [D loss: -0.225891] [G loss: -1.863671]\n",
      "852 [D loss: -0.748767] [G loss: -1.572662]\n",
      "853 [D loss: 0.037185] [G loss: -2.264965]\n",
      "854 [D loss: -0.346674] [G loss: -1.773967]\n",
      "855 [D loss: -0.205666] [G loss: -1.782996]\n",
      "856 [D loss: -0.445363] [G loss: -1.528485]\n",
      "857 [D loss: 0.125398] [G loss: -1.978001]\n",
      "858 [D loss: -0.299930] [G loss: -1.545687]\n",
      "859 [D loss: -0.440774] [G loss: -1.841131]\n",
      "860 [D loss: -0.173670] [G loss: -1.752013]\n",
      "861 [D loss: -0.105736] [G loss: -1.807673]\n",
      "862 [D loss: 0.155558] [G loss: -1.981750]\n",
      "863 [D loss: -0.786989] [G loss: -1.747356]\n",
      "864 [D loss: -0.527640] [G loss: -1.864377]\n",
      "865 [D loss: -0.781138] [G loss: -2.055024]\n",
      "866 [D loss: -0.693484] [G loss: -1.745362]\n",
      "867 [D loss: -0.022742] [G loss: -1.784206]\n",
      "868 [D loss: -0.463541] [G loss: -1.950212]\n",
      "869 [D loss: -0.706561] [G loss: -1.667653]\n",
      "870 [D loss: -0.224780] [G loss: -1.678540]\n",
      "871 [D loss: -1.187877] [G loss: -1.545783]\n",
      "872 [D loss: -0.654404] [G loss: -1.958214]\n",
      "873 [D loss: -0.381839] [G loss: -1.721188]\n",
      "874 [D loss: -0.341945] [G loss: -1.627280]\n",
      "875 [D loss: -0.198481] [G loss: -1.802635]\n",
      "876 [D loss: -1.019904] [G loss: -1.638050]\n",
      "877 [D loss: -0.249840] [G loss: -1.583451]\n",
      "878 [D loss: -0.692688] [G loss: -1.702815]\n",
      "879 [D loss: -0.391288] [G loss: -1.430119]\n",
      "880 [D loss: -0.842372] [G loss: -1.293925]\n",
      "881 [D loss: -0.362395] [G loss: -1.913447]\n",
      "882 [D loss: -0.122304] [G loss: -1.443403]\n",
      "883 [D loss: -0.488001] [G loss: -1.335373]\n",
      "884 [D loss: -0.424441] [G loss: -1.784646]\n",
      "885 [D loss: -0.505205] [G loss: -1.377969]\n",
      "886 [D loss: -0.453388] [G loss: -1.679696]\n",
      "887 [D loss: -0.444178] [G loss: -1.798713]\n",
      "888 [D loss: -0.321486] [G loss: -1.747507]\n",
      "889 [D loss: -0.199727] [G loss: -2.000739]\n",
      "890 [D loss: -0.865448] [G loss: -1.750165]\n",
      "891 [D loss: -0.357777] [G loss: -1.497752]\n",
      "892 [D loss: -0.436445] [G loss: -1.818147]\n",
      "893 [D loss: -0.744600] [G loss: -2.035561]\n",
      "894 [D loss: -0.423514] [G loss: -1.478069]\n",
      "895 [D loss: -0.830706] [G loss: -1.862465]\n",
      "896 [D loss: -0.364284] [G loss: -1.847164]\n",
      "897 [D loss: -0.603515] [G loss: -1.970794]\n",
      "898 [D loss: -0.613796] [G loss: -1.616098]\n",
      "899 [D loss: -0.905607] [G loss: -1.633062]\n",
      "900 [D loss: -0.653318] [G loss: -1.268569]\n",
      "901 [D loss: -0.660723] [G loss: -1.817937]\n",
      "902 [D loss: -0.272951] [G loss: -1.501315]\n",
      "903 [D loss: -0.968425] [G loss: -1.420226]\n",
      "904 [D loss: -0.870570] [G loss: -1.699413]\n",
      "905 [D loss: -0.687097] [G loss: -1.521718]\n",
      "906 [D loss: -0.272613] [G loss: -1.462082]\n",
      "907 [D loss: -0.140480] [G loss: -1.735328]\n",
      "908 [D loss: -0.658741] [G loss: -1.486291]\n",
      "909 [D loss: -0.759587] [G loss: -1.816450]\n",
      "910 [D loss: -0.284139] [G loss: -1.565512]\n",
      "911 [D loss: -0.377697] [G loss: -2.037071]\n",
      "912 [D loss: -1.204316] [G loss: -1.322632]\n",
      "913 [D loss: -0.522025] [G loss: -1.384926]\n",
      "914 [D loss: -0.793454] [G loss: -1.549531]\n",
      "915 [D loss: -0.974177] [G loss: -1.954094]\n",
      "916 [D loss: -0.912875] [G loss: -1.666210]\n",
      "917 [D loss: -0.454670] [G loss: -1.662596]\n",
      "918 [D loss: -0.701953] [G loss: -1.544523]\n",
      "919 [D loss: -0.684543] [G loss: -1.640556]\n",
      "920 [D loss: -0.463682] [G loss: -1.801270]\n",
      "921 [D loss: -0.901348] [G loss: -1.635844]\n",
      "922 [D loss: -0.342106] [G loss: -1.833898]\n",
      "923 [D loss: -0.195669] [G loss: -1.934788]\n",
      "924 [D loss: -1.358315] [G loss: -1.875005]\n",
      "925 [D loss: -0.163683] [G loss: -2.302987]\n",
      "926 [D loss: -0.810078] [G loss: -1.605712]\n",
      "927 [D loss: -1.003124] [G loss: -1.934302]\n",
      "928 [D loss: -0.808569] [G loss: -1.735310]\n",
      "929 [D loss: -0.329765] [G loss: -1.461567]\n",
      "930 [D loss: -0.220095] [G loss: -1.499475]\n",
      "931 [D loss: -0.436931] [G loss: -1.937737]\n",
      "932 [D loss: -0.227754] [G loss: -1.541419]\n",
      "933 [D loss: -0.359017] [G loss: -1.606953]\n",
      "934 [D loss: -0.235479] [G loss: -2.060560]\n",
      "935 [D loss: -0.395531] [G loss: -1.677548]\n",
      "936 [D loss: -0.521075] [G loss: -1.635786]\n",
      "937 [D loss: -0.802409] [G loss: -1.526019]\n",
      "938 [D loss: -0.588225] [G loss: -1.640258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "939 [D loss: -0.624783] [G loss: -2.010655]\n",
      "940 [D loss: -0.790605] [G loss: -1.271477]\n",
      "941 [D loss: -0.283358] [G loss: -1.693277]\n",
      "942 [D loss: -0.338650] [G loss: -1.836375]\n",
      "943 [D loss: -1.196499] [G loss: -1.537358]\n",
      "944 [D loss: -0.611151] [G loss: -1.808706]\n",
      "945 [D loss: -0.473140] [G loss: -1.869097]\n",
      "946 [D loss: -0.498925] [G loss: -1.284256]\n",
      "947 [D loss: -0.252894] [G loss: -2.145879]\n",
      "948 [D loss: -0.064050] [G loss: -1.552098]\n",
      "949 [D loss: -0.760182] [G loss: -1.439293]\n",
      "950 [D loss: 0.366946] [G loss: -1.308657]\n",
      "951 [D loss: -0.556600] [G loss: -1.192475]\n",
      "952 [D loss: -0.704558] [G loss: -1.679031]\n",
      "953 [D loss: 0.057306] [G loss: -1.535048]\n",
      "954 [D loss: -1.338893] [G loss: -1.276674]\n",
      "955 [D loss: -0.409955] [G loss: -1.632029]\n",
      "956 [D loss: -0.528384] [G loss: -1.423527]\n",
      "957 [D loss: -0.673371] [G loss: -1.756269]\n",
      "958 [D loss: -0.581719] [G loss: -1.654210]\n",
      "959 [D loss: -0.509836] [G loss: -1.391332]\n",
      "960 [D loss: -0.822713] [G loss: -1.516870]\n",
      "961 [D loss: -0.571318] [G loss: -1.373550]\n",
      "962 [D loss: -0.984894] [G loss: -1.468943]\n",
      "963 [D loss: -0.769057] [G loss: -1.757721]\n",
      "964 [D loss: -0.272573] [G loss: -1.604369]\n",
      "965 [D loss: -0.641303] [G loss: -1.298477]\n",
      "966 [D loss: -0.699225] [G loss: -1.507371]\n",
      "967 [D loss: -0.526182] [G loss: -1.341951]\n",
      "968 [D loss: -0.662510] [G loss: -1.745742]\n",
      "969 [D loss: -0.672944] [G loss: -1.515578]\n",
      "970 [D loss: -0.253401] [G loss: -1.236068]\n",
      "971 [D loss: -0.825068] [G loss: -1.561832]\n",
      "972 [D loss: -0.045601] [G loss: -1.742642]\n",
      "973 [D loss: -0.943837] [G loss: -1.409425]\n",
      "974 [D loss: -0.306510] [G loss: -1.734380]\n",
      "975 [D loss: -0.703955] [G loss: -1.483961]\n",
      "976 [D loss: -0.481840] [G loss: -1.344408]\n",
      "977 [D loss: -1.007612] [G loss: -1.702855]\n",
      "978 [D loss: -0.468062] [G loss: -1.207388]\n",
      "979 [D loss: -0.629052] [G loss: -1.628268]\n",
      "980 [D loss: -0.478613] [G loss: -1.666512]\n",
      "981 [D loss: -0.186114] [G loss: -1.680308]\n",
      "982 [D loss: -0.675159] [G loss: -1.459495]\n",
      "983 [D loss: -1.151837] [G loss: -1.836281]\n",
      "984 [D loss: -0.424307] [G loss: -1.789146]\n",
      "985 [D loss: -0.599575] [G loss: -1.735764]\n",
      "986 [D loss: -0.890735] [G loss: -1.771725]\n",
      "987 [D loss: -0.505345] [G loss: -1.587679]\n",
      "988 [D loss: -0.262892] [G loss: -1.013842]\n",
      "989 [D loss: -0.534046] [G loss: -2.053335]\n",
      "990 [D loss: -0.276432] [G loss: -1.825426]\n",
      "991 [D loss: -0.630978] [G loss: -1.633266]\n",
      "992 [D loss: -0.905844] [G loss: -1.761226]\n",
      "993 [D loss: -0.302012] [G loss: -1.966245]\n",
      "994 [D loss: -0.307112] [G loss: -2.403165]\n",
      "995 [D loss: -0.253442] [G loss: -1.671117]\n",
      "996 [D loss: -0.271116] [G loss: -1.882159]\n",
      "997 [D loss: -0.080406] [G loss: -1.977906]\n",
      "998 [D loss: -1.062170] [G loss: -1.794958]\n",
      "999 [D loss: -0.753713] [G loss: -1.740021]\n",
      "1000 [D loss: -0.478241] [G loss: -1.860890]\n",
      "1001 [D loss: -1.145993] [G loss: -1.828200]\n",
      "1002 [D loss: -1.063214] [G loss: -1.965815]\n",
      "1003 [D loss: -0.682418] [G loss: -2.195546]\n",
      "1004 [D loss: -0.832509] [G loss: -2.124704]\n",
      "1005 [D loss: -1.046026] [G loss: -1.872183]\n",
      "1006 [D loss: -0.406902] [G loss: -1.665102]\n",
      "1007 [D loss: -0.600620] [G loss: -1.835703]\n",
      "1008 [D loss: -0.641692] [G loss: -1.752481]\n",
      "1009 [D loss: -0.434954] [G loss: -1.565821]\n",
      "1010 [D loss: -0.373335] [G loss: -2.255784]\n",
      "1011 [D loss: -1.039694] [G loss: -1.745930]\n",
      "1012 [D loss: -1.101812] [G loss: -2.125672]\n",
      "1013 [D loss: -0.726225] [G loss: -1.891291]\n",
      "1014 [D loss: -0.626333] [G loss: -1.970779]\n",
      "1015 [D loss: -0.451069] [G loss: -1.563056]\n",
      "1016 [D loss: -0.928805] [G loss: -1.857338]\n",
      "1017 [D loss: -0.794001] [G loss: -2.002024]\n",
      "1018 [D loss: -0.136464] [G loss: -1.953785]\n",
      "1019 [D loss: -0.496033] [G loss: -1.716725]\n",
      "1020 [D loss: -1.012601] [G loss: -2.219118]\n",
      "1021 [D loss: -0.326483] [G loss: -1.768611]\n",
      "1022 [D loss: -1.099538] [G loss: -1.624231]\n",
      "1023 [D loss: -0.472686] [G loss: -1.678685]\n",
      "1024 [D loss: -0.795462] [G loss: -1.966741]\n",
      "1025 [D loss: -0.324379] [G loss: -1.771100]\n",
      "1026 [D loss: -0.444800] [G loss: -1.892076]\n",
      "1027 [D loss: -0.835870] [G loss: -1.304851]\n",
      "1028 [D loss: -0.287525] [G loss: -2.581313]\n",
      "1029 [D loss: -0.572335] [G loss: -1.571040]\n",
      "1030 [D loss: -0.419707] [G loss: -1.663918]\n",
      "1031 [D loss: -0.561081] [G loss: -1.860192]\n",
      "1032 [D loss: -0.449378] [G loss: -1.737604]\n",
      "1033 [D loss: -0.540828] [G loss: -1.657611]\n",
      "1034 [D loss: -0.675135] [G loss: -1.808656]\n",
      "1035 [D loss: -0.730771] [G loss: -1.851606]\n",
      "1036 [D loss: -0.708576] [G loss: -1.623066]\n",
      "1037 [D loss: -0.504115] [G loss: -1.935061]\n",
      "1038 [D loss: -0.938981] [G loss: -1.847492]\n",
      "1039 [D loss: -0.338919] [G loss: -1.767619]\n",
      "1040 [D loss: -0.858273] [G loss: -1.613094]\n",
      "1041 [D loss: -0.821093] [G loss: -1.876409]\n",
      "1042 [D loss: -1.402237] [G loss: -1.563994]\n",
      "1043 [D loss: -0.770396] [G loss: -1.701534]\n",
      "1044 [D loss: -0.730393] [G loss: -1.342736]\n",
      "1045 [D loss: -0.838162] [G loss: -1.466794]\n",
      "1046 [D loss: -0.726860] [G loss: -1.737835]\n",
      "1047 [D loss: -0.700198] [G loss: -1.249549]\n",
      "1048 [D loss: -0.356788] [G loss: -1.420869]\n",
      "1049 [D loss: -0.693916] [G loss: -1.495572]\n",
      "1050 [D loss: -0.889639] [G loss: -1.786314]\n",
      "1051 [D loss: -0.697685] [G loss: -1.285209]\n",
      "1052 [D loss: -0.396837] [G loss: -1.746605]\n",
      "1053 [D loss: -0.885308] [G loss: -1.736146]\n",
      "1054 [D loss: -0.595038] [G loss: -1.280962]\n",
      "1055 [D loss: -0.732688] [G loss: -1.687025]\n",
      "1056 [D loss: -0.995579] [G loss: -1.194681]\n",
      "1057 [D loss: -0.520507] [G loss: -1.154070]\n",
      "1058 [D loss: -0.554595] [G loss: -1.371290]\n",
      "1059 [D loss: -0.894864] [G loss: -1.828554]\n",
      "1060 [D loss: -0.857125] [G loss: -1.411335]\n",
      "1061 [D loss: -0.052375] [G loss: -1.758410]\n",
      "1062 [D loss: -0.493740] [G loss: -1.252544]\n",
      "1063 [D loss: -0.703065] [G loss: -1.526957]\n",
      "1064 [D loss: -0.535401] [G loss: -1.593647]\n",
      "1065 [D loss: -1.089683] [G loss: -1.123442]\n",
      "1066 [D loss: -0.682277] [G loss: -1.380459]\n",
      "1067 [D loss: -1.194277] [G loss: -1.454656]\n",
      "1068 [D loss: -0.550071] [G loss: -1.342639]\n",
      "1069 [D loss: -1.085805] [G loss: -1.442188]\n",
      "1070 [D loss: -0.045485] [G loss: -1.562591]\n",
      "1071 [D loss: -0.412250] [G loss: -1.319846]\n",
      "1072 [D loss: -0.029588] [G loss: -1.576664]\n",
      "1073 [D loss: -0.857289] [G loss: -1.775257]\n",
      "1074 [D loss: -1.564660] [G loss: -1.040034]\n",
      "1075 [D loss: -0.939793] [G loss: -1.393364]\n",
      "1076 [D loss: -0.353412] [G loss: -1.260423]\n",
      "1077 [D loss: -0.666716] [G loss: -1.239078]\n",
      "1078 [D loss: -0.693970] [G loss: -1.431481]\n",
      "1079 [D loss: -0.809603] [G loss: -1.713648]\n",
      "1080 [D loss: -0.514344] [G loss: -0.934703]\n",
      "1081 [D loss: -0.714681] [G loss: -1.537661]\n",
      "1082 [D loss: -0.734061] [G loss: -0.971525]\n",
      "1083 [D loss: -0.366283] [G loss: -1.055583]\n",
      "1084 [D loss: 0.072601] [G loss: -1.255646]\n",
      "1085 [D loss: -0.257219] [G loss: -1.193253]\n",
      "1086 [D loss: -0.272478] [G loss: -1.380716]\n",
      "1087 [D loss: -0.382340] [G loss: -1.216479]\n",
      "1088 [D loss: -0.629251] [G loss: -1.330621]\n",
      "1089 [D loss: -0.700321] [G loss: -0.998392]\n",
      "1090 [D loss: -0.086098] [G loss: -1.219972]\n",
      "1091 [D loss: -0.691892] [G loss: -1.234903]\n",
      "1092 [D loss: -0.479469] [G loss: -1.168068]\n",
      "1093 [D loss: -0.491740] [G loss: -1.392411]\n",
      "1094 [D loss: -0.529325] [G loss: -1.278440]\n",
      "1095 [D loss: -0.276236] [G loss: -1.170692]\n",
      "1096 [D loss: -0.550731] [G loss: -1.641828]\n",
      "1097 [D loss: -1.194822] [G loss: -0.671032]\n",
      "1098 [D loss: -0.650170] [G loss: -0.795031]\n",
      "1099 [D loss: -0.658103] [G loss: -1.420642]\n",
      "1100 [D loss: -0.777280] [G loss: -1.359160]\n",
      "1101 [D loss: -0.573176] [G loss: -1.690924]\n",
      "1102 [D loss: -0.249938] [G loss: -1.312824]\n",
      "1103 [D loss: -0.337633] [G loss: -1.389885]\n",
      "1104 [D loss: -1.159501] [G loss: -1.339553]\n",
      "1105 [D loss: -0.735728] [G loss: -1.256628]\n",
      "1106 [D loss: -0.720723] [G loss: -1.711411]\n",
      "1107 [D loss: -0.591544] [G loss: -1.524093]\n",
      "1108 [D loss: -0.268591] [G loss: -1.524604]\n",
      "1109 [D loss: -0.622855] [G loss: -1.484530]\n",
      "1110 [D loss: -0.770821] [G loss: -1.331757]\n",
      "1111 [D loss: -0.327249] [G loss: -1.258513]\n",
      "1112 [D loss: -1.084946] [G loss: -1.508520]\n",
      "1113 [D loss: -0.632974] [G loss: -1.481273]\n",
      "1114 [D loss: 0.158589] [G loss: -1.870091]\n",
      "1115 [D loss: -0.560556] [G loss: -1.558499]\n",
      "1116 [D loss: -0.805540] [G loss: -1.557549]\n",
      "1117 [D loss: -0.372875] [G loss: -1.702398]\n",
      "1118 [D loss: -0.713566] [G loss: -1.431311]\n",
      "1119 [D loss: -0.737180] [G loss: -1.400108]\n",
      "1120 [D loss: -0.915129] [G loss: -1.381631]\n",
      "1121 [D loss: -0.914037] [G loss: -1.336329]\n",
      "1122 [D loss: -0.536053] [G loss: -1.211705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1123 [D loss: -0.498669] [G loss: -1.465722]\n",
      "1124 [D loss: -1.106763] [G loss: -1.550880]\n",
      "1125 [D loss: -0.798536] [G loss: -1.821907]\n",
      "1126 [D loss: -0.404165] [G loss: -1.261321]\n",
      "1127 [D loss: -0.448486] [G loss: -1.347453]\n",
      "1128 [D loss: -0.912222] [G loss: -1.688787]\n",
      "1129 [D loss: -0.281690] [G loss: -1.521643]\n",
      "1130 [D loss: -1.171750] [G loss: -1.227340]\n",
      "1131 [D loss: -0.199425] [G loss: -1.348649]\n",
      "1132 [D loss: -0.290584] [G loss: -1.905881]\n",
      "1133 [D loss: -0.981544] [G loss: -2.119254]\n",
      "1134 [D loss: -0.199573] [G loss: -1.464419]\n",
      "1135 [D loss: -1.262376] [G loss: -1.772467]\n",
      "1136 [D loss: -0.870273] [G loss: -1.358447]\n",
      "1137 [D loss: -0.908664] [G loss: -1.493763]\n",
      "1138 [D loss: -0.254390] [G loss: -1.740179]\n",
      "1139 [D loss: -0.233860] [G loss: -1.489768]\n",
      "1140 [D loss: -0.545242] [G loss: -1.823038]\n",
      "1141 [D loss: -0.303103] [G loss: -1.993078]\n",
      "1142 [D loss: -1.180987] [G loss: -1.520151]\n",
      "1143 [D loss: -0.948818] [G loss: -1.692334]\n",
      "1144 [D loss: -0.658545] [G loss: -1.685074]\n",
      "1145 [D loss: -0.649376] [G loss: -1.855376]\n",
      "1146 [D loss: -0.134737] [G loss: -1.764865]\n",
      "1147 [D loss: -0.187203] [G loss: -1.380646]\n",
      "1148 [D loss: -0.613957] [G loss: -1.802646]\n",
      "1149 [D loss: -0.314466] [G loss: -1.806623]\n",
      "1150 [D loss: -0.955043] [G loss: -1.460862]\n",
      "1151 [D loss: -1.059483] [G loss: -1.948789]\n",
      "1152 [D loss: -0.763846] [G loss: -1.237663]\n",
      "1153 [D loss: -0.388449] [G loss: -1.809968]\n",
      "1154 [D loss: -1.056473] [G loss: -1.432751]\n",
      "1155 [D loss: -0.129200] [G loss: -1.475837]\n",
      "1156 [D loss: -0.791840] [G loss: -1.203341]\n",
      "1157 [D loss: -0.717948] [G loss: -1.509164]\n",
      "1158 [D loss: -0.870701] [G loss: -1.834405]\n",
      "1159 [D loss: -0.746967] [G loss: -1.636786]\n",
      "1160 [D loss: -0.713493] [G loss: -1.463825]\n",
      "1161 [D loss: -0.728987] [G loss: -1.907912]\n",
      "1162 [D loss: -1.001372] [G loss: -1.529567]\n",
      "1163 [D loss: -0.748615] [G loss: -1.423868]\n",
      "1164 [D loss: -1.166302] [G loss: -1.469029]\n",
      "1165 [D loss: -0.508604] [G loss: -1.592709]\n",
      "1166 [D loss: -0.642485] [G loss: -1.549453]\n",
      "1167 [D loss: -1.294728] [G loss: -1.469500]\n",
      "1168 [D loss: -0.601296] [G loss: -1.423421]\n",
      "1169 [D loss: -0.457803] [G loss: -1.115898]\n",
      "1170 [D loss: -0.298715] [G loss: -1.309715]\n",
      "1171 [D loss: -1.198639] [G loss: -1.747230]\n",
      "1172 [D loss: -0.760261] [G loss: -1.141352]\n",
      "1173 [D loss: -0.980231] [G loss: -1.354512]\n",
      "1174 [D loss: -0.482255] [G loss: -1.389979]\n",
      "1175 [D loss: -0.562930] [G loss: -1.038289]\n",
      "1176 [D loss: -0.562625] [G loss: -1.508707]\n",
      "1177 [D loss: -0.482729] [G loss: -1.549967]\n",
      "1178 [D loss: -0.741454] [G loss: -1.285179]\n",
      "1179 [D loss: -0.814537] [G loss: -1.314059]\n",
      "1180 [D loss: -0.654996] [G loss: -1.187480]\n",
      "1181 [D loss: -0.762895] [G loss: -1.201202]\n",
      "1182 [D loss: -1.218156] [G loss: -1.283517]\n",
      "1183 [D loss: -0.726744] [G loss: -0.914830]\n",
      "1184 [D loss: -0.205736] [G loss: -1.016928]\n",
      "1185 [D loss: -0.485375] [G loss: -0.982862]\n",
      "1186 [D loss: -0.611735] [G loss: -1.242103]\n",
      "1187 [D loss: -0.077434] [G loss: -1.008549]\n",
      "1188 [D loss: -0.282777] [G loss: -1.238105]\n",
      "1189 [D loss: -1.123098] [G loss: -1.147482]\n",
      "1190 [D loss: -0.896946] [G loss: -0.906997]\n",
      "1191 [D loss: -0.871210] [G loss: -1.204202]\n",
      "1192 [D loss: -0.567027] [G loss: -1.146642]\n",
      "1193 [D loss: -1.652526] [G loss: -0.903306]\n",
      "1194 [D loss: -0.597505] [G loss: -1.354366]\n",
      "1195 [D loss: -0.935809] [G loss: -0.549076]\n",
      "1196 [D loss: -0.637792] [G loss: -1.070509]\n",
      "1197 [D loss: -1.022657] [G loss: -0.918966]\n",
      "1198 [D loss: -0.259500] [G loss: -1.282423]\n",
      "1199 [D loss: -1.355057] [G loss: -0.994402]\n",
      "1200 [D loss: -1.299740] [G loss: -1.406242]\n",
      "1201 [D loss: -0.206477] [G loss: -1.477290]\n",
      "1202 [D loss: -0.708731] [G loss: -1.120688]\n",
      "1203 [D loss: -0.564397] [G loss: -0.680493]\n",
      "1204 [D loss: -0.515665] [G loss: -1.485752]\n",
      "1205 [D loss: -0.604874] [G loss: -1.258904]\n",
      "1206 [D loss: -0.574225] [G loss: -1.278366]\n",
      "1207 [D loss: 0.039408] [G loss: -1.824563]\n",
      "1208 [D loss: -0.490609] [G loss: -0.999806]\n",
      "1209 [D loss: -0.966949] [G loss: -0.610728]\n",
      "1210 [D loss: -0.685966] [G loss: -1.363637]\n",
      "1211 [D loss: -0.755367] [G loss: -1.019078]\n",
      "1212 [D loss: -0.716284] [G loss: -0.949100]\n",
      "1213 [D loss: -0.707430] [G loss: -1.307853]\n",
      "1214 [D loss: -0.893175] [G loss: -1.505560]\n",
      "1215 [D loss: 0.039523] [G loss: -1.413455]\n",
      "1216 [D loss: -0.739677] [G loss: -1.206749]\n",
      "1217 [D loss: -0.695866] [G loss: -1.258687]\n",
      "1218 [D loss: -1.140304] [G loss: -1.127051]\n",
      "1219 [D loss: -0.455900] [G loss: -0.951914]\n",
      "1220 [D loss: -0.068619] [G loss: -1.250435]\n",
      "1221 [D loss: -0.984872] [G loss: -1.588950]\n",
      "1222 [D loss: -0.539839] [G loss: -1.169657]\n",
      "1223 [D loss: -0.272817] [G loss: -1.506922]\n",
      "1224 [D loss: -0.532256] [G loss: -1.511859]\n",
      "1225 [D loss: -0.730042] [G loss: -1.227434]\n",
      "1226 [D loss: -0.922926] [G loss: -1.288375]\n",
      "1227 [D loss: -0.559498] [G loss: -1.707941]\n",
      "1228 [D loss: -0.769433] [G loss: -1.621880]\n",
      "1229 [D loss: -0.914648] [G loss: -1.324163]\n",
      "1230 [D loss: -0.904610] [G loss: -1.368527]\n",
      "1231 [D loss: -0.405909] [G loss: -1.377725]\n",
      "1232 [D loss: -1.227648] [G loss: -1.623226]\n",
      "1233 [D loss: -0.302836] [G loss: -1.292933]\n",
      "1234 [D loss: -0.525939] [G loss: -1.570573]\n",
      "1235 [D loss: -0.185090] [G loss: -1.109040]\n",
      "1236 [D loss: -1.562259] [G loss: -1.100721]\n",
      "1237 [D loss: -0.943718] [G loss: -1.614287]\n",
      "1238 [D loss: -0.721088] [G loss: -1.637424]\n",
      "1239 [D loss: -0.346431] [G loss: -0.864618]\n",
      "1240 [D loss: -0.522021] [G loss: -1.182334]\n",
      "1241 [D loss: -0.467226] [G loss: -1.206311]\n",
      "1242 [D loss: -0.615396] [G loss: -1.018880]\n",
      "1243 [D loss: -0.980570] [G loss: -1.107282]\n",
      "1244 [D loss: -0.915449] [G loss: -1.237662]\n",
      "1245 [D loss: -0.627678] [G loss: -1.241765]\n",
      "1246 [D loss: -0.757579] [G loss: -1.294811]\n",
      "1247 [D loss: -0.998796] [G loss: -1.209765]\n",
      "1248 [D loss: -1.586530] [G loss: -1.491645]\n",
      "1249 [D loss: -0.819051] [G loss: -1.181374]\n",
      "1250 [D loss: -0.772714] [G loss: -1.618883]\n",
      "1251 [D loss: -1.211130] [G loss: -0.760340]\n",
      "1252 [D loss: -0.684119] [G loss: -1.502974]\n",
      "1253 [D loss: -0.940270] [G loss: -1.330775]\n",
      "1254 [D loss: -0.317974] [G loss: -1.679840]\n",
      "1255 [D loss: -0.626416] [G loss: -1.525231]\n",
      "1256 [D loss: -0.941353] [G loss: -1.464549]\n",
      "1257 [D loss: -0.885283] [G loss: -1.327573]\n",
      "1258 [D loss: -0.135257] [G loss: -1.534345]\n",
      "1259 [D loss: -0.560858] [G loss: -1.619233]\n",
      "1260 [D loss: -0.593243] [G loss: -1.210432]\n",
      "1261 [D loss: -0.968907] [G loss: -1.533537]\n",
      "1262 [D loss: -0.610493] [G loss: -1.418221]\n",
      "1263 [D loss: -0.915374] [G loss: -1.581905]\n",
      "1264 [D loss: -0.942414] [G loss: -1.310132]\n",
      "1265 [D loss: -1.106372] [G loss: -1.203121]\n",
      "1266 [D loss: -0.453077] [G loss: -1.224677]\n",
      "1267 [D loss: -0.786296] [G loss: -1.225653]\n",
      "1268 [D loss: -0.523045] [G loss: -1.406622]\n",
      "1269 [D loss: -1.035811] [G loss: -2.033764]\n",
      "1270 [D loss: -0.897745] [G loss: -1.239817]\n",
      "1271 [D loss: -1.403160] [G loss: -1.339694]\n",
      "1272 [D loss: -1.000369] [G loss: -1.177980]\n",
      "1273 [D loss: -0.940501] [G loss: -1.306073]\n",
      "1274 [D loss: -0.667568] [G loss: -1.528658]\n",
      "1275 [D loss: -0.831433] [G loss: -1.212225]\n",
      "1276 [D loss: -0.641798] [G loss: -1.325113]\n",
      "1277 [D loss: -0.722255] [G loss: -1.185066]\n",
      "1278 [D loss: -0.033794] [G loss: -1.613290]\n",
      "1279 [D loss: -1.224648] [G loss: -1.375143]\n",
      "1280 [D loss: -0.601848] [G loss: -1.090295]\n",
      "1281 [D loss: -0.979303] [G loss: -1.509367]\n",
      "1282 [D loss: -0.991495] [G loss: -1.068150]\n",
      "1283 [D loss: -0.482019] [G loss: -0.863225]\n",
      "1284 [D loss: -0.614119] [G loss: -1.213717]\n",
      "1285 [D loss: -0.714551] [G loss: -1.388055]\n",
      "1286 [D loss: -0.278096] [G loss: -1.372030]\n",
      "1287 [D loss: -1.028375] [G loss: -1.107975]\n",
      "1288 [D loss: -1.079483] [G loss: -1.120032]\n",
      "1289 [D loss: -0.495854] [G loss: -0.885163]\n",
      "1290 [D loss: -0.276663] [G loss: -1.509323]\n",
      "1291 [D loss: -0.099358] [G loss: -1.222742]\n",
      "1292 [D loss: -0.670094] [G loss: -0.896605]\n",
      "1293 [D loss: -0.389937] [G loss: -1.190114]\n",
      "1294 [D loss: -1.586070] [G loss: -1.089734]\n",
      "1295 [D loss: -0.673697] [G loss: -1.042111]\n",
      "1296 [D loss: -0.942595] [G loss: -0.903339]\n",
      "1297 [D loss: -0.816563] [G loss: -1.392224]\n",
      "1298 [D loss: -0.403553] [G loss: -1.268521]\n",
      "1299 [D loss: -1.311549] [G loss: -1.362188]\n",
      "1300 [D loss: -0.301952] [G loss: -1.434707]\n",
      "1301 [D loss: -0.490659] [G loss: -1.239033]\n",
      "1302 [D loss: -0.893440] [G loss: -1.482311]\n",
      "1303 [D loss: -0.193290] [G loss: -1.836922]\n",
      "1304 [D loss: -0.847753] [G loss: -1.170660]\n",
      "1305 [D loss: -0.948445] [G loss: -1.712683]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1306 [D loss: -0.515226] [G loss: -1.809892]\n",
      "1307 [D loss: -0.448425] [G loss: -1.476485]\n",
      "1308 [D loss: -0.931819] [G loss: -1.068906]\n",
      "1309 [D loss: -0.933616] [G loss: -1.403155]\n",
      "1310 [D loss: -0.884369] [G loss: -1.464419]\n",
      "1311 [D loss: -0.610432] [G loss: -1.099793]\n",
      "1312 [D loss: -0.895283] [G loss: -1.618497]\n",
      "1313 [D loss: -0.416039] [G loss: -1.725578]\n",
      "1314 [D loss: -0.402085] [G loss: -1.643953]\n",
      "1315 [D loss: -1.130455] [G loss: -1.474515]\n",
      "1316 [D loss: -0.724107] [G loss: -1.509644]\n",
      "1317 [D loss: -0.734595] [G loss: -1.259122]\n",
      "1318 [D loss: -0.857239] [G loss: -1.735345]\n",
      "1319 [D loss: -0.921250] [G loss: -1.303730]\n",
      "1320 [D loss: -0.758149] [G loss: -1.505711]\n",
      "1321 [D loss: -0.546723] [G loss: -1.283880]\n",
      "1322 [D loss: -1.018838] [G loss: -1.397989]\n",
      "1323 [D loss: -1.111800] [G loss: -1.254336]\n",
      "1324 [D loss: -0.395644] [G loss: -1.756588]\n",
      "1325 [D loss: -0.817407] [G loss: -1.564938]\n",
      "1326 [D loss: -0.776689] [G loss: -1.476901]\n",
      "1327 [D loss: -0.809448] [G loss: -1.337738]\n",
      "1328 [D loss: -0.968295] [G loss: -1.262876]\n",
      "1329 [D loss: -0.757187] [G loss: -1.213103]\n",
      "1330 [D loss: -0.957973] [G loss: -1.400889]\n",
      "1331 [D loss: -0.737107] [G loss: -0.891958]\n",
      "1332 [D loss: -0.911072] [G loss: -1.111857]\n",
      "1333 [D loss: -1.008543] [G loss: -0.843021]\n",
      "1334 [D loss: -0.334485] [G loss: -1.117436]\n",
      "1335 [D loss: -0.770327] [G loss: -0.982163]\n",
      "1336 [D loss: -0.058755] [G loss: -0.950030]\n",
      "1337 [D loss: -0.790565] [G loss: -0.642754]\n",
      "1338 [D loss: -0.523739] [G loss: -1.313781]\n",
      "1339 [D loss: -1.066766] [G loss: -1.072022]\n",
      "1340 [D loss: -0.498154] [G loss: -1.556573]\n",
      "1341 [D loss: -0.605452] [G loss: -1.436889]\n",
      "1342 [D loss: -0.875251] [G loss: -0.983514]\n",
      "1343 [D loss: -1.085616] [G loss: -0.888946]\n",
      "1344 [D loss: -0.234330] [G loss: -0.680214]\n",
      "1345 [D loss: -0.572186] [G loss: -0.823219]\n",
      "1346 [D loss: -0.625237] [G loss: -0.801016]\n",
      "1347 [D loss: -1.319125] [G loss: -0.930785]\n",
      "1348 [D loss: -0.588631] [G loss: -0.783608]\n",
      "1349 [D loss: -0.675988] [G loss: -0.634783]\n",
      "1350 [D loss: -0.914663] [G loss: -1.076756]\n",
      "1351 [D loss: -0.805137] [G loss: -0.769976]\n",
      "1352 [D loss: -1.022056] [G loss: -0.840503]\n",
      "1353 [D loss: -1.351190] [G loss: -0.869696]\n",
      "1354 [D loss: -1.426009] [G loss: -0.654558]\n",
      "1355 [D loss: -0.588994] [G loss: -0.598113]\n",
      "1356 [D loss: -1.444023] [G loss: -0.766919]\n",
      "1357 [D loss: -0.768393] [G loss: -0.580479]\n",
      "1358 [D loss: -0.311518] [G loss: -0.709666]\n",
      "1359 [D loss: -1.510457] [G loss: -0.933957]\n",
      "1360 [D loss: -1.381125] [G loss: -0.853199]\n",
      "1361 [D loss: -0.292903] [G loss: -0.836690]\n",
      "1362 [D loss: -1.542158] [G loss: -0.295299]\n",
      "1363 [D loss: -0.827501] [G loss: -0.846000]\n",
      "1364 [D loss: -0.663940] [G loss: -0.450030]\n",
      "1365 [D loss: -0.664599] [G loss: -0.567501]\n",
      "1366 [D loss: -0.340013] [G loss: -0.513444]\n",
      "1367 [D loss: -0.930050] [G loss: -1.161825]\n",
      "1368 [D loss: -1.021804] [G loss: -0.868777]\n",
      "1369 [D loss: -0.201544] [G loss: -0.821281]\n",
      "1370 [D loss: -0.922472] [G loss: -0.866319]\n",
      "1371 [D loss: -0.464295] [G loss: -0.883748]\n",
      "1372 [D loss: -1.002807] [G loss: -0.377251]\n",
      "1373 [D loss: -1.506531] [G loss: -0.884623]\n",
      "1374 [D loss: -1.129224] [G loss: -0.872020]\n",
      "1375 [D loss: -0.903119] [G loss: -1.081302]\n",
      "1376 [D loss: -0.975537] [G loss: -0.701001]\n",
      "1377 [D loss: -0.615388] [G loss: -1.097516]\n",
      "1378 [D loss: 0.180809] [G loss: -0.635959]\n",
      "1379 [D loss: -0.464956] [G loss: -0.757657]\n",
      "1380 [D loss: -0.798161] [G loss: -1.049844]\n",
      "1381 [D loss: -0.829679] [G loss: -0.940610]\n",
      "1382 [D loss: -0.877699] [G loss: -1.089743]\n",
      "1383 [D loss: -0.829226] [G loss: -0.747195]\n",
      "1384 [D loss: -1.304388] [G loss: -0.874097]\n",
      "1385 [D loss: -1.105059] [G loss: -1.451875]\n",
      "1386 [D loss: -1.154980] [G loss: -0.932919]\n",
      "1387 [D loss: -1.134811] [G loss: -0.930698]\n",
      "1388 [D loss: -0.908218] [G loss: -1.157720]\n",
      "1389 [D loss: -0.423838] [G loss: -0.737438]\n",
      "1390 [D loss: -0.549645] [G loss: -1.478783]\n",
      "1391 [D loss: -0.661559] [G loss: -1.398470]\n",
      "1392 [D loss: -0.582034] [G loss: -1.007518]\n",
      "1393 [D loss: -1.111078] [G loss: -1.187728]\n",
      "1394 [D loss: -0.379063] [G loss: -1.225793]\n",
      "1395 [D loss: -0.765734] [G loss: -0.934128]\n",
      "1396 [D loss: -1.171695] [G loss: -1.038002]\n",
      "1397 [D loss: -0.393496] [G loss: -0.796427]\n",
      "1398 [D loss: -0.580014] [G loss: -0.973835]\n",
      "1399 [D loss: -0.972041] [G loss: -0.922156]\n",
      "1400 [D loss: -0.823036] [G loss: -1.126047]\n",
      "1401 [D loss: 0.235606] [G loss: -1.199930]\n",
      "1402 [D loss: -1.317728] [G loss: -1.018869]\n",
      "1403 [D loss: -0.621596] [G loss: -1.691010]\n",
      "1404 [D loss: -1.201530] [G loss: -1.187837]\n",
      "1405 [D loss: -0.343318] [G loss: -1.373848]\n",
      "1406 [D loss: -0.480054] [G loss: -1.028155]\n",
      "1407 [D loss: -1.206324] [G loss: -1.158337]\n",
      "1408 [D loss: -0.719969] [G loss: -1.594956]\n",
      "1409 [D loss: -0.453141] [G loss: -1.197263]\n",
      "1410 [D loss: -0.284351] [G loss: -1.713887]\n",
      "1411 [D loss: -0.452624] [G loss: -1.332298]\n",
      "1412 [D loss: -0.957301] [G loss: -1.413022]\n",
      "1413 [D loss: -0.762332] [G loss: -1.242107]\n",
      "1414 [D loss: -0.399052] [G loss: -1.470127]\n",
      "1415 [D loss: -0.514327] [G loss: -1.573088]\n",
      "1416 [D loss: -1.080024] [G loss: -1.469215]\n",
      "1417 [D loss: -0.703640] [G loss: -1.663377]\n",
      "1418 [D loss: -0.879920] [G loss: -1.798353]\n",
      "1419 [D loss: -0.632964] [G loss: -1.605015]\n",
      "1420 [D loss: -0.946466] [G loss: -1.811605]\n",
      "1421 [D loss: -0.793200] [G loss: -1.595382]\n",
      "1422 [D loss: -0.579114] [G loss: -1.386420]\n",
      "1423 [D loss: -0.910900] [G loss: -1.601376]\n",
      "1424 [D loss: -0.569286] [G loss: -1.368618]\n",
      "1425 [D loss: -0.891002] [G loss: -1.488079]\n",
      "1426 [D loss: -0.643042] [G loss: -1.428547]\n",
      "1427 [D loss: -0.498254] [G loss: -1.969778]\n",
      "1428 [D loss: -0.998553] [G loss: -1.385921]\n",
      "1429 [D loss: -0.926198] [G loss: -1.245497]\n",
      "1430 [D loss: -1.186839] [G loss: -1.516620]\n",
      "1431 [D loss: -1.664599] [G loss: -0.899430]\n",
      "1432 [D loss: -0.813496] [G loss: -1.560509]\n",
      "1433 [D loss: -0.978632] [G loss: -1.384904]\n",
      "1434 [D loss: -0.286958] [G loss: -1.444756]\n",
      "1435 [D loss: -0.674932] [G loss: -1.618974]\n",
      "1436 [D loss: -0.927408] [G loss: -1.101355]\n",
      "1437 [D loss: -0.079355] [G loss: -1.492550]\n",
      "1438 [D loss: -0.751027] [G loss: -1.573583]\n",
      "1439 [D loss: -0.439984] [G loss: -1.378833]\n",
      "1440 [D loss: -0.795309] [G loss: -1.378123]\n",
      "1441 [D loss: -0.268389] [G loss: -1.826959]\n",
      "1442 [D loss: -0.427231] [G loss: -1.481369]\n",
      "1443 [D loss: -0.884681] [G loss: -1.126423]\n",
      "1444 [D loss: -0.624116] [G loss: -1.315592]\n",
      "1445 [D loss: -0.646323] [G loss: -2.025391]\n",
      "1446 [D loss: -0.391734] [G loss: -1.570720]\n",
      "1447 [D loss: -0.867881] [G loss: -1.801353]\n",
      "1448 [D loss: -0.521930] [G loss: -1.816528]\n",
      "1449 [D loss: -0.778798] [G loss: -1.522878]\n",
      "1450 [D loss: -0.856084] [G loss: -1.610536]\n",
      "1451 [D loss: -1.069387] [G loss: -1.583495]\n",
      "1452 [D loss: -0.167904] [G loss: -1.673124]\n",
      "1453 [D loss: -0.340076] [G loss: -1.564566]\n",
      "1454 [D loss: -1.021246] [G loss: -1.472865]\n",
      "1455 [D loss: -0.819643] [G loss: -1.755237]\n",
      "1456 [D loss: -0.394515] [G loss: -1.893290]\n",
      "1457 [D loss: -1.140656] [G loss: -1.285400]\n",
      "1458 [D loss: -1.342775] [G loss: -1.464995]\n",
      "1459 [D loss: -0.588438] [G loss: -1.719158]\n",
      "1460 [D loss: 0.068298] [G loss: -1.618002]\n",
      "1461 [D loss: -0.535097] [G loss: -1.787277]\n",
      "1462 [D loss: -0.076588] [G loss: -1.275977]\n",
      "1463 [D loss: -0.315323] [G loss: -1.949241]\n",
      "1464 [D loss: -1.035801] [G loss: -2.002055]\n",
      "1465 [D loss: -1.006883] [G loss: -1.743117]\n",
      "1466 [D loss: -0.383165] [G loss: -1.513292]\n",
      "1467 [D loss: -1.170305] [G loss: -1.927509]\n",
      "1468 [D loss: -0.530421] [G loss: -1.480008]\n",
      "1469 [D loss: -0.815458] [G loss: -1.301765]\n",
      "1470 [D loss: -0.584007] [G loss: -1.326390]\n",
      "1471 [D loss: -0.838166] [G loss: -1.840016]\n",
      "1472 [D loss: -0.291601] [G loss: -1.751890]\n",
      "1473 [D loss: -1.090958] [G loss: -1.389041]\n",
      "1474 [D loss: -0.809513] [G loss: -1.712149]\n",
      "1475 [D loss: -1.228081] [G loss: -1.290511]\n",
      "1476 [D loss: -1.184048] [G loss: -1.423656]\n",
      "1477 [D loss: 0.172883] [G loss: -1.317363]\n",
      "1478 [D loss: -0.976611] [G loss: -1.364335]\n",
      "1479 [D loss: -0.504931] [G loss: -1.358789]\n",
      "1480 [D loss: -1.114244] [G loss: -1.255007]\n",
      "1481 [D loss: -0.663633] [G loss: -1.593471]\n",
      "1482 [D loss: -0.561866] [G loss: -1.766325]\n",
      "1483 [D loss: -1.490521] [G loss: -1.590654]\n",
      "1484 [D loss: -0.444933] [G loss: -1.104457]\n",
      "1485 [D loss: -1.060799] [G loss: -1.282683]\n",
      "1486 [D loss: -0.644494] [G loss: -1.404805]\n",
      "1487 [D loss: -0.155539] [G loss: -0.983753]\n",
      "1488 [D loss: -1.233881] [G loss: -1.497859]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1489 [D loss: -1.181655] [G loss: -1.075167]\n",
      "1490 [D loss: -0.891814] [G loss: -1.693891]\n",
      "1491 [D loss: -0.546488] [G loss: -1.876808]\n",
      "1492 [D loss: -0.877470] [G loss: -0.695112]\n",
      "1493 [D loss: -0.987214] [G loss: -1.106535]\n",
      "1494 [D loss: -0.775393] [G loss: -1.280608]\n",
      "1495 [D loss: -1.045232] [G loss: -1.572209]\n",
      "1496 [D loss: -1.134486] [G loss: -1.232468]\n",
      "1497 [D loss: -0.979453] [G loss: -1.307210]\n",
      "1498 [D loss: -0.583753] [G loss: -0.825313]\n",
      "1499 [D loss: -0.698796] [G loss: -1.345503]\n",
      "1500 [D loss: -0.551603] [G loss: -1.340374]\n",
      "1501 [D loss: -1.019949] [G loss: -1.056295]\n",
      "1502 [D loss: -0.780593] [G loss: -1.704847]\n",
      "1503 [D loss: -0.692405] [G loss: -1.416139]\n",
      "1504 [D loss: -0.639911] [G loss: -1.171041]\n",
      "1505 [D loss: -0.511885] [G loss: -1.199240]\n",
      "1506 [D loss: -1.241216] [G loss: -1.848867]\n",
      "1507 [D loss: 0.042408] [G loss: -1.681597]\n",
      "1508 [D loss: -1.287283] [G loss: -1.062534]\n",
      "1509 [D loss: -0.987360] [G loss: -1.102125]\n",
      "1510 [D loss: -0.775799] [G loss: -1.517687]\n",
      "1511 [D loss: -0.046648] [G loss: -1.911425]\n",
      "1512 [D loss: -0.754392] [G loss: -1.513295]\n",
      "1513 [D loss: -1.079239] [G loss: -1.363297]\n",
      "1514 [D loss: -1.109247] [G loss: -1.429676]\n",
      "1515 [D loss: -0.748891] [G loss: -1.737216]\n",
      "1516 [D loss: -0.801193] [G loss: -1.718888]\n",
      "1517 [D loss: -0.354425] [G loss: -1.795834]\n",
      "1518 [D loss: -0.893194] [G loss: -1.395743]\n",
      "1519 [D loss: -0.671794] [G loss: -2.021955]\n",
      "1520 [D loss: -0.828818] [G loss: -1.574574]\n",
      "1521 [D loss: -0.790412] [G loss: -1.641131]\n",
      "1522 [D loss: -1.127922] [G loss: -1.030166]\n",
      "1523 [D loss: -0.568095] [G loss: -1.683041]\n",
      "1524 [D loss: -0.683888] [G loss: -1.041786]\n",
      "1525 [D loss: -0.940517] [G loss: -1.103510]\n",
      "1526 [D loss: -0.263757] [G loss: -1.584317]\n",
      "1527 [D loss: -1.268586] [G loss: -1.545886]\n",
      "1528 [D loss: -0.758433] [G loss: -1.962892]\n",
      "1529 [D loss: -0.503923] [G loss: -1.906773]\n",
      "1530 [D loss: -1.210756] [G loss: -1.717422]\n",
      "1531 [D loss: -1.030353] [G loss: -1.808620]\n",
      "1532 [D loss: -1.455281] [G loss: -1.658339]\n",
      "1533 [D loss: -0.547400] [G loss: -1.910483]\n",
      "1534 [D loss: -0.167812] [G loss: -1.377622]\n",
      "1535 [D loss: -0.390297] [G loss: -1.729403]\n",
      "1536 [D loss: -0.457508] [G loss: -1.268505]\n",
      "1537 [D loss: -0.843777] [G loss: -2.052822]\n",
      "1538 [D loss: -0.211211] [G loss: -1.783138]\n",
      "1539 [D loss: -0.055839] [G loss: -1.832935]\n",
      "1540 [D loss: -0.973683] [G loss: -2.063651]\n",
      "1541 [D loss: -0.676124] [G loss: -1.862748]\n",
      "1542 [D loss: -0.566633] [G loss: -1.957880]\n",
      "1543 [D loss: -0.868800] [G loss: -1.739371]\n",
      "1544 [D loss: -1.313968] [G loss: -1.577471]\n",
      "1545 [D loss: -1.291458] [G loss: -1.609418]\n",
      "1546 [D loss: -0.703894] [G loss: -1.732600]\n",
      "1547 [D loss: -0.352597] [G loss: -1.400754]\n",
      "1548 [D loss: -0.809205] [G loss: -1.745438]\n",
      "1549 [D loss: -0.869713] [G loss: -1.641311]\n",
      "1550 [D loss: -0.531463] [G loss: -1.671905]\n",
      "1551 [D loss: -1.696164] [G loss: -2.096844]\n",
      "1552 [D loss: -0.345538] [G loss: -2.145538]\n",
      "1553 [D loss: -0.800593] [G loss: -1.919159]\n",
      "1554 [D loss: -0.634778] [G loss: -2.015544]\n",
      "1555 [D loss: -0.978294] [G loss: -1.935036]\n",
      "1556 [D loss: -0.577902] [G loss: -1.639423]\n",
      "1557 [D loss: -0.619181] [G loss: -1.539457]\n",
      "1558 [D loss: -1.481612] [G loss: -1.282671]\n",
      "1559 [D loss: -0.129473] [G loss: -1.639816]\n",
      "1560 [D loss: -1.096169] [G loss: -1.294839]\n",
      "1561 [D loss: -0.624715] [G loss: -1.968546]\n",
      "1562 [D loss: -0.336041] [G loss: -1.714258]\n",
      "1563 [D loss: -0.563864] [G loss: -1.517599]\n",
      "1564 [D loss: -0.656251] [G loss: -1.782383]\n",
      "1565 [D loss: -0.792807] [G loss: -1.975377]\n",
      "1566 [D loss: -0.414704] [G loss: -1.799642]\n",
      "1567 [D loss: -0.473135] [G loss: -1.843080]\n",
      "1568 [D loss: -0.260395] [G loss: -1.764255]\n",
      "1569 [D loss: -0.596846] [G loss: -1.450642]\n",
      "1570 [D loss: -0.495061] [G loss: -1.310345]\n",
      "1571 [D loss: -0.113287] [G loss: -1.568936]\n",
      "1572 [D loss: -0.268002] [G loss: -1.409477]\n",
      "1573 [D loss: -0.906055] [G loss: -1.888029]\n",
      "1574 [D loss: -0.511498] [G loss: -1.742609]\n",
      "1575 [D loss: -0.889828] [G loss: -1.360821]\n",
      "1576 [D loss: -1.277927] [G loss: -2.192101]\n",
      "1577 [D loss: -0.739682] [G loss: -1.736995]\n",
      "1578 [D loss: -1.010637] [G loss: -1.826365]\n",
      "1579 [D loss: -0.429644] [G loss: -1.620116]\n",
      "1580 [D loss: -1.076173] [G loss: -2.224780]\n",
      "1581 [D loss: -0.763376] [G loss: -1.494266]\n",
      "1582 [D loss: -0.708986] [G loss: -1.696701]\n",
      "1583 [D loss: -1.072617] [G loss: -1.298494]\n",
      "1584 [D loss: -1.428430] [G loss: -1.041605]\n",
      "1585 [D loss: -0.680436] [G loss: -1.350816]\n",
      "1586 [D loss: -0.416329] [G loss: -1.312540]\n",
      "1587 [D loss: -0.902388] [G loss: -1.723421]\n",
      "1588 [D loss: -0.814489] [G loss: -1.562584]\n",
      "1589 [D loss: -1.064046] [G loss: -1.512484]\n",
      "1590 [D loss: -0.384327] [G loss: -1.440333]\n",
      "1591 [D loss: -0.778097] [G loss: -1.626984]\n",
      "1592 [D loss: -0.694867] [G loss: -1.507209]\n",
      "1593 [D loss: -0.751128] [G loss: -1.497747]\n",
      "1594 [D loss: -1.030278] [G loss: -1.205626]\n",
      "1595 [D loss: -0.591398] [G loss: -1.961794]\n",
      "1596 [D loss: -0.205876] [G loss: -1.909439]\n",
      "1597 [D loss: -0.978034] [G loss: -1.310145]\n",
      "1598 [D loss: -0.839018] [G loss: -1.412741]\n",
      "1599 [D loss: -0.639302] [G loss: -1.645316]\n",
      "1600 [D loss: -0.580242] [G loss: -1.549729]\n",
      "1601 [D loss: -1.014136] [G loss: -1.563948]\n",
      "1602 [D loss: -0.931830] [G loss: -1.360602]\n",
      "1603 [D loss: -0.863129] [G loss: -2.162453]\n",
      "1604 [D loss: -1.077077] [G loss: -1.528927]\n",
      "1605 [D loss: -0.965063] [G loss: -1.714680]\n",
      "1606 [D loss: -0.390844] [G loss: -2.127795]\n",
      "1607 [D loss: -0.580770] [G loss: -1.923702]\n",
      "1608 [D loss: -0.871423] [G loss: -1.479073]\n",
      "1609 [D loss: 0.070276] [G loss: -1.970225]\n",
      "1610 [D loss: -0.415849] [G loss: -1.815866]\n",
      "1611 [D loss: -0.373112] [G loss: -1.712868]\n",
      "1612 [D loss: -0.602206] [G loss: -1.661438]\n",
      "1613 [D loss: -0.722373] [G loss: -1.947194]\n",
      "1614 [D loss: -0.138641] [G loss: -2.003686]\n",
      "1615 [D loss: -0.995073] [G loss: -2.164752]\n",
      "1616 [D loss: 0.041085] [G loss: -2.344312]\n",
      "1617 [D loss: -0.475058] [G loss: -1.931102]\n",
      "1618 [D loss: -0.920535] [G loss: -1.750359]\n",
      "1619 [D loss: -0.653204] [G loss: -2.139825]\n",
      "1620 [D loss: -0.358337] [G loss: -1.684720]\n",
      "1621 [D loss: -0.445081] [G loss: -1.618222]\n",
      "1622 [D loss: -0.759658] [G loss: -1.536163]\n",
      "1623 [D loss: -0.799408] [G loss: -1.589453]\n",
      "1624 [D loss: -1.048246] [G loss: -1.670590]\n",
      "1625 [D loss: -1.078588] [G loss: -1.479343]\n",
      "1626 [D loss: -0.682120] [G loss: -1.700167]\n",
      "1627 [D loss: -0.674423] [G loss: -1.852527]\n",
      "1628 [D loss: -0.386227] [G loss: -1.711713]\n",
      "1629 [D loss: -0.687223] [G loss: -1.466484]\n",
      "1630 [D loss: -1.228227] [G loss: -2.142218]\n",
      "1631 [D loss: -1.032026] [G loss: -1.400803]\n",
      "1632 [D loss: -0.917882] [G loss: -1.907598]\n",
      "1633 [D loss: -0.440251] [G loss: -1.577341]\n",
      "1634 [D loss: -0.715631] [G loss: -2.325193]\n",
      "1635 [D loss: 0.003250] [G loss: -1.822577]\n",
      "1636 [D loss: -0.427899] [G loss: -2.103482]\n",
      "1637 [D loss: -0.354957] [G loss: -1.830093]\n",
      "1638 [D loss: -0.833894] [G loss: -1.854095]\n",
      "1639 [D loss: -0.622007] [G loss: -1.781646]\n",
      "1640 [D loss: -0.157779] [G loss: -1.708340]\n",
      "1641 [D loss: -0.494208] [G loss: -1.605475]\n",
      "1642 [D loss: -1.239231] [G loss: -2.354566]\n",
      "1643 [D loss: -0.747026] [G loss: -1.582511]\n",
      "1644 [D loss: 0.043837] [G loss: -2.349480]\n",
      "1645 [D loss: -0.507575] [G loss: -1.936229]\n",
      "1646 [D loss: -0.014990] [G loss: -1.971203]\n",
      "1647 [D loss: -0.468720] [G loss: -2.340375]\n",
      "1648 [D loss: -0.853690] [G loss: -2.114078]\n",
      "1649 [D loss: -0.215739] [G loss: -2.161586]\n",
      "1650 [D loss: -0.755986] [G loss: -2.600811]\n",
      "1651 [D loss: -1.177597] [G loss: -1.584829]\n",
      "1652 [D loss: -1.372146] [G loss: -1.397473]\n",
      "1653 [D loss: -0.369656] [G loss: -1.567080]\n",
      "1654 [D loss: -0.160480] [G loss: -2.384650]\n",
      "1655 [D loss: -0.548265] [G loss: -2.040718]\n",
      "1656 [D loss: -1.260892] [G loss: -1.852605]\n",
      "1657 [D loss: -0.794180] [G loss: -1.416357]\n",
      "1658 [D loss: -0.378067] [G loss: -1.576868]\n",
      "1659 [D loss: -1.492819] [G loss: -2.388875]\n",
      "1660 [D loss: -0.945533] [G loss: -1.550027]\n",
      "1661 [D loss: 0.205599] [G loss: -1.988087]\n",
      "1662 [D loss: -0.867789] [G loss: -2.273130]\n",
      "1663 [D loss: -0.693828] [G loss: -2.120397]\n",
      "1664 [D loss: -1.382168] [G loss: -1.193663]\n",
      "1665 [D loss: -0.939781] [G loss: -1.503963]\n",
      "1666 [D loss: -0.415039] [G loss: -1.711088]\n",
      "1667 [D loss: -0.917947] [G loss: -2.764897]\n",
      "1668 [D loss: -0.935614] [G loss: -2.038785]\n",
      "1669 [D loss: -0.089505] [G loss: -1.750656]\n",
      "1670 [D loss: -0.405394] [G loss: -1.783165]\n",
      "1671 [D loss: -0.685535] [G loss: -1.790134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1672 [D loss: -0.622559] [G loss: -1.861017]\n",
      "1673 [D loss: -1.245827] [G loss: -1.738809]\n",
      "1674 [D loss: 0.114097] [G loss: -1.490810]\n",
      "1675 [D loss: 0.373652] [G loss: -2.057019]\n",
      "1676 [D loss: -0.700028] [G loss: -1.732629]\n",
      "1677 [D loss: -1.358112] [G loss: -1.676101]\n",
      "1678 [D loss: -0.591870] [G loss: -1.522661]\n",
      "1679 [D loss: -0.810456] [G loss: -1.797536]\n",
      "1680 [D loss: -0.162743] [G loss: -1.974154]\n",
      "1681 [D loss: -0.499246] [G loss: -1.804046]\n",
      "1682 [D loss: -0.926766] [G loss: -1.347299]\n",
      "1683 [D loss: -0.259621] [G loss: -1.851345]\n",
      "1684 [D loss: -0.509387] [G loss: -1.856544]\n",
      "1685 [D loss: -0.293045] [G loss: -2.159860]\n",
      "1686 [D loss: -0.672155] [G loss: -1.755100]\n",
      "1687 [D loss: -0.791084] [G loss: -1.210799]\n",
      "1688 [D loss: -0.829979] [G loss: -1.940119]\n",
      "1689 [D loss: -0.716766] [G loss: -1.584898]\n",
      "1690 [D loss: -0.106602] [G loss: -1.929594]\n",
      "1691 [D loss: -0.669605] [G loss: -1.830287]\n",
      "1692 [D loss: -0.194003] [G loss: -1.694161]\n",
      "1693 [D loss: -0.813217] [G loss: -2.150539]\n",
      "1694 [D loss: -0.427016] [G loss: -1.891976]\n",
      "1695 [D loss: -1.107198] [G loss: -2.467817]\n",
      "1696 [D loss: -0.145996] [G loss: -1.967122]\n",
      "1697 [D loss: -0.399909] [G loss: -2.096167]\n",
      "1698 [D loss: -0.829794] [G loss: -1.762959]\n",
      "1699 [D loss: -0.568513] [G loss: -1.906516]\n",
      "1700 [D loss: -1.085922] [G loss: -1.954373]\n",
      "1701 [D loss: -0.292560] [G loss: -2.334998]\n",
      "1702 [D loss: -0.588526] [G loss: -2.135714]\n",
      "1703 [D loss: -0.324802] [G loss: -1.890628]\n",
      "1704 [D loss: -1.144599] [G loss: -2.002979]\n",
      "1705 [D loss: -0.894849] [G loss: -2.206294]\n",
      "1706 [D loss: -0.554866] [G loss: -2.275680]\n",
      "1707 [D loss: -0.827555] [G loss: -1.805282]\n",
      "1708 [D loss: -0.625026] [G loss: -2.028681]\n",
      "1709 [D loss: -0.318079] [G loss: -2.097955]\n",
      "1710 [D loss: -0.186023] [G loss: -2.195169]\n",
      "1711 [D loss: -1.037398] [G loss: -2.292802]\n",
      "1712 [D loss: -0.877717] [G loss: -2.267794]\n",
      "1713 [D loss: -0.687563] [G loss: -2.223086]\n",
      "1714 [D loss: -0.611258] [G loss: -1.883654]\n",
      "1715 [D loss: -0.894588] [G loss: -2.256818]\n",
      "1716 [D loss: -0.139295] [G loss: -2.375602]\n",
      "1717 [D loss: -0.389187] [G loss: -1.802220]\n",
      "1718 [D loss: -0.435343] [G loss: -2.140425]\n",
      "1719 [D loss: -0.600380] [G loss: -2.086606]\n",
      "1720 [D loss: -0.199241] [G loss: -1.890615]\n",
      "1721 [D loss: -0.349296] [G loss: -2.024388]\n",
      "1722 [D loss: -0.447793] [G loss: -2.217038]\n",
      "1723 [D loss: -0.660303] [G loss: -1.840139]\n",
      "1724 [D loss: -1.343232] [G loss: -2.086746]\n",
      "1725 [D loss: 0.141265] [G loss: -2.040755]\n",
      "1726 [D loss: -0.871763] [G loss: -1.900382]\n",
      "1727 [D loss: -0.919562] [G loss: -2.134872]\n",
      "1728 [D loss: -0.430855] [G loss: -2.230461]\n",
      "1729 [D loss: -0.274700] [G loss: -1.869484]\n",
      "1730 [D loss: -1.051208] [G loss: -1.907970]\n",
      "1731 [D loss: -0.155454] [G loss: -2.066874]\n",
      "1732 [D loss: -0.682563] [G loss: -2.219210]\n",
      "1733 [D loss: -0.520896] [G loss: -1.666235]\n",
      "1734 [D loss: -0.772538] [G loss: -2.024691]\n",
      "1735 [D loss: -0.476443] [G loss: -2.214988]\n",
      "1736 [D loss: -0.339955] [G loss: -1.969898]\n",
      "1737 [D loss: -0.488698] [G loss: -2.132750]\n",
      "1738 [D loss: -0.550300] [G loss: -1.869695]\n",
      "1739 [D loss: -0.425218] [G loss: -2.300343]\n",
      "1740 [D loss: 0.019558] [G loss: -2.445502]\n",
      "1741 [D loss: -0.755007] [G loss: -1.646255]\n",
      "1742 [D loss: -0.863304] [G loss: -2.263637]\n",
      "1743 [D loss: -0.819745] [G loss: -1.608259]\n",
      "1744 [D loss: -0.787702] [G loss: -2.125241]\n",
      "1745 [D loss: -0.338205] [G loss: -2.062044]\n",
      "1746 [D loss: -0.790042] [G loss: -2.447487]\n",
      "1747 [D loss: -0.925548] [G loss: -2.183029]\n",
      "1748 [D loss: -1.064077] [G loss: -2.428512]\n",
      "1749 [D loss: -0.398315] [G loss: -2.366539]\n",
      "1750 [D loss: -0.492541] [G loss: -2.239499]\n",
      "1751 [D loss: -1.100239] [G loss: -2.196138]\n",
      "1752 [D loss: -0.425395] [G loss: -2.145906]\n",
      "1753 [D loss: -0.206405] [G loss: -2.440724]\n",
      "1754 [D loss: -0.258639] [G loss: -2.318838]\n",
      "1755 [D loss: -1.158142] [G loss: -1.812968]\n",
      "1756 [D loss: -0.282579] [G loss: -2.358588]\n",
      "1757 [D loss: -1.171098] [G loss: -2.015084]\n",
      "1758 [D loss: -1.245609] [G loss: -2.182124]\n",
      "1759 [D loss: -0.528766] [G loss: -2.219329]\n",
      "1760 [D loss: -0.739260] [G loss: -2.247583]\n",
      "1761 [D loss: -0.055903] [G loss: -2.617087]\n",
      "1762 [D loss: -1.120877] [G loss: -2.480917]\n",
      "1763 [D loss: -0.385310] [G loss: -2.167281]\n",
      "1764 [D loss: -0.467075] [G loss: -2.102479]\n",
      "1765 [D loss: -0.713249] [G loss: -1.883546]\n",
      "1766 [D loss: -0.725218] [G loss: -2.590296]\n",
      "1767 [D loss: -0.687200] [G loss: -2.247077]\n",
      "1768 [D loss: -0.659665] [G loss: -1.993544]\n",
      "1769 [D loss: -0.230735] [G loss: -1.891897]\n",
      "1770 [D loss: -0.365031] [G loss: -2.176979]\n",
      "1771 [D loss: -1.456362] [G loss: -1.959199]\n",
      "1772 [D loss: -0.895745] [G loss: -2.391484]\n",
      "1773 [D loss: -0.247177] [G loss: -2.477357]\n",
      "1774 [D loss: 0.081760] [G loss: -2.191923]\n",
      "1775 [D loss: -1.293156] [G loss: -1.840561]\n",
      "1776 [D loss: -0.664510] [G loss: -2.151155]\n",
      "1777 [D loss: -0.635524] [G loss: -1.988678]\n",
      "1778 [D loss: -0.486118] [G loss: -2.097795]\n",
      "1779 [D loss: -0.573989] [G loss: -1.904135]\n",
      "1780 [D loss: -0.818476] [G loss: -2.249878]\n",
      "1781 [D loss: -0.599091] [G loss: -2.044619]\n",
      "1782 [D loss: -0.441523] [G loss: -2.681135]\n",
      "1783 [D loss: -1.080945] [G loss: -2.100021]\n",
      "1784 [D loss: -0.037082] [G loss: -2.427650]\n",
      "1785 [D loss: -0.097194] [G loss: -2.485243]\n",
      "1786 [D loss: -0.388111] [G loss: -2.421355]\n",
      "1787 [D loss: -0.913965] [G loss: -2.116350]\n",
      "1788 [D loss: -0.789936] [G loss: -2.214921]\n",
      "1789 [D loss: -0.451954] [G loss: -1.848710]\n",
      "1790 [D loss: -0.687357] [G loss: -2.314454]\n",
      "1791 [D loss: -0.567072] [G loss: -1.706527]\n",
      "1792 [D loss: -0.168172] [G loss: -2.683409]\n",
      "1793 [D loss: -1.028144] [G loss: -1.852428]\n",
      "1794 [D loss: -0.355451] [G loss: -1.771850]\n",
      "1795 [D loss: -0.216576] [G loss: -2.609935]\n",
      "1796 [D loss: -0.259867] [G loss: -2.261712]\n",
      "1797 [D loss: -0.435411] [G loss: -2.061548]\n",
      "1798 [D loss: -0.078547] [G loss: -2.577479]\n",
      "1799 [D loss: -0.131346] [G loss: -2.523415]\n",
      "1800 [D loss: -0.986310] [G loss: -1.950770]\n",
      "1801 [D loss: 0.013902] [G loss: -2.617017]\n",
      "1802 [D loss: -0.953972] [G loss: -2.994008]\n",
      "1803 [D loss: -0.818721] [G loss: -2.095687]\n",
      "1804 [D loss: -0.587725] [G loss: -2.430969]\n",
      "1805 [D loss: 0.113361] [G loss: -2.468272]\n",
      "1806 [D loss: -0.784332] [G loss: -2.820598]\n",
      "1807 [D loss: -0.266630] [G loss: -2.261848]\n",
      "1808 [D loss: -1.006044] [G loss: -2.359874]\n",
      "1809 [D loss: -0.769920] [G loss: -2.827022]\n",
      "1810 [D loss: -0.116718] [G loss: -2.516040]\n",
      "1811 [D loss: -0.684181] [G loss: -2.658382]\n",
      "1812 [D loss: -0.812946] [G loss: -2.783674]\n",
      "1813 [D loss: -0.550418] [G loss: -2.429645]\n",
      "1814 [D loss: -0.876837] [G loss: -2.708056]\n",
      "1815 [D loss: -0.849226] [G loss: -2.280478]\n",
      "1816 [D loss: -0.949038] [G loss: -2.648449]\n",
      "1817 [D loss: -0.514071] [G loss: -2.635746]\n",
      "1818 [D loss: -0.327357] [G loss: -2.774195]\n",
      "1819 [D loss: -1.590372] [G loss: -2.293922]\n",
      "1820 [D loss: 0.103791] [G loss: -2.702227]\n",
      "1821 [D loss: -0.646173] [G loss: -2.331440]\n",
      "1822 [D loss: -0.863414] [G loss: -2.177538]\n",
      "1823 [D loss: -0.646170] [G loss: -2.120618]\n",
      "1824 [D loss: -0.039342] [G loss: -2.536530]\n",
      "1825 [D loss: -0.025872] [G loss: -2.831156]\n",
      "1826 [D loss: -0.322821] [G loss: -2.256260]\n",
      "1827 [D loss: -0.286417] [G loss: -2.746831]\n",
      "1828 [D loss: -1.076970] [G loss: -2.596986]\n",
      "1829 [D loss: -0.936160] [G loss: -2.088065]\n",
      "1830 [D loss: -0.552719] [G loss: -2.567409]\n",
      "1831 [D loss: -0.387845] [G loss: -2.742332]\n",
      "1832 [D loss: -0.791460] [G loss: -2.475833]\n",
      "1833 [D loss: -1.060621] [G loss: -2.600096]\n",
      "1834 [D loss: -0.364216] [G loss: -2.351050]\n",
      "1835 [D loss: -0.759845] [G loss: -1.927617]\n",
      "1836 [D loss: 0.023561] [G loss: -2.915991]\n",
      "1837 [D loss: -1.038278] [G loss: -2.268101]\n",
      "1838 [D loss: -0.304224] [G loss: -2.366912]\n",
      "1839 [D loss: -0.849993] [G loss: -2.530732]\n",
      "1840 [D loss: -0.509617] [G loss: -2.545854]\n",
      "1841 [D loss: 0.207174] [G loss: -2.314815]\n",
      "1842 [D loss: -0.648036] [G loss: -2.183198]\n",
      "1843 [D loss: -0.270208] [G loss: -2.072057]\n",
      "1844 [D loss: -0.539772] [G loss: -1.839980]\n",
      "1845 [D loss: 0.178142] [G loss: -2.445611]\n",
      "1846 [D loss: -0.724201] [G loss: -1.895771]\n",
      "1847 [D loss: -0.703259] [G loss: -1.735618]\n",
      "1848 [D loss: -0.382281] [G loss: -1.603695]\n",
      "1849 [D loss: -0.963723] [G loss: -2.590364]\n",
      "1850 [D loss: -0.562543] [G loss: -2.374631]\n",
      "1851 [D loss: -0.258677] [G loss: -1.735750]\n",
      "1852 [D loss: -1.337693] [G loss: -2.023173]\n",
      "1853 [D loss: -0.478050] [G loss: -1.567427]\n",
      "1854 [D loss: -0.748872] [G loss: -1.827943]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1855 [D loss: -0.988637] [G loss: -2.113969]\n",
      "1856 [D loss: -0.776804] [G loss: -1.919339]\n",
      "1857 [D loss: -0.382754] [G loss: -1.930172]\n",
      "1858 [D loss: -0.087768] [G loss: -1.766867]\n",
      "1859 [D loss: -0.054947] [G loss: -1.738984]\n",
      "1860 [D loss: -0.879640] [G loss: -2.174910]\n",
      "1861 [D loss: -0.762565] [G loss: -1.892388]\n",
      "1862 [D loss: -0.249053] [G loss: -2.037551]\n",
      "1863 [D loss: -0.352670] [G loss: -1.759832]\n",
      "1864 [D loss: 0.000738] [G loss: -2.325510]\n",
      "1865 [D loss: -1.139810] [G loss: -2.285292]\n",
      "1866 [D loss: -1.379116] [G loss: -1.826293]\n",
      "1867 [D loss: -0.298641] [G loss: -1.641609]\n",
      "1868 [D loss: -0.753340] [G loss: -1.957334]\n",
      "1869 [D loss: -0.562584] [G loss: -1.513638]\n",
      "1870 [D loss: -0.934779] [G loss: -1.730546]\n",
      "1871 [D loss: -0.364630] [G loss: -1.969151]\n",
      "1872 [D loss: -1.301138] [G loss: -1.515683]\n",
      "1873 [D loss: -0.818116] [G loss: -2.052368]\n",
      "1874 [D loss: -0.604533] [G loss: -1.818001]\n",
      "1875 [D loss: -0.311011] [G loss: -1.861258]\n",
      "1876 [D loss: -0.365949] [G loss: -1.939403]\n",
      "1877 [D loss: -0.324607] [G loss: -1.896230]\n",
      "1878 [D loss: -0.746440] [G loss: -1.810246]\n",
      "1879 [D loss: -0.545042] [G loss: -2.385035]\n",
      "1880 [D loss: -0.464776] [G loss: -2.102559]\n",
      "1881 [D loss: -0.497867] [G loss: -1.895794]\n",
      "1882 [D loss: -0.448072] [G loss: -1.119375]\n",
      "1883 [D loss: -0.196469] [G loss: -2.384189]\n",
      "1884 [D loss: -0.432814] [G loss: -2.333647]\n",
      "1885 [D loss: -0.344864] [G loss: -2.329717]\n",
      "1886 [D loss: -0.219139] [G loss: -1.906515]\n",
      "1887 [D loss: -0.584498] [G loss: -2.246198]\n",
      "1888 [D loss: -0.227446] [G loss: -2.906528]\n",
      "1889 [D loss: -1.543492] [G loss: -2.347722]\n",
      "1890 [D loss: -0.678440] [G loss: -2.167136]\n",
      "1891 [D loss: -1.002513] [G loss: -2.440154]\n",
      "1892 [D loss: -0.623990] [G loss: -2.329266]\n",
      "1893 [D loss: -0.714854] [G loss: -2.005760]\n",
      "1894 [D loss: -0.481374] [G loss: -2.644589]\n",
      "1895 [D loss: -0.592465] [G loss: -2.600742]\n",
      "1896 [D loss: -0.563022] [G loss: -2.281476]\n",
      "1897 [D loss: -0.577435] [G loss: -2.594486]\n",
      "1898 [D loss: 0.376826] [G loss: -2.380157]\n",
      "1899 [D loss: -0.676033] [G loss: -2.296064]\n",
      "1900 [D loss: -0.589752] [G loss: -2.324938]\n",
      "1901 [D loss: -0.234903] [G loss: -2.590586]\n",
      "1902 [D loss: -0.582520] [G loss: -2.575538]\n",
      "1903 [D loss: -0.323497] [G loss: -2.023113]\n",
      "1904 [D loss: -0.421516] [G loss: -2.589511]\n",
      "1905 [D loss: -0.602088] [G loss: -1.865782]\n",
      "1906 [D loss: -0.081410] [G loss: -2.746885]\n",
      "1907 [D loss: -0.734731] [G loss: -2.374867]\n",
      "1908 [D loss: -0.992584] [G loss: -2.408443]\n",
      "1909 [D loss: -0.549639] [G loss: -2.491822]\n",
      "1910 [D loss: -0.113869] [G loss: -2.301812]\n",
      "1911 [D loss: -0.581009] [G loss: -2.519449]\n",
      "1912 [D loss: -0.299705] [G loss: -1.773824]\n",
      "1913 [D loss: -0.263167] [G loss: -2.284693]\n",
      "1914 [D loss: -0.731652] [G loss: -2.483431]\n",
      "1915 [D loss: 0.416824] [G loss: -2.086675]\n",
      "1916 [D loss: -0.717942] [G loss: -2.574094]\n",
      "1917 [D loss: -0.583744] [G loss: -2.197165]\n",
      "1918 [D loss: -0.615550] [G loss: -2.131054]\n",
      "1919 [D loss: -0.146132] [G loss: -2.076973]\n",
      "1920 [D loss: -0.847010] [G loss: -2.009918]\n",
      "1921 [D loss: -0.956438] [G loss: -1.955285]\n",
      "1922 [D loss: -0.690351] [G loss: -2.256204]\n",
      "1923 [D loss: -0.800903] [G loss: -2.543182]\n",
      "1924 [D loss: -0.150163] [G loss: -2.509999]\n",
      "1925 [D loss: -0.951296] [G loss: -2.399632]\n",
      "1926 [D loss: -0.132788] [G loss: -2.330551]\n",
      "1927 [D loss: -0.551190] [G loss: -1.847778]\n",
      "1928 [D loss: -0.386411] [G loss: -2.624178]\n",
      "1929 [D loss: -0.058754] [G loss: -2.492650]\n",
      "1930 [D loss: -0.808122] [G loss: -2.012418]\n",
      "1931 [D loss: 0.007161] [G loss: -2.341686]\n",
      "1932 [D loss: -0.920093] [G loss: -2.449669]\n",
      "1933 [D loss: -0.540062] [G loss: -2.177050]\n",
      "1934 [D loss: -0.321629] [G loss: -2.466385]\n",
      "1935 [D loss: -0.484415] [G loss: -2.442237]\n",
      "1936 [D loss: -0.777829] [G loss: -2.507118]\n",
      "1937 [D loss: -0.725336] [G loss: -2.880605]\n",
      "1938 [D loss: -0.314200] [G loss: -2.282539]\n",
      "1939 [D loss: -0.712376] [G loss: -2.062420]\n",
      "1940 [D loss: -0.450014] [G loss: -2.183335]\n",
      "1941 [D loss: -0.826424] [G loss: -2.175092]\n",
      "1942 [D loss: -0.517181] [G loss: -2.293931]\n",
      "1943 [D loss: -0.912292] [G loss: -1.865315]\n",
      "1944 [D loss: -1.066386] [G loss: -2.719216]\n",
      "1945 [D loss: -0.770908] [G loss: -2.286346]\n",
      "1946 [D loss: -0.674805] [G loss: -2.114736]\n",
      "1947 [D loss: -0.546026] [G loss: -2.450824]\n",
      "1948 [D loss: 0.124665] [G loss: -2.444005]\n",
      "1949 [D loss: -0.388291] [G loss: -2.605233]\n",
      "1950 [D loss: -0.408284] [G loss: -2.331407]\n",
      "1951 [D loss: -0.528207] [G loss: -2.339572]\n",
      "1952 [D loss: -0.945368] [G loss: -2.411388]\n",
      "1953 [D loss: -0.481011] [G loss: -2.727682]\n",
      "1954 [D loss: -0.214764] [G loss: -2.485827]\n",
      "1955 [D loss: -0.931210] [G loss: -3.215574]\n",
      "1956 [D loss: -0.372939] [G loss: -2.660652]\n",
      "1957 [D loss: -0.658987] [G loss: -2.390319]\n",
      "1958 [D loss: -0.714985] [G loss: -2.632604]\n",
      "1959 [D loss: -0.477433] [G loss: -2.270478]\n",
      "1960 [D loss: -0.519945] [G loss: -2.490885]\n",
      "1961 [D loss: -0.093177] [G loss: -2.399871]\n",
      "1962 [D loss: -0.722947] [G loss: -2.155822]\n",
      "1963 [D loss: -0.567642] [G loss: -2.740392]\n",
      "1964 [D loss: -0.485442] [G loss: -2.362851]\n",
      "1965 [D loss: -0.378398] [G loss: -2.512282]\n",
      "1966 [D loss: -1.285754] [G loss: -1.621375]\n",
      "1967 [D loss: -0.868665] [G loss: -2.147757]\n",
      "1968 [D loss: -0.584586] [G loss: -2.337380]\n",
      "1969 [D loss: -0.135259] [G loss: -2.098536]\n",
      "1970 [D loss: -1.507402] [G loss: -1.770713]\n",
      "1971 [D loss: -0.581326] [G loss: -2.248034]\n",
      "1972 [D loss: -0.089274] [G loss: -2.291718]\n",
      "1973 [D loss: -0.290528] [G loss: -2.251151]\n",
      "1974 [D loss: -0.846707] [G loss: -2.548357]\n",
      "1975 [D loss: -0.942985] [G loss: -2.358232]\n",
      "1976 [D loss: -0.361566] [G loss: -2.298480]\n",
      "1977 [D loss: -0.286710] [G loss: -2.585719]\n",
      "1978 [D loss: -0.942838] [G loss: -2.223208]\n",
      "1979 [D loss: -0.487793] [G loss: -2.959420]\n",
      "1980 [D loss: -0.512141] [G loss: -2.763470]\n",
      "1981 [D loss: -0.521699] [G loss: -2.013123]\n",
      "1982 [D loss: -0.699831] [G loss: -2.316103]\n",
      "1983 [D loss: -0.319178] [G loss: -2.179814]\n",
      "1984 [D loss: -0.410591] [G loss: -2.184525]\n",
      "1985 [D loss: -1.118133] [G loss: -2.426978]\n",
      "1986 [D loss: -0.803689] [G loss: -2.475170]\n",
      "1987 [D loss: -0.309826] [G loss: -2.836137]\n",
      "1988 [D loss: -0.495903] [G loss: -2.573400]\n",
      "1989 [D loss: -0.704011] [G loss: -2.203095]\n",
      "1990 [D loss: -0.260805] [G loss: -2.378467]\n",
      "1991 [D loss: -0.959470] [G loss: -2.558953]\n",
      "1992 [D loss: -0.753717] [G loss: -2.032893]\n",
      "1993 [D loss: -0.390134] [G loss: -2.533855]\n",
      "1994 [D loss: -0.685903] [G loss: -2.323614]\n",
      "1995 [D loss: -0.012373] [G loss: -2.113576]\n",
      "1996 [D loss: -1.135378] [G loss: -2.545840]\n",
      "1997 [D loss: -0.830898] [G loss: -2.049464]\n",
      "1998 [D loss: -0.746920] [G loss: -2.516430]\n",
      "1999 [D loss: -1.857798] [G loss: -1.994536]\n",
      "2000 [D loss: -0.586697] [G loss: -2.237032]\n",
      "2001 [D loss: -0.061108] [G loss: -2.623039]\n",
      "2002 [D loss: -0.399970] [G loss: -1.749651]\n",
      "2003 [D loss: -0.011449] [G loss: -2.032333]\n",
      "2004 [D loss: -0.911081] [G loss: -2.221618]\n",
      "2005 [D loss: -0.444403] [G loss: -1.918850]\n",
      "2006 [D loss: -0.092838] [G loss: -2.279890]\n",
      "2007 [D loss: -0.136416] [G loss: -2.183842]\n",
      "2008 [D loss: -1.247995] [G loss: -2.797759]\n",
      "2009 [D loss: -0.355951] [G loss: -2.399334]\n",
      "2010 [D loss: -0.552233] [G loss: -2.610674]\n",
      "2011 [D loss: -0.842275] [G loss: -2.534938]\n",
      "2012 [D loss: -0.641230] [G loss: -2.952250]\n",
      "2013 [D loss: -0.348826] [G loss: -2.522755]\n",
      "2014 [D loss: -0.220714] [G loss: -2.522258]\n",
      "2015 [D loss: -0.651263] [G loss: -2.249643]\n",
      "2016 [D loss: 0.114701] [G loss: -2.629485]\n",
      "2017 [D loss: -0.419418] [G loss: -2.866725]\n",
      "2018 [D loss: -0.285848] [G loss: -2.294199]\n",
      "2019 [D loss: -1.058230] [G loss: -2.506919]\n",
      "2020 [D loss: -0.883975] [G loss: -2.525919]\n",
      "2021 [D loss: -0.744228] [G loss: -2.395943]\n",
      "2022 [D loss: 0.031424] [G loss: -2.061688]\n",
      "2023 [D loss: -0.023074] [G loss: -2.480316]\n",
      "2024 [D loss: -0.325273] [G loss: -2.752901]\n",
      "2025 [D loss: -0.748653] [G loss: -2.502068]\n",
      "2026 [D loss: -0.461789] [G loss: -2.200514]\n",
      "2027 [D loss: -0.843943] [G loss: -2.884465]\n",
      "2028 [D loss: -0.429400] [G loss: -2.687433]\n",
      "2029 [D loss: -1.291610] [G loss: -2.857207]\n",
      "2030 [D loss: -1.036390] [G loss: -2.854542]\n",
      "2031 [D loss: -0.174833] [G loss: -2.412238]\n",
      "2032 [D loss: 0.089643] [G loss: -2.394379]\n",
      "2033 [D loss: -0.279455] [G loss: -2.572067]\n",
      "2034 [D loss: -0.841754] [G loss: -2.313793]\n",
      "2035 [D loss: -0.998927] [G loss: -2.480092]\n",
      "2036 [D loss: -0.531510] [G loss: -2.870015]\n",
      "2037 [D loss: -0.909746] [G loss: -2.585647]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2038 [D loss: -0.817918] [G loss: -2.339494]\n",
      "2039 [D loss: -1.260243] [G loss: -2.533943]\n",
      "2040 [D loss: -0.577466] [G loss: -2.921619]\n",
      "2041 [D loss: -0.076755] [G loss: -2.368608]\n",
      "2042 [D loss: -0.701330] [G loss: -2.408623]\n",
      "2043 [D loss: -0.094309] [G loss: -2.699965]\n",
      "2044 [D loss: -0.290492] [G loss: -2.395531]\n",
      "2045 [D loss: 0.077911] [G loss: -2.394835]\n",
      "2046 [D loss: -0.828805] [G loss: -2.552104]\n",
      "2047 [D loss: -0.585250] [G loss: -2.235034]\n",
      "2048 [D loss: 0.180572] [G loss: -2.698664]\n",
      "2049 [D loss: -0.555679] [G loss: -2.530669]\n",
      "2050 [D loss: -0.037083] [G loss: -2.176950]\n",
      "2051 [D loss: -0.558550] [G loss: -2.404366]\n",
      "2052 [D loss: -0.606759] [G loss: -2.218429]\n",
      "2053 [D loss: -0.111152] [G loss: -1.906287]\n",
      "2054 [D loss: -0.105357] [G loss: -2.073812]\n",
      "2055 [D loss: -0.654343] [G loss: -2.343739]\n",
      "2056 [D loss: -0.047823] [G loss: -2.592374]\n",
      "2057 [D loss: -1.048707] [G loss: -2.531675]\n",
      "2058 [D loss: -1.167579] [G loss: -2.387008]\n",
      "2059 [D loss: -0.429050] [G loss: -2.503247]\n",
      "2060 [D loss: -0.121791] [G loss: -2.093212]\n",
      "2061 [D loss: -0.627484] [G loss: -2.413943]\n",
      "2062 [D loss: -0.540537] [G loss: -2.171973]\n",
      "2063 [D loss: -0.767465] [G loss: -2.611112]\n",
      "2064 [D loss: -0.185482] [G loss: -2.332253]\n",
      "2065 [D loss: -0.487906] [G loss: -2.259811]\n",
      "2066 [D loss: -0.307243] [G loss: -2.630725]\n",
      "2067 [D loss: -0.924067] [G loss: -2.041538]\n",
      "2068 [D loss: -0.313080] [G loss: -2.087512]\n",
      "2069 [D loss: -0.228486] [G loss: -2.411597]\n",
      "2070 [D loss: -0.932515] [G loss: -2.198542]\n",
      "2071 [D loss: -0.563645] [G loss: -2.184939]\n",
      "2072 [D loss: 0.449914] [G loss: -2.490954]\n",
      "2073 [D loss: -0.316619] [G loss: -2.354021]\n",
      "2074 [D loss: -0.946340] [G loss: -2.300558]\n",
      "2075 [D loss: -0.026688] [G loss: -1.873732]\n",
      "2076 [D loss: -1.421951] [G loss: -2.500825]\n",
      "2077 [D loss: -0.482616] [G loss: -2.594636]\n",
      "2078 [D loss: -1.382516] [G loss: -2.570255]\n",
      "2079 [D loss: -1.107343] [G loss: -2.660431]\n",
      "2080 [D loss: -0.305049] [G loss: -1.910085]\n",
      "2081 [D loss: -0.629238] [G loss: -2.144338]\n",
      "2082 [D loss: -0.720825] [G loss: -2.241818]\n",
      "2083 [D loss: -0.246543] [G loss: -2.748057]\n",
      "2084 [D loss: -0.384003] [G loss: -2.227455]\n",
      "2085 [D loss: -1.284848] [G loss: -1.839529]\n",
      "2086 [D loss: 0.024971] [G loss: -2.550377]\n",
      "2087 [D loss: -0.662053] [G loss: -2.386203]\n",
      "2088 [D loss: -0.892250] [G loss: -2.145490]\n",
      "2089 [D loss: -0.895294] [G loss: -2.642427]\n",
      "2090 [D loss: -0.281677] [G loss: -2.033064]\n",
      "2091 [D loss: 0.270255] [G loss: -2.098451]\n",
      "2092 [D loss: -0.931926] [G loss: -2.545157]\n",
      "2093 [D loss: -0.547463] [G loss: -2.582279]\n",
      "2094 [D loss: -0.497273] [G loss: -2.091061]\n",
      "2095 [D loss: -0.217039] [G loss: -2.422973]\n",
      "2096 [D loss: -1.115014] [G loss: -2.635954]\n",
      "2097 [D loss: -0.539758] [G loss: -2.170388]\n",
      "2098 [D loss: -0.727174] [G loss: -2.401409]\n",
      "2099 [D loss: 0.074959] [G loss: -2.454472]\n",
      "2100 [D loss: -0.335964] [G loss: -2.987763]\n",
      "2101 [D loss: -0.510060] [G loss: -2.344377]\n",
      "2102 [D loss: -0.514411] [G loss: -2.729990]\n",
      "2103 [D loss: -1.076469] [G loss: -2.482436]\n",
      "2104 [D loss: -0.587618] [G loss: -2.534813]\n",
      "2105 [D loss: -0.487366] [G loss: -2.368281]\n",
      "2106 [D loss: 0.009224] [G loss: -2.028747]\n",
      "2107 [D loss: -0.661516] [G loss: -2.147670]\n",
      "2108 [D loss: -0.845229] [G loss: -2.489325]\n",
      "2109 [D loss: -1.008283] [G loss: -2.526133]\n",
      "2110 [D loss: -0.597948] [G loss: -2.074038]\n",
      "2111 [D loss: -0.878784] [G loss: -1.723092]\n",
      "2112 [D loss: -0.071159] [G loss: -2.399200]\n",
      "2113 [D loss: -0.172651] [G loss: -2.224765]\n",
      "2114 [D loss: -0.658629] [G loss: -1.790851]\n",
      "2115 [D loss: -0.777200] [G loss: -2.145298]\n",
      "2116 [D loss: -0.662098] [G loss: -2.526635]\n",
      "2117 [D loss: -1.113539] [G loss: -1.762224]\n",
      "2118 [D loss: -0.142542] [G loss: -1.266417]\n",
      "2119 [D loss: -0.577041] [G loss: -2.460958]\n",
      "2120 [D loss: -1.261097] [G loss: -2.593705]\n",
      "2121 [D loss: -0.624256] [G loss: -2.069618]\n",
      "2122 [D loss: -0.261749] [G loss: -1.716295]\n",
      "2123 [D loss: -0.773078] [G loss: -2.001209]\n",
      "2124 [D loss: -0.830869] [G loss: -2.305297]\n",
      "2125 [D loss: -1.060785] [G loss: -2.423945]\n",
      "2126 [D loss: -0.066159] [G loss: -2.189364]\n",
      "2127 [D loss: -0.393201] [G loss: -2.448039]\n",
      "2128 [D loss: -1.091534] [G loss: -2.276127]\n",
      "2129 [D loss: -0.884338] [G loss: -1.753754]\n",
      "2130 [D loss: -0.653721] [G loss: -2.275606]\n",
      "2131 [D loss: -0.329800] [G loss: -1.887191]\n",
      "2132 [D loss: -0.625496] [G loss: -2.131329]\n",
      "2133 [D loss: -0.243677] [G loss: -1.958215]\n",
      "2134 [D loss: -0.470428] [G loss: -2.749849]\n",
      "2135 [D loss: -0.021929] [G loss: -2.096714]\n",
      "2136 [D loss: -0.411631] [G loss: -1.967577]\n",
      "2137 [D loss: -0.560544] [G loss: -2.524808]\n",
      "2138 [D loss: -0.669752] [G loss: -2.466422]\n",
      "2139 [D loss: -0.103363] [G loss: -2.155302]\n",
      "2140 [D loss: -1.281631] [G loss: -2.037814]\n",
      "2141 [D loss: -0.839815] [G loss: -2.757478]\n",
      "2142 [D loss: -0.148433] [G loss: -2.463058]\n",
      "2143 [D loss: -0.303809] [G loss: -1.885940]\n",
      "2144 [D loss: -1.129079] [G loss: -2.471751]\n",
      "2145 [D loss: -0.603342] [G loss: -2.127390]\n",
      "2146 [D loss: -0.245806] [G loss: -2.733018]\n",
      "2147 [D loss: 0.013217] [G loss: -2.572402]\n",
      "2148 [D loss: 0.008401] [G loss: -1.954376]\n",
      "2149 [D loss: 0.295155] [G loss: -1.839220]\n",
      "2150 [D loss: -0.385651] [G loss: -2.408165]\n",
      "2151 [D loss: 0.177927] [G loss: -2.202360]\n",
      "2152 [D loss: -0.128169] [G loss: -2.422903]\n",
      "2153 [D loss: -0.808698] [G loss: -1.967358]\n",
      "2154 [D loss: -0.420676] [G loss: -1.996639]\n",
      "2155 [D loss: -0.904132] [G loss: -2.202779]\n",
      "2156 [D loss: 0.150974] [G loss: -2.312055]\n",
      "2157 [D loss: 0.214371] [G loss: -2.093488]\n",
      "2158 [D loss: -0.789924] [G loss: -2.015176]\n",
      "2159 [D loss: -0.382018] [G loss: -2.135910]\n",
      "2160 [D loss: -0.645738] [G loss: -1.821798]\n",
      "2161 [D loss: -0.519572] [G loss: -2.118843]\n",
      "2162 [D loss: -0.737652] [G loss: -2.017423]\n",
      "2163 [D loss: -0.859003] [G loss: -2.122095]\n",
      "2164 [D loss: -0.601719] [G loss: -2.321266]\n",
      "2165 [D loss: 0.027185] [G loss: -2.157634]\n",
      "2166 [D loss: -0.495056] [G loss: -2.156543]\n",
      "2167 [D loss: -0.416052] [G loss: -2.484918]\n",
      "2168 [D loss: -0.835689] [G loss: -1.665595]\n",
      "2169 [D loss: -0.321662] [G loss: -2.357642]\n",
      "2170 [D loss: -0.220767] [G loss: -2.254811]\n",
      "2171 [D loss: -0.495975] [G loss: -2.211612]\n",
      "2172 [D loss: -0.949622] [G loss: -2.062428]\n",
      "2173 [D loss: -0.210441] [G loss: -2.210988]\n",
      "2174 [D loss: -0.261940] [G loss: -2.083740]\n",
      "2175 [D loss: -0.491389] [G loss: -2.159090]\n",
      "2176 [D loss: -0.829178] [G loss: -1.557590]\n",
      "2177 [D loss: -0.660797] [G loss: -2.374501]\n",
      "2178 [D loss: 0.008412] [G loss: -2.509185]\n",
      "2179 [D loss: -0.337098] [G loss: -1.900866]\n",
      "2180 [D loss: -0.258411] [G loss: -2.488479]\n",
      "2181 [D loss: -1.121462] [G loss: -1.530423]\n",
      "2182 [D loss: -0.874138] [G loss: -2.350165]\n",
      "2183 [D loss: -0.541342] [G loss: -1.842823]\n",
      "2184 [D loss: -0.195486] [G loss: -2.006605]\n",
      "2185 [D loss: -0.078612] [G loss: -2.106535]\n",
      "2186 [D loss: -0.575296] [G loss: -1.824385]\n",
      "2187 [D loss: -1.386043] [G loss: -2.041133]\n",
      "2188 [D loss: -0.697736] [G loss: -2.437599]\n",
      "2189 [D loss: -0.734144] [G loss: -2.232976]\n",
      "2190 [D loss: 0.190316] [G loss: -1.980020]\n",
      "2191 [D loss: 0.148821] [G loss: -2.655069]\n",
      "2192 [D loss: -1.201929] [G loss: -2.297669]\n",
      "2193 [D loss: -0.900643] [G loss: -1.950346]\n",
      "2194 [D loss: -0.240744] [G loss: -2.236478]\n",
      "2195 [D loss: 0.078301] [G loss: -1.904110]\n",
      "2196 [D loss: -0.622879] [G loss: -1.746325]\n",
      "2197 [D loss: 0.245766] [G loss: -2.011657]\n",
      "2198 [D loss: -0.339644] [G loss: -2.354169]\n",
      "2199 [D loss: -0.631092] [G loss: -1.866296]\n",
      "2200 [D loss: -0.357234] [G loss: -2.321716]\n",
      "2201 [D loss: -0.068004] [G loss: -2.104781]\n",
      "2202 [D loss: -0.561664] [G loss: -1.599206]\n",
      "2203 [D loss: -0.677495] [G loss: -2.152043]\n",
      "2204 [D loss: -0.552290] [G loss: -1.891279]\n",
      "2205 [D loss: -0.769093] [G loss: -1.878975]\n",
      "2206 [D loss: -0.110907] [G loss: -2.339440]\n",
      "2207 [D loss: -0.322881] [G loss: -1.456624]\n",
      "2208 [D loss: -0.458345] [G loss: -1.618944]\n",
      "2209 [D loss: -0.292113] [G loss: -1.380159]\n",
      "2210 [D loss: -0.379164] [G loss: -2.405513]\n",
      "2211 [D loss: -0.907779] [G loss: -1.845888]\n",
      "2212 [D loss: -0.689955] [G loss: -1.830572]\n",
      "2213 [D loss: -0.401821] [G loss: -2.027586]\n",
      "2214 [D loss: -0.388807] [G loss: -2.026230]\n",
      "2215 [D loss: -1.070805] [G loss: -1.253289]\n",
      "2216 [D loss: -1.312640] [G loss: -2.121425]\n",
      "2217 [D loss: -0.840169] [G loss: -1.579692]\n",
      "2218 [D loss: -0.236861] [G loss: -1.736903]\n",
      "2219 [D loss: -0.634406] [G loss: -1.820211]\n",
      "2220 [D loss: -0.435625] [G loss: -1.976377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2221 [D loss: -1.130304] [G loss: -1.807348]\n",
      "2222 [D loss: 0.090930] [G loss: -1.769228]\n",
      "2223 [D loss: -0.852175] [G loss: -2.471143]\n",
      "2224 [D loss: -0.314273] [G loss: -1.639558]\n",
      "2225 [D loss: -0.589768] [G loss: -1.708480]\n",
      "2226 [D loss: -0.320955] [G loss: -2.343223]\n",
      "2227 [D loss: -0.678880] [G loss: -1.910802]\n",
      "2228 [D loss: -1.190974] [G loss: -1.877266]\n",
      "2229 [D loss: -0.236641] [G loss: -2.085944]\n",
      "2230 [D loss: -0.443879] [G loss: -2.287195]\n",
      "2231 [D loss: -0.377624] [G loss: -2.311561]\n",
      "2232 [D loss: -0.411889] [G loss: -2.082436]\n",
      "2233 [D loss: 0.307874] [G loss: -2.138365]\n",
      "2234 [D loss: -0.312012] [G loss: -2.197287]\n",
      "2235 [D loss: -0.420102] [G loss: -1.980470]\n",
      "2236 [D loss: -0.530272] [G loss: -2.048275]\n",
      "2237 [D loss: -0.643914] [G loss: -1.814736]\n",
      "2238 [D loss: -0.823666] [G loss: -2.131377]\n",
      "2239 [D loss: -0.783549] [G loss: -2.109231]\n",
      "2240 [D loss: -0.861815] [G loss: -2.039255]\n",
      "2241 [D loss: -0.344496] [G loss: -1.940282]\n",
      "2242 [D loss: -0.350523] [G loss: -1.784209]\n",
      "2243 [D loss: -0.908649] [G loss: -2.143170]\n",
      "2244 [D loss: -0.461155] [G loss: -1.987674]\n",
      "2245 [D loss: -0.871093] [G loss: -1.655310]\n",
      "2246 [D loss: -0.415459] [G loss: -2.031869]\n",
      "2247 [D loss: -1.022087] [G loss: -1.671711]\n",
      "2248 [D loss: 0.254548] [G loss: -2.075091]\n",
      "2249 [D loss: -1.130846] [G loss: -2.049635]\n",
      "2250 [D loss: -0.518993] [G loss: -1.631595]\n",
      "2251 [D loss: -0.357357] [G loss: -1.664302]\n",
      "2252 [D loss: -1.176289] [G loss: -1.417501]\n",
      "2253 [D loss: -0.856115] [G loss: -2.383901]\n",
      "2254 [D loss: -0.150244] [G loss: -1.776142]\n",
      "2255 [D loss: -0.714697] [G loss: -1.623140]\n",
      "2256 [D loss: -0.391285] [G loss: -1.548951]\n",
      "2257 [D loss: -0.334058] [G loss: -1.768198]\n",
      "2258 [D loss: -0.071662] [G loss: -1.990639]\n",
      "2259 [D loss: -0.535194] [G loss: -1.754995]\n",
      "2260 [D loss: -0.516536] [G loss: -1.754594]\n",
      "2261 [D loss: -0.097006] [G loss: -1.875371]\n",
      "2262 [D loss: -1.041629] [G loss: -2.052981]\n",
      "2263 [D loss: -1.060419] [G loss: -1.897245]\n",
      "2264 [D loss: -0.256868] [G loss: -1.787713]\n",
      "2265 [D loss: -0.706386] [G loss: -1.976564]\n",
      "2266 [D loss: -0.202222] [G loss: -1.578528]\n",
      "2267 [D loss: -0.024451] [G loss: -2.005657]\n",
      "2268 [D loss: -0.622112] [G loss: -1.629740]\n",
      "2269 [D loss: -0.527394] [G loss: -1.749656]\n",
      "2270 [D loss: -1.300854] [G loss: -1.679397]\n",
      "2271 [D loss: -1.222933] [G loss: -2.314182]\n",
      "2272 [D loss: -0.434587] [G loss: -1.905571]\n",
      "2273 [D loss: -0.673567] [G loss: -1.890030]\n",
      "2274 [D loss: -1.044745] [G loss: -1.885882]\n",
      "2275 [D loss: -0.517155] [G loss: -2.206614]\n",
      "2276 [D loss: 0.156735] [G loss: -2.106053]\n",
      "2277 [D loss: -0.893512] [G loss: -2.218411]\n",
      "2278 [D loss: -0.429102] [G loss: -1.622689]\n",
      "2279 [D loss: -0.686702] [G loss: -1.956432]\n",
      "2280 [D loss: -0.274201] [G loss: -2.091308]\n",
      "2281 [D loss: -0.507745] [G loss: -1.863452]\n",
      "2282 [D loss: -0.577127] [G loss: -1.892166]\n",
      "2283 [D loss: -0.658581] [G loss: -1.752254]\n",
      "2284 [D loss: -0.513252] [G loss: -2.396286]\n",
      "2285 [D loss: -0.733474] [G loss: -2.140945]\n",
      "2286 [D loss: -0.699412] [G loss: -2.290321]\n",
      "2287 [D loss: 0.584998] [G loss: -2.476254]\n",
      "2288 [D loss: 0.408911] [G loss: -2.197373]\n",
      "2289 [D loss: -0.566060] [G loss: -2.004739]\n",
      "2290 [D loss: -0.555446] [G loss: -2.230029]\n",
      "2291 [D loss: -0.417518] [G loss: -2.438650]\n",
      "2292 [D loss: -0.754523] [G loss: -1.631767]\n",
      "2293 [D loss: -0.877899] [G loss: -1.701440]\n",
      "2294 [D loss: -1.153971] [G loss: -2.057620]\n",
      "2295 [D loss: -0.200694] [G loss: -2.186990]\n",
      "2296 [D loss: -0.842441] [G loss: -1.930480]\n",
      "2297 [D loss: -0.057222] [G loss: -2.295945]\n",
      "2298 [D loss: 0.302974] [G loss: -1.932700]\n",
      "2299 [D loss: -1.317844] [G loss: -2.291684]\n",
      "2300 [D loss: 0.303412] [G loss: -2.036855]\n",
      "2301 [D loss: -0.683933] [G loss: -1.819573]\n",
      "2302 [D loss: -0.064731] [G loss: -2.489089]\n",
      "2303 [D loss: -0.670236] [G loss: -1.886094]\n",
      "2304 [D loss: 0.023535] [G loss: -1.811035]\n",
      "2305 [D loss: -1.639326] [G loss: -1.685705]\n",
      "2306 [D loss: -0.960984] [G loss: -2.891774]\n",
      "2307 [D loss: -0.346967] [G loss: -2.281289]\n",
      "2308 [D loss: -0.825980] [G loss: -2.017640]\n",
      "2309 [D loss: -0.570584] [G loss: -1.674715]\n",
      "2310 [D loss: -0.419916] [G loss: -1.913788]\n",
      "2311 [D loss: -1.029797] [G loss: -1.860322]\n",
      "2312 [D loss: -0.373545] [G loss: -2.172419]\n",
      "2313 [D loss: -0.711488] [G loss: -1.951200]\n",
      "2314 [D loss: -0.676143] [G loss: -1.523645]\n",
      "2315 [D loss: -0.255018] [G loss: -2.365283]\n",
      "2316 [D loss: -0.216028] [G loss: -2.170607]\n",
      "2317 [D loss: -0.695200] [G loss: -1.620909]\n",
      "2318 [D loss: -0.856429] [G loss: -1.898789]\n",
      "2319 [D loss: -0.765274] [G loss: -2.349335]\n",
      "2320 [D loss: -0.500546] [G loss: -1.939338]\n",
      "2321 [D loss: -0.393190] [G loss: -2.043017]\n",
      "2322 [D loss: -0.345810] [G loss: -1.901774]\n",
      "2323 [D loss: -0.446498] [G loss: -1.632985]\n",
      "2324 [D loss: -0.195403] [G loss: -1.290904]\n",
      "2325 [D loss: -1.278403] [G loss: -1.637410]\n",
      "2326 [D loss: 0.062596] [G loss: -1.838995]\n",
      "2327 [D loss: -0.621774] [G loss: -1.687809]\n",
      "2328 [D loss: -0.388253] [G loss: -1.708554]\n",
      "2329 [D loss: -0.742327] [G loss: -2.093597]\n",
      "2330 [D loss: -0.864796] [G loss: -1.884066]\n",
      "2331 [D loss: 0.231215] [G loss: -1.387604]\n",
      "2332 [D loss: -0.612579] [G loss: -2.281651]\n",
      "2333 [D loss: -0.904385] [G loss: -1.936658]\n",
      "2334 [D loss: -0.222471] [G loss: -1.936330]\n",
      "2335 [D loss: -1.206855] [G loss: -2.083126]\n",
      "2336 [D loss: -1.013348] [G loss: -1.897755]\n",
      "2337 [D loss: 0.068192] [G loss: -1.989454]\n",
      "2338 [D loss: -0.271882] [G loss: -2.107428]\n",
      "2339 [D loss: -0.647869] [G loss: -2.240985]\n",
      "2340 [D loss: -1.246797] [G loss: -2.463584]\n",
      "2341 [D loss: -0.663587] [G loss: -2.162603]\n",
      "2342 [D loss: -0.655903] [G loss: -2.565051]\n",
      "2343 [D loss: -0.737114] [G loss: -2.609850]\n",
      "2344 [D loss: -0.085299] [G loss: -2.485055]\n",
      "2345 [D loss: -0.576934] [G loss: -2.583629]\n",
      "2346 [D loss: 0.003899] [G loss: -2.241562]\n",
      "2347 [D loss: -0.394881] [G loss: -2.063714]\n",
      "2348 [D loss: -0.466539] [G loss: -1.934060]\n",
      "2349 [D loss: -0.314506] [G loss: -1.995102]\n",
      "2350 [D loss: -0.483233] [G loss: -2.222169]\n",
      "2351 [D loss: -0.582597] [G loss: -2.596367]\n",
      "2352 [D loss: -0.240511] [G loss: -2.214475]\n",
      "2353 [D loss: -0.724203] [G loss: -2.374451]\n",
      "2354 [D loss: -0.345124] [G loss: -2.444552]\n",
      "2355 [D loss: -0.647992] [G loss: -2.210468]\n",
      "2356 [D loss: -0.753560] [G loss: -1.914039]\n",
      "2357 [D loss: -0.648914] [G loss: -1.764598]\n",
      "2358 [D loss: 0.212866] [G loss: -2.402200]\n",
      "2359 [D loss: 0.295803] [G loss: -2.296709]\n",
      "2360 [D loss: -0.366505] [G loss: -1.904817]\n",
      "2361 [D loss: -0.014878] [G loss: -1.820561]\n",
      "2362 [D loss: -0.462369] [G loss: -1.768752]\n",
      "2363 [D loss: 0.193743] [G loss: -1.759501]\n",
      "2364 [D loss: -0.475850] [G loss: -2.158393]\n",
      "2365 [D loss: -0.822711] [G loss: -1.629460]\n",
      "2366 [D loss: -0.873252] [G loss: -1.708987]\n",
      "2367 [D loss: -0.792047] [G loss: -2.050224]\n",
      "2368 [D loss: -1.035892] [G loss: -1.640676]\n",
      "2369 [D loss: -0.108119] [G loss: -1.751639]\n",
      "2370 [D loss: -0.073838] [G loss: -1.648879]\n",
      "2371 [D loss: -0.769490] [G loss: -1.995372]\n",
      "2372 [D loss: 0.232162] [G loss: -2.272292]\n",
      "2373 [D loss: -1.146717] [G loss: -2.264636]\n",
      "2374 [D loss: -1.243687] [G loss: -1.969109]\n",
      "2375 [D loss: -0.466392] [G loss: -1.744045]\n",
      "2376 [D loss: -0.001465] [G loss: -1.984527]\n",
      "2377 [D loss: -0.685844] [G loss: -1.808018]\n",
      "2378 [D loss: -0.437120] [G loss: -2.010771]\n",
      "2379 [D loss: -1.191534] [G loss: -2.158296]\n",
      "2380 [D loss: -0.805352] [G loss: -1.607167]\n",
      "2381 [D loss: -1.108086] [G loss: -1.592447]\n",
      "2382 [D loss: -0.631917] [G loss: -1.719968]\n",
      "2383 [D loss: -0.250438] [G loss: -2.313291]\n",
      "2384 [D loss: -0.679361] [G loss: -2.120259]\n",
      "2385 [D loss: -0.470518] [G loss: -1.996292]\n",
      "2386 [D loss: -0.612497] [G loss: -2.127454]\n",
      "2387 [D loss: -1.087299] [G loss: -1.685870]\n",
      "2388 [D loss: -0.284093] [G loss: -2.626600]\n",
      "2389 [D loss: -0.159021] [G loss: -2.076974]\n",
      "2390 [D loss: -0.268062] [G loss: -2.038986]\n",
      "2391 [D loss: -0.531431] [G loss: -2.352728]\n",
      "2392 [D loss: -0.733038] [G loss: -2.563188]\n",
      "2393 [D loss: -1.045041] [G loss: -2.145112]\n",
      "2394 [D loss: -1.000464] [G loss: -2.233130]\n",
      "2395 [D loss: -0.789970] [G loss: -2.205581]\n",
      "2396 [D loss: -0.025815] [G loss: -2.210075]\n",
      "2397 [D loss: -0.425955] [G loss: -1.922678]\n",
      "2398 [D loss: -0.274015] [G loss: -2.207345]\n",
      "2399 [D loss: -1.015541] [G loss: -1.870692]\n",
      "2400 [D loss: -0.622632] [G loss: -1.675250]\n",
      "2401 [D loss: -0.193038] [G loss: -2.243021]\n",
      "2402 [D loss: -0.153945] [G loss: -1.883468]\n",
      "2403 [D loss: -0.564216] [G loss: -1.930758]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2404 [D loss: -1.020910] [G loss: -1.913471]\n",
      "2405 [D loss: -0.276055] [G loss: -1.998328]\n",
      "2406 [D loss: -0.790286] [G loss: -2.327264]\n",
      "2407 [D loss: -0.131145] [G loss: -2.652147]\n",
      "2408 [D loss: -0.606227] [G loss: -2.310210]\n",
      "2409 [D loss: -0.156687] [G loss: -1.813769]\n",
      "2410 [D loss: -0.230035] [G loss: -1.695987]\n",
      "2411 [D loss: -0.354228] [G loss: -1.760433]\n",
      "2412 [D loss: -0.926099] [G loss: -2.032331]\n",
      "2413 [D loss: -0.678029] [G loss: -1.797156]\n",
      "2414 [D loss: -0.792520] [G loss: -1.678052]\n",
      "2415 [D loss: -1.130708] [G loss: -1.893006]\n",
      "2416 [D loss: -0.709074] [G loss: -1.607876]\n",
      "2417 [D loss: -0.689505] [G loss: -2.328692]\n",
      "2418 [D loss: -0.745283] [G loss: -1.848363]\n",
      "2419 [D loss: -1.100448] [G loss: -2.041864]\n",
      "2420 [D loss: -0.476270] [G loss: -1.897526]\n",
      "2421 [D loss: -0.159874] [G loss: -2.060741]\n",
      "2422 [D loss: -0.649533] [G loss: -2.559935]\n",
      "2423 [D loss: -0.074863] [G loss: -2.467247]\n",
      "2424 [D loss: -0.634030] [G loss: -1.756130]\n",
      "2425 [D loss: -0.142192] [G loss: -2.650424]\n",
      "2426 [D loss: -0.630582] [G loss: -2.403410]\n",
      "2427 [D loss: -0.771402] [G loss: -2.176483]\n",
      "2428 [D loss: -0.871755] [G loss: -2.059700]\n",
      "2429 [D loss: -0.379869] [G loss: -2.731302]\n",
      "2430 [D loss: -0.709693] [G loss: -1.760073]\n",
      "2431 [D loss: -0.053811] [G loss: -2.638995]\n",
      "2432 [D loss: -0.583374] [G loss: -2.348596]\n",
      "2433 [D loss: -0.589848] [G loss: -2.125265]\n",
      "2434 [D loss: -0.551449] [G loss: -2.276409]\n",
      "2435 [D loss: -0.101604] [G loss: -2.168881]\n",
      "2436 [D loss: -0.296514] [G loss: -1.779024]\n",
      "2437 [D loss: -0.667951] [G loss: -2.365642]\n",
      "2438 [D loss: -1.014048] [G loss: -2.026467]\n",
      "2439 [D loss: -0.505906] [G loss: -2.010765]\n",
      "2440 [D loss: -0.223360] [G loss: -2.105240]\n",
      "2441 [D loss: -0.442637] [G loss: -1.801800]\n",
      "2442 [D loss: -0.815843] [G loss: -1.907579]\n",
      "2443 [D loss: -0.053325] [G loss: -2.092863]\n",
      "2444 [D loss: -0.884097] [G loss: -2.494296]\n",
      "2445 [D loss: -0.348303] [G loss: -2.144568]\n",
      "2446 [D loss: -0.469427] [G loss: -1.918060]\n",
      "2447 [D loss: -0.027659] [G loss: -1.635188]\n",
      "2448 [D loss: -0.330577] [G loss: -2.178195]\n",
      "2449 [D loss: -0.692803] [G loss: -1.609025]\n",
      "2450 [D loss: -0.033031] [G loss: -1.853572]\n",
      "2451 [D loss: -0.638640] [G loss: -2.084479]\n",
      "2452 [D loss: -0.557170] [G loss: -1.864466]\n",
      "2453 [D loss: -0.803195] [G loss: -1.311327]\n",
      "2454 [D loss: -0.579120] [G loss: -1.551311]\n",
      "2455 [D loss: 0.086226] [G loss: -1.598234]\n",
      "2456 [D loss: -0.842120] [G loss: -1.873735]\n",
      "2457 [D loss: -0.888521] [G loss: -2.655058]\n",
      "2458 [D loss: -1.398711] [G loss: -2.135960]\n",
      "2459 [D loss: 0.008763] [G loss: -1.875433]\n",
      "2460 [D loss: 0.148453] [G loss: -2.321169]\n",
      "2461 [D loss: -0.480707] [G loss: -1.948748]\n",
      "2462 [D loss: -0.924863] [G loss: -1.897671]\n",
      "2463 [D loss: -0.666653] [G loss: -2.014883]\n",
      "2464 [D loss: -0.406773] [G loss: -2.309554]\n",
      "2465 [D loss: -0.531955] [G loss: -1.838116]\n",
      "2466 [D loss: -0.502252] [G loss: -1.838526]\n",
      "2467 [D loss: -0.602332] [G loss: -1.946355]\n",
      "2468 [D loss: -0.534377] [G loss: -1.599274]\n",
      "2469 [D loss: 0.041832] [G loss: -1.823516]\n",
      "2470 [D loss: -0.254746] [G loss: -2.285978]\n",
      "2471 [D loss: -0.488441] [G loss: -2.462874]\n",
      "2472 [D loss: -0.584175] [G loss: -1.953981]\n",
      "2473 [D loss: -0.383807] [G loss: -1.987998]\n",
      "2474 [D loss: -0.022690] [G loss: -2.241368]\n",
      "2475 [D loss: -0.983734] [G loss: -1.917283]\n",
      "2476 [D loss: -0.450958] [G loss: -2.482472]\n",
      "2477 [D loss: -0.665781] [G loss: -1.887560]\n",
      "2478 [D loss: -0.859414] [G loss: -1.644890]\n",
      "2479 [D loss: -0.242647] [G loss: -1.898807]\n",
      "2480 [D loss: -0.243461] [G loss: -2.046609]\n",
      "2481 [D loss: -0.170174] [G loss: -1.309560]\n",
      "2482 [D loss: -0.729053] [G loss: -1.970005]\n",
      "2483 [D loss: 0.055307] [G loss: -1.703286]\n",
      "2484 [D loss: -0.085281] [G loss: -1.523294]\n",
      "2485 [D loss: -0.477956] [G loss: -1.827965]\n",
      "2486 [D loss: -0.181018] [G loss: -1.724108]\n",
      "2487 [D loss: -1.032770] [G loss: -1.768210]\n",
      "2488 [D loss: -0.207865] [G loss: -1.726787]\n",
      "2489 [D loss: -0.372931] [G loss: -1.737030]\n",
      "2490 [D loss: -0.062954] [G loss: -2.305077]\n",
      "2491 [D loss: -0.717032] [G loss: -1.477052]\n",
      "2492 [D loss: -0.331452] [G loss: -1.432312]\n",
      "2493 [D loss: -0.466753] [G loss: -1.493467]\n",
      "2494 [D loss: -0.095204] [G loss: -2.000307]\n",
      "2495 [D loss: -0.554378] [G loss: -1.961342]\n",
      "2496 [D loss: -0.279862] [G loss: -2.373225]\n",
      "2497 [D loss: -0.666258] [G loss: -2.297030]\n",
      "2498 [D loss: -0.330963] [G loss: -2.191883]\n",
      "2499 [D loss: -0.605067] [G loss: -1.771339]\n",
      "2500 [D loss: -0.934784] [G loss: -1.870648]\n",
      "2501 [D loss: -0.784203] [G loss: -2.075408]\n",
      "2502 [D loss: -1.356248] [G loss: -2.180310]\n",
      "2503 [D loss: -0.588688] [G loss: -2.389954]\n",
      "2504 [D loss: -0.624300] [G loss: -2.425638]\n",
      "2505 [D loss: -0.319212] [G loss: -1.970884]\n",
      "2506 [D loss: -0.241068] [G loss: -2.263127]\n",
      "2507 [D loss: -0.738321] [G loss: -1.625701]\n",
      "2508 [D loss: -0.405975] [G loss: -1.984559]\n",
      "2509 [D loss: -0.384319] [G loss: -1.885673]\n",
      "2510 [D loss: -0.886653] [G loss: -2.448958]\n",
      "2511 [D loss: -0.845677] [G loss: -1.492524]\n",
      "2512 [D loss: -0.683679] [G loss: -1.941328]\n",
      "2513 [D loss: -0.335373] [G loss: -1.632394]\n",
      "2514 [D loss: -0.455365] [G loss: -1.766031]\n",
      "2515 [D loss: 0.111019] [G loss: -2.325833]\n",
      "2516 [D loss: -0.990679] [G loss: -2.373344]\n",
      "2517 [D loss: -1.039506] [G loss: -2.075291]\n",
      "2518 [D loss: -0.511704] [G loss: -2.156929]\n",
      "2519 [D loss: -0.107824] [G loss: -2.114263]\n",
      "2520 [D loss: -0.202715] [G loss: -2.478204]\n",
      "2521 [D loss: -0.401209] [G loss: -2.035211]\n",
      "2522 [D loss: -0.897333] [G loss: -1.291624]\n",
      "2523 [D loss: -0.025621] [G loss: -1.577634]\n",
      "2524 [D loss: -0.071988] [G loss: -1.900319]\n",
      "2525 [D loss: -0.326528] [G loss: -1.968697]\n",
      "2526 [D loss: -0.575167] [G loss: -1.719057]\n",
      "2527 [D loss: -0.531067] [G loss: -1.935511]\n",
      "2528 [D loss: -0.854766] [G loss: -1.319044]\n",
      "2529 [D loss: -0.910371] [G loss: -1.164024]\n",
      "2530 [D loss: -0.635802] [G loss: -1.745324]\n",
      "2531 [D loss: -0.701340] [G loss: -1.365898]\n",
      "2532 [D loss: -0.304614] [G loss: -1.743757]\n",
      "2533 [D loss: -1.232455] [G loss: -1.498026]\n",
      "2534 [D loss: -0.810701] [G loss: -1.701085]\n",
      "2535 [D loss: -0.461168] [G loss: -1.074472]\n",
      "2536 [D loss: -0.094408] [G loss: -1.472961]\n",
      "2537 [D loss: -1.110658] [G loss: -1.553873]\n",
      "2538 [D loss: -0.567427] [G loss: -1.905523]\n",
      "2539 [D loss: 0.404487] [G loss: -1.658462]\n",
      "2540 [D loss: -0.028468] [G loss: -2.010456]\n",
      "2541 [D loss: -1.251039] [G loss: -1.397895]\n",
      "2542 [D loss: -1.121424] [G loss: -1.430716]\n",
      "2543 [D loss: 0.099208] [G loss: -1.837737]\n",
      "2544 [D loss: -0.377775] [G loss: -1.575471]\n",
      "2545 [D loss: -0.317779] [G loss: -1.517223]\n",
      "2546 [D loss: -0.775545] [G loss: -1.866528]\n",
      "2547 [D loss: -0.669980] [G loss: -2.086236]\n",
      "2548 [D loss: 0.070536] [G loss: -1.766073]\n",
      "2549 [D loss: -0.801241] [G loss: -1.261678]\n",
      "2550 [D loss: -0.066527] [G loss: -1.606005]\n",
      "2551 [D loss: -0.151417] [G loss: -1.832893]\n",
      "2552 [D loss: -0.833206] [G loss: -2.710581]\n",
      "2553 [D loss: -0.396955] [G loss: -1.863017]\n",
      "2554 [D loss: -0.402888] [G loss: -2.118011]\n",
      "2555 [D loss: -0.552602] [G loss: -2.324690]\n",
      "2556 [D loss: -0.701621] [G loss: -1.919474]\n",
      "2557 [D loss: -0.765747] [G loss: -1.812011]\n",
      "2558 [D loss: -1.331573] [G loss: -1.685006]\n",
      "2559 [D loss: -0.727040] [G loss: -2.146205]\n",
      "2560 [D loss: -0.188451] [G loss: -2.469105]\n",
      "2561 [D loss: -0.659704] [G loss: -2.163836]\n",
      "2562 [D loss: 0.145433] [G loss: -2.563755]\n",
      "2563 [D loss: -0.256661] [G loss: -2.144210]\n",
      "2564 [D loss: -0.724604] [G loss: -2.379206]\n",
      "2565 [D loss: -0.081705] [G loss: -2.027549]\n",
      "2566 [D loss: -1.238614] [G loss: -2.199997]\n",
      "2567 [D loss: -0.650613] [G loss: -2.889650]\n",
      "2568 [D loss: 0.285342] [G loss: -2.585406]\n",
      "2569 [D loss: -1.099038] [G loss: -2.093548]\n",
      "2570 [D loss: -0.542669] [G loss: -2.575931]\n",
      "2571 [D loss: -0.553594] [G loss: -3.037531]\n",
      "2572 [D loss: 0.400224] [G loss: -2.442108]\n",
      "2573 [D loss: -0.009994] [G loss: -2.577309]\n",
      "2574 [D loss: -0.065464] [G loss: -2.533508]\n",
      "2575 [D loss: -0.836707] [G loss: -2.724918]\n",
      "2576 [D loss: 0.166192] [G loss: -2.675876]\n",
      "2577 [D loss: -0.789792] [G loss: -2.504117]\n",
      "2578 [D loss: -0.231840] [G loss: -2.699551]\n",
      "2579 [D loss: -0.838014] [G loss: -2.699936]\n",
      "2580 [D loss: -0.762979] [G loss: -2.836310]\n",
      "2581 [D loss: -0.277135] [G loss: -2.552421]\n",
      "2582 [D loss: -0.437048] [G loss: -2.867057]\n",
      "2583 [D loss: -0.915980] [G loss: -3.077311]\n",
      "2584 [D loss: -0.228129] [G loss: -2.803003]\n",
      "2585 [D loss: -0.956092] [G loss: -3.041330]\n",
      "2586 [D loss: -0.512616] [G loss: -2.410950]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2587 [D loss: -0.022238] [G loss: -3.427532]\n",
      "2588 [D loss: -1.135002] [G loss: -2.551661]\n",
      "2589 [D loss: -0.895322] [G loss: -2.826949]\n",
      "2590 [D loss: -0.735255] [G loss: -2.528460]\n",
      "2591 [D loss: -0.464322] [G loss: -3.087500]\n",
      "2592 [D loss: -1.002025] [G loss: -2.577790]\n",
      "2593 [D loss: -0.295367] [G loss: -2.780842]\n",
      "2594 [D loss: -0.383757] [G loss: -2.628974]\n",
      "2595 [D loss: 0.091811] [G loss: -2.634712]\n",
      "2596 [D loss: -0.155601] [G loss: -2.216531]\n",
      "2597 [D loss: -0.757965] [G loss: -2.599074]\n",
      "2598 [D loss: -0.389670] [G loss: -2.233825]\n",
      "2599 [D loss: -0.158039] [G loss: -2.048759]\n",
      "2600 [D loss: -0.207427] [G loss: -2.743587]\n",
      "2601 [D loss: -0.949204] [G loss: -2.818963]\n",
      "2602 [D loss: -1.128345] [G loss: -1.793543]\n",
      "2603 [D loss: -0.816962] [G loss: -2.938653]\n",
      "2604 [D loss: -0.522919] [G loss: -2.487713]\n",
      "2605 [D loss: -0.640048] [G loss: -2.302669]\n",
      "2606 [D loss: -0.916172] [G loss: -2.558714]\n",
      "2607 [D loss: 0.783831] [G loss: -2.546304]\n",
      "2608 [D loss: -0.422204] [G loss: -1.916369]\n",
      "2609 [D loss: -0.312467] [G loss: -2.697627]\n",
      "2610 [D loss: -0.477534] [G loss: -2.523541]\n",
      "2611 [D loss: -0.408722] [G loss: -2.105423]\n",
      "2612 [D loss: -0.376277] [G loss: -2.456303]\n",
      "2613 [D loss: -0.446239] [G loss: -2.387224]\n",
      "2614 [D loss: -0.089124] [G loss: -2.207131]\n",
      "2615 [D loss: -0.467575] [G loss: -2.072831]\n",
      "2616 [D loss: -0.294295] [G loss: -2.096734]\n",
      "2617 [D loss: -0.703222] [G loss: -2.273195]\n",
      "2618 [D loss: -0.393026] [G loss: -2.513329]\n",
      "2619 [D loss: -0.013200] [G loss: -2.251631]\n",
      "2620 [D loss: -0.502859] [G loss: -2.104522]\n",
      "2621 [D loss: -0.766400] [G loss: -2.247033]\n",
      "2622 [D loss: -0.675355] [G loss: -2.631256]\n",
      "2623 [D loss: -0.566090] [G loss: -2.071607]\n",
      "2624 [D loss: -0.586260] [G loss: -2.143389]\n",
      "2625 [D loss: -0.750131] [G loss: -1.968686]\n",
      "2626 [D loss: 0.001645] [G loss: -2.115717]\n",
      "2627 [D loss: -0.397511] [G loss: -2.187615]\n",
      "2628 [D loss: -0.622288] [G loss: -2.421543]\n",
      "2629 [D loss: -0.173596] [G loss: -2.351257]\n",
      "2630 [D loss: -0.114558] [G loss: -2.089847]\n",
      "2631 [D loss: -0.082585] [G loss: -2.246290]\n",
      "2632 [D loss: -1.020469] [G loss: -2.209238]\n",
      "2633 [D loss: 0.192794] [G loss: -2.336071]\n",
      "2634 [D loss: -0.790449] [G loss: -2.417492]\n",
      "2635 [D loss: -0.480506] [G loss: -1.737306]\n",
      "2636 [D loss: -1.047225] [G loss: -2.364949]\n",
      "2637 [D loss: -0.951657] [G loss: -2.674916]\n",
      "2638 [D loss: -0.041053] [G loss: -2.250069]\n",
      "2639 [D loss: -1.591373] [G loss: -1.997581]\n",
      "2640 [D loss: -0.888697] [G loss: -2.236480]\n",
      "2641 [D loss: 0.125165] [G loss: -1.997126]\n",
      "2642 [D loss: -0.431778] [G loss: -2.038151]\n",
      "2643 [D loss: -0.600491] [G loss: -2.039346]\n",
      "2644 [D loss: -0.458124] [G loss: -1.488611]\n",
      "2645 [D loss: -0.434177] [G loss: -1.977550]\n",
      "2646 [D loss: -1.902309] [G loss: -2.018728]\n",
      "2647 [D loss: -0.549247] [G loss: -1.758131]\n",
      "2648 [D loss: -0.669860] [G loss: -2.424795]\n",
      "2649 [D loss: -0.527177] [G loss: -2.515204]\n",
      "2650 [D loss: -0.440616] [G loss: -1.510696]\n",
      "2651 [D loss: -1.140287] [G loss: -2.151590]\n",
      "2652 [D loss: -0.100365] [G loss: -2.528066]\n",
      "2653 [D loss: -0.042943] [G loss: -2.086638]\n",
      "2654 [D loss: 0.614543] [G loss: -1.959713]\n",
      "2655 [D loss: -0.116500] [G loss: -1.794660]\n",
      "2656 [D loss: -0.225493] [G loss: -1.978994]\n",
      "2657 [D loss: -0.706164] [G loss: -1.827160]\n",
      "2658 [D loss: -0.445209] [G loss: -1.846839]\n",
      "2659 [D loss: -0.096281] [G loss: -1.535857]\n",
      "2660 [D loss: -0.720016] [G loss: -1.440560]\n",
      "2661 [D loss: -0.049823] [G loss: -1.437733]\n",
      "2662 [D loss: -0.068428] [G loss: -1.876369]\n",
      "2663 [D loss: -0.431619] [G loss: -1.850434]\n",
      "2664 [D loss: -1.232128] [G loss: -1.887976]\n",
      "2665 [D loss: -0.980295] [G loss: -1.611997]\n",
      "2666 [D loss: -0.069436] [G loss: -1.699095]\n",
      "2667 [D loss: -0.252191] [G loss: -1.833948]\n",
      "2668 [D loss: -0.488478] [G loss: -1.510956]\n",
      "2669 [D loss: -0.630288] [G loss: -1.830757]\n",
      "2670 [D loss: -0.624236] [G loss: -1.692792]\n",
      "2671 [D loss: -0.811840] [G loss: -2.055649]\n",
      "2672 [D loss: -0.614558] [G loss: -1.869132]\n",
      "2673 [D loss: -0.965345] [G loss: -1.681913]\n",
      "2674 [D loss: -0.582956] [G loss: -1.624723]\n",
      "2675 [D loss: -0.086118] [G loss: -1.419540]\n",
      "2676 [D loss: -0.323470] [G loss: -1.931420]\n",
      "2677 [D loss: -0.249689] [G loss: -1.731345]\n",
      "2678 [D loss: -0.672852] [G loss: -1.251000]\n",
      "2679 [D loss: -0.148278] [G loss: -1.408721]\n",
      "2680 [D loss: -0.574941] [G loss: -1.729989]\n",
      "2681 [D loss: -0.437319] [G loss: -1.926210]\n",
      "2682 [D loss: 0.136064] [G loss: -1.420630]\n",
      "2683 [D loss: -0.641962] [G loss: -1.545253]\n",
      "2684 [D loss: -0.975823] [G loss: -1.104446]\n",
      "2685 [D loss: -0.011066] [G loss: -1.968160]\n",
      "2686 [D loss: -1.022088] [G loss: -2.298201]\n",
      "2687 [D loss: -1.027033] [G loss: -1.623772]\n",
      "2688 [D loss: -1.010837] [G loss: -1.662538]\n",
      "2689 [D loss: -0.111262] [G loss: -1.296534]\n",
      "2690 [D loss: -0.486074] [G loss: -1.457594]\n",
      "2691 [D loss: 0.019958] [G loss: -1.917345]\n",
      "2692 [D loss: -0.199727] [G loss: -2.327056]\n",
      "2693 [D loss: -0.373743] [G loss: -1.798426]\n",
      "2694 [D loss: -0.628095] [G loss: -1.097306]\n",
      "2695 [D loss: -0.671153] [G loss: -1.562190]\n",
      "2696 [D loss: -0.566048] [G loss: -1.806257]\n",
      "2697 [D loss: -0.959870] [G loss: -2.029615]\n",
      "2698 [D loss: -0.344917] [G loss: -1.712579]\n",
      "2699 [D loss: -0.543623] [G loss: -2.095067]\n",
      "2700 [D loss: -0.314271] [G loss: -1.867567]\n",
      "2701 [D loss: -0.724579] [G loss: -1.378799]\n",
      "2702 [D loss: -0.233540] [G loss: -1.730458]\n",
      "2703 [D loss: -0.745497] [G loss: -1.674438]\n",
      "2704 [D loss: 0.098320] [G loss: -1.877981]\n",
      "2705 [D loss: -0.270636] [G loss: -1.486572]\n",
      "2706 [D loss: -1.247766] [G loss: -1.440482]\n",
      "2707 [D loss: -0.736304] [G loss: -1.988589]\n",
      "2708 [D loss: -0.257879] [G loss: -1.858755]\n",
      "2709 [D loss: -0.438513] [G loss: -2.226276]\n",
      "2710 [D loss: -1.411671] [G loss: -1.941875]\n",
      "2711 [D loss: -0.469932] [G loss: -1.547050]\n",
      "2712 [D loss: -0.425260] [G loss: -1.758003]\n",
      "2713 [D loss: -0.164419] [G loss: -1.904499]\n",
      "2714 [D loss: -0.607766] [G loss: -1.975099]\n",
      "2715 [D loss: -0.601281] [G loss: -1.723554]\n",
      "2716 [D loss: -0.238745] [G loss: -1.509077]\n",
      "2717 [D loss: -0.816798] [G loss: -1.501072]\n",
      "2718 [D loss: -0.100323] [G loss: -1.679028]\n",
      "2719 [D loss: -0.441307] [G loss: -1.836317]\n",
      "2720 [D loss: -1.087056] [G loss: -1.900098]\n",
      "2721 [D loss: -0.613112] [G loss: -1.629126]\n",
      "2722 [D loss: -0.508934] [G loss: -1.978880]\n",
      "2723 [D loss: -1.053255] [G loss: -1.680667]\n",
      "2724 [D loss: -0.695936] [G loss: -1.770021]\n",
      "2725 [D loss: -0.932907] [G loss: -1.642777]\n",
      "2726 [D loss: 0.013313] [G loss: -1.793679]\n",
      "2727 [D loss: -0.479085] [G loss: -1.256875]\n",
      "2728 [D loss: -0.847275] [G loss: -2.302198]\n",
      "2729 [D loss: -0.388264] [G loss: -2.136991]\n",
      "2730 [D loss: -0.244910] [G loss: -2.104290]\n",
      "2731 [D loss: -0.784437] [G loss: -1.753722]\n",
      "2732 [D loss: -0.716651] [G loss: -2.051554]\n",
      "2733 [D loss: -0.572639] [G loss: -1.854615]\n",
      "2734 [D loss: -0.103242] [G loss: -2.043422]\n",
      "2735 [D loss: -0.203553] [G loss: -1.644669]\n",
      "2736 [D loss: -0.656406] [G loss: -1.558759]\n",
      "2737 [D loss: -0.463456] [G loss: -1.735425]\n",
      "2738 [D loss: -1.008826] [G loss: -1.823239]\n",
      "2739 [D loss: 0.223878] [G loss: -1.913357]\n",
      "2740 [D loss: -0.943683] [G loss: -1.550949]\n",
      "2741 [D loss: 0.051559] [G loss: -1.398103]\n",
      "2742 [D loss: -0.484573] [G loss: -2.126299]\n",
      "2743 [D loss: -0.538161] [G loss: -1.951304]\n",
      "2744 [D loss: -0.665250] [G loss: -1.788632]\n",
      "2745 [D loss: -0.487827] [G loss: -2.044706]\n",
      "2746 [D loss: -0.011154] [G loss: -1.502660]\n",
      "2747 [D loss: -0.524913] [G loss: -2.187867]\n",
      "2748 [D loss: -0.755282] [G loss: -2.032164]\n",
      "2749 [D loss: -1.315708] [G loss: -1.861804]\n",
      "2750 [D loss: -0.729881] [G loss: -1.929712]\n",
      "2751 [D loss: 0.062395] [G loss: -1.888973]\n",
      "2752 [D loss: -1.282678] [G loss: -1.527960]\n",
      "2753 [D loss: -0.474956] [G loss: -2.617679]\n",
      "2754 [D loss: -0.094758] [G loss: -2.033494]\n",
      "2755 [D loss: -0.336269] [G loss: -2.012593]\n",
      "2756 [D loss: -1.002912] [G loss: -1.883326]\n",
      "2757 [D loss: -0.325122] [G loss: -1.664235]\n",
      "2758 [D loss: -0.378738] [G loss: -2.207953]\n",
      "2759 [D loss: -1.176093] [G loss: -1.590791]\n",
      "2760 [D loss: -0.813881] [G loss: -1.749406]\n",
      "2761 [D loss: -0.714598] [G loss: -1.888729]\n",
      "2762 [D loss: -0.457282] [G loss: -1.957832]\n",
      "2763 [D loss: -0.417032] [G loss: -1.666766]\n",
      "2764 [D loss: -0.132428] [G loss: -1.758225]\n",
      "2765 [D loss: -0.190111] [G loss: -2.138313]\n",
      "2766 [D loss: -0.549261] [G loss: -1.981664]\n",
      "2767 [D loss: -1.186762] [G loss: -1.221946]\n",
      "2768 [D loss: -0.495702] [G loss: -1.432200]\n",
      "2769 [D loss: -0.977154] [G loss: -1.818215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2770 [D loss: -0.515736] [G loss: -1.808630]\n",
      "2771 [D loss: -0.229639] [G loss: -1.334187]\n",
      "2772 [D loss: -1.272943] [G loss: -1.539988]\n",
      "2773 [D loss: -0.678001] [G loss: -2.090042]\n",
      "2774 [D loss: 0.508894] [G loss: -1.816962]\n",
      "2775 [D loss: -0.269314] [G loss: -2.091750]\n",
      "2776 [D loss: -0.293400] [G loss: -2.043352]\n",
      "2777 [D loss: -0.211729] [G loss: -2.206542]\n",
      "2778 [D loss: -0.564505] [G loss: -1.974314]\n",
      "2779 [D loss: -0.315506] [G loss: -1.895327]\n",
      "2780 [D loss: -0.486703] [G loss: -1.655749]\n",
      "2781 [D loss: -0.621419] [G loss: -1.806374]\n",
      "2782 [D loss: -0.914662] [G loss: -1.582647]\n",
      "2783 [D loss: -0.575426] [G loss: -1.838942]\n",
      "2784 [D loss: -0.347804] [G loss: -1.639193]\n",
      "2785 [D loss: -0.303407] [G loss: -1.819773]\n",
      "2786 [D loss: 0.164593] [G loss: -1.517576]\n",
      "2787 [D loss: 0.046832] [G loss: -1.470986]\n",
      "2788 [D loss: -0.551222] [G loss: -1.384371]\n",
      "2789 [D loss: -0.070516] [G loss: -1.922161]\n",
      "2790 [D loss: -0.482492] [G loss: -1.887630]\n",
      "2791 [D loss: -0.132974] [G loss: -1.777997]\n",
      "2792 [D loss: -1.203867] [G loss: -1.343299]\n",
      "2793 [D loss: -0.728896] [G loss: -1.792720]\n",
      "2794 [D loss: -0.022569] [G loss: -1.368882]\n",
      "2795 [D loss: -1.056894] [G loss: -1.402021]\n",
      "2796 [D loss: -0.202600] [G loss: -1.450056]\n",
      "2797 [D loss: -0.314269] [G loss: -2.040539]\n",
      "2798 [D loss: -0.889268] [G loss: -1.175536]\n",
      "2799 [D loss: -0.939527] [G loss: -1.230199]\n",
      "2800 [D loss: -1.476104] [G loss: -1.747982]\n",
      "2801 [D loss: -0.712802] [G loss: -1.734627]\n",
      "2802 [D loss: -0.989710] [G loss: -2.435848]\n",
      "2803 [D loss: -0.378602] [G loss: -1.793064]\n",
      "2804 [D loss: -0.830793] [G loss: -1.699988]\n",
      "2805 [D loss: -0.137093] [G loss: -1.607098]\n",
      "2806 [D loss: -0.985613] [G loss: -1.973390]\n",
      "2807 [D loss: -0.889620] [G loss: -1.764905]\n",
      "2808 [D loss: -0.557473] [G loss: -0.891559]\n",
      "2809 [D loss: -0.620268] [G loss: -1.495234]\n",
      "2810 [D loss: -1.487780] [G loss: -1.566302]\n",
      "2811 [D loss: -0.211911] [G loss: -1.863752]\n",
      "2812 [D loss: -0.340004] [G loss: -1.941815]\n",
      "2813 [D loss: -0.806042] [G loss: -1.823642]\n",
      "2814 [D loss: -0.683789] [G loss: -1.812414]\n",
      "2815 [D loss: -0.434637] [G loss: -2.180268]\n",
      "2816 [D loss: -0.827764] [G loss: -0.970856]\n",
      "2817 [D loss: -0.330307] [G loss: -1.867630]\n",
      "2818 [D loss: -0.038617] [G loss: -1.895921]\n",
      "2819 [D loss: -1.256782] [G loss: -1.447828]\n",
      "2820 [D loss: -0.147324] [G loss: -1.462864]\n",
      "2821 [D loss: -0.886862] [G loss: -1.585998]\n",
      "2822 [D loss: -0.940714] [G loss: -1.237559]\n",
      "2823 [D loss: -0.007764] [G loss: -1.478697]\n",
      "2824 [D loss: -0.564109] [G loss: -1.842125]\n",
      "2825 [D loss: -0.988399] [G loss: -1.623286]\n",
      "2826 [D loss: -0.805957] [G loss: -1.355937]\n",
      "2827 [D loss: -1.034063] [G loss: -1.484096]\n",
      "2828 [D loss: -0.737143] [G loss: -1.089193]\n",
      "2829 [D loss: -0.184607] [G loss: -1.631150]\n",
      "2830 [D loss: -0.776330] [G loss: -1.884426]\n",
      "2831 [D loss: -0.872025] [G loss: -1.475093]\n",
      "2832 [D loss: -0.219522] [G loss: -1.626984]\n",
      "2833 [D loss: -0.593337] [G loss: -1.700104]\n",
      "2834 [D loss: 0.004339] [G loss: -2.091839]\n",
      "2835 [D loss: 0.468380] [G loss: -1.371184]\n",
      "2836 [D loss: -0.969234] [G loss: -1.066240]\n",
      "2837 [D loss: 0.109255] [G loss: -1.688123]\n",
      "2838 [D loss: -0.145948] [G loss: -1.968208]\n",
      "2839 [D loss: -0.609680] [G loss: -1.408143]\n",
      "2840 [D loss: -0.254889] [G loss: -1.668865]\n",
      "2841 [D loss: -0.578233] [G loss: -1.766355]\n",
      "2842 [D loss: -0.825915] [G loss: -1.834991]\n",
      "2843 [D loss: -0.761237] [G loss: -1.128389]\n",
      "2844 [D loss: -0.383323] [G loss: -1.958534]\n",
      "2845 [D loss: -0.101211] [G loss: -1.637872]\n",
      "2846 [D loss: -0.457472] [G loss: -1.129807]\n",
      "2847 [D loss: -1.205325] [G loss: -1.512905]\n",
      "2848 [D loss: 0.133486] [G loss: -1.568036]\n",
      "2849 [D loss: -1.346686] [G loss: -0.945004]\n",
      "2850 [D loss: -0.674549] [G loss: -1.056897]\n",
      "2851 [D loss: -1.410080] [G loss: -1.532639]\n",
      "2852 [D loss: -1.269361] [G loss: -1.475375]\n",
      "2853 [D loss: -0.053114] [G loss: -1.482653]\n",
      "2854 [D loss: -0.864228] [G loss: -1.407676]\n",
      "2855 [D loss: -0.967426] [G loss: -1.169987]\n",
      "2856 [D loss: -0.781369] [G loss: -1.280346]\n",
      "2857 [D loss: -1.455241] [G loss: -1.772558]\n",
      "2858 [D loss: -0.613872] [G loss: -0.943198]\n",
      "2859 [D loss: -0.178708] [G loss: -1.813246]\n",
      "2860 [D loss: 0.056090] [G loss: -1.017030]\n",
      "2861 [D loss: 0.129743] [G loss: -2.033956]\n",
      "2862 [D loss: -0.793028] [G loss: -1.730908]\n",
      "2863 [D loss: -0.964451] [G loss: -1.238064]\n",
      "2864 [D loss: 0.003526] [G loss: -1.657550]\n",
      "2865 [D loss: -0.616696] [G loss: -1.351772]\n",
      "2866 [D loss: -0.115166] [G loss: -1.168945]\n",
      "2867 [D loss: 0.084106] [G loss: -2.098372]\n",
      "2868 [D loss: -0.327123] [G loss: -1.766096]\n",
      "2869 [D loss: -0.823054] [G loss: -1.530213]\n",
      "2870 [D loss: -0.374547] [G loss: -1.484385]\n",
      "2871 [D loss: -1.057045] [G loss: -2.307105]\n",
      "2872 [D loss: -1.002697] [G loss: -1.532361]\n",
      "2873 [D loss: -0.264556] [G loss: -2.112244]\n",
      "2874 [D loss: -0.735113] [G loss: -1.626528]\n",
      "2875 [D loss: -0.745636] [G loss: -1.919398]\n",
      "2876 [D loss: -0.457022] [G loss: -2.070958]\n",
      "2877 [D loss: -0.346725] [G loss: -1.872097]\n",
      "2878 [D loss: -0.672956] [G loss: -2.192910]\n",
      "2879 [D loss: -0.715945] [G loss: -1.571397]\n",
      "2880 [D loss: -0.120188] [G loss: -2.189149]\n",
      "2881 [D loss: -0.691185] [G loss: -1.699461]\n",
      "2882 [D loss: -0.771687] [G loss: -1.960327]\n",
      "2883 [D loss: -0.618071] [G loss: -2.122161]\n",
      "2884 [D loss: -1.209858] [G loss: -1.816299]\n",
      "2885 [D loss: -0.570867] [G loss: -1.928099]\n",
      "2886 [D loss: -0.609208] [G loss: -2.063932]\n",
      "2887 [D loss: -0.728736] [G loss: -2.219042]\n",
      "2888 [D loss: -0.495782] [G loss: -1.567919]\n",
      "2889 [D loss: -0.743530] [G loss: -2.286157]\n",
      "2890 [D loss: -0.659631] [G loss: -2.001473]\n",
      "2891 [D loss: -0.794063] [G loss: -2.057066]\n",
      "2892 [D loss: -1.172750] [G loss: -2.146605]\n",
      "2893 [D loss: -0.822129] [G loss: -1.494961]\n",
      "2894 [D loss: -0.966154] [G loss: -2.104012]\n",
      "2895 [D loss: -0.708549] [G loss: -1.985171]\n",
      "2896 [D loss: -0.709207] [G loss: -2.316441]\n",
      "2897 [D loss: -1.006050] [G loss: -1.913541]\n",
      "2898 [D loss: -0.296453] [G loss: -1.880645]\n",
      "2899 [D loss: -0.609212] [G loss: -2.338472]\n",
      "2900 [D loss: -0.627187] [G loss: -1.956239]\n",
      "2901 [D loss: -0.320505] [G loss: -2.095841]\n",
      "2902 [D loss: -0.713645] [G loss: -1.382113]\n",
      "2903 [D loss: -0.929269] [G loss: -1.928431]\n",
      "2904 [D loss: -0.149837] [G loss: -1.934345]\n",
      "2905 [D loss: -0.282417] [G loss: -1.599667]\n",
      "2906 [D loss: -0.119658] [G loss: -1.867447]\n",
      "2907 [D loss: -1.467691] [G loss: -1.649921]\n",
      "2908 [D loss: -0.632927] [G loss: -2.364534]\n",
      "2909 [D loss: 0.302208] [G loss: -2.248659]\n",
      "2910 [D loss: -0.197096] [G loss: -1.439265]\n",
      "2911 [D loss: -0.118100] [G loss: -1.727388]\n",
      "2912 [D loss: -1.151014] [G loss: -2.180847]\n",
      "2913 [D loss: -0.722761] [G loss: -1.681452]\n",
      "2914 [D loss: -0.334857] [G loss: -2.227129]\n",
      "2915 [D loss: -0.080401] [G loss: -1.898348]\n",
      "2916 [D loss: -1.079514] [G loss: -1.992042]\n",
      "2917 [D loss: -0.559653] [G loss: -2.113704]\n",
      "2918 [D loss: -0.323797] [G loss: -1.780717]\n",
      "2919 [D loss: -0.643303] [G loss: -1.855998]\n",
      "2920 [D loss: -0.772120] [G loss: -1.576536]\n",
      "2921 [D loss: -0.063664] [G loss: -1.509131]\n",
      "2922 [D loss: -0.318182] [G loss: -1.468558]\n",
      "2923 [D loss: -0.720183] [G loss: -1.469416]\n",
      "2924 [D loss: 0.180770] [G loss: -1.968705]\n",
      "2925 [D loss: -0.615520] [G loss: -2.307209]\n",
      "2926 [D loss: -0.488948] [G loss: -1.630754]\n",
      "2927 [D loss: -0.731706] [G loss: -1.680320]\n",
      "2928 [D loss: -0.716067] [G loss: -1.093803]\n",
      "2929 [D loss: -0.848028] [G loss: -1.745433]\n",
      "2930 [D loss: -0.528714] [G loss: -1.763672]\n",
      "2931 [D loss: -0.839548] [G loss: -1.320344]\n",
      "2932 [D loss: -0.657018] [G loss: -1.818101]\n",
      "2933 [D loss: -1.003998] [G loss: -1.652515]\n",
      "2934 [D loss: -0.641312] [G loss: -1.910709]\n",
      "2935 [D loss: -0.751271] [G loss: -2.029704]\n",
      "2936 [D loss: -1.061153] [G loss: -1.520210]\n",
      "2937 [D loss: -0.700151] [G loss: -1.413185]\n",
      "2938 [D loss: -0.374473] [G loss: -1.591023]\n",
      "2939 [D loss: -0.413290] [G loss: -1.374170]\n",
      "2940 [D loss: 0.611761] [G loss: -1.583312]\n",
      "2941 [D loss: -0.598358] [G loss: -1.795547]\n",
      "2942 [D loss: -0.921910] [G loss: -1.442775]\n",
      "2943 [D loss: -0.804484] [G loss: -1.315725]\n",
      "2944 [D loss: -0.643422] [G loss: -1.731080]\n",
      "2945 [D loss: -0.654412] [G loss: -1.408348]\n",
      "2946 [D loss: -0.131288] [G loss: -1.638455]\n",
      "2947 [D loss: -1.229413] [G loss: -2.116314]\n",
      "2948 [D loss: -0.312897] [G loss: -1.543633]\n",
      "2949 [D loss: 0.124563] [G loss: -1.725711]\n",
      "2950 [D loss: 0.084689] [G loss: -1.446599]\n",
      "2951 [D loss: -1.037443] [G loss: -1.757298]\n",
      "2952 [D loss: -1.071888] [G loss: -1.336133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2953 [D loss: -0.905535] [G loss: -1.091216]\n",
      "2954 [D loss: -0.549705] [G loss: -1.695293]\n",
      "2955 [D loss: 0.094844] [G loss: -1.161019]\n",
      "2956 [D loss: -0.966454] [G loss: -0.985748]\n",
      "2957 [D loss: -0.745350] [G loss: -1.092926]\n",
      "2958 [D loss: -0.684335] [G loss: -1.239940]\n",
      "2959 [D loss: -0.959831] [G loss: -1.067470]\n",
      "2960 [D loss: -1.015069] [G loss: -1.386908]\n",
      "2961 [D loss: -1.111937] [G loss: -1.353749]\n",
      "2962 [D loss: -0.330204] [G loss: -1.453471]\n",
      "2963 [D loss: 0.075954] [G loss: -1.356759]\n",
      "2964 [D loss: -0.258247] [G loss: -1.669227]\n",
      "2965 [D loss: -0.163486] [G loss: -1.205422]\n",
      "2966 [D loss: -1.168602] [G loss: -1.359461]\n",
      "2967 [D loss: -0.563122] [G loss: -0.920528]\n",
      "2968 [D loss: -1.211767] [G loss: -1.456300]\n",
      "2969 [D loss: -0.387015] [G loss: -1.049794]\n",
      "2970 [D loss: -0.642056] [G loss: -1.412410]\n",
      "2971 [D loss: -0.505135] [G loss: -1.436659]\n",
      "2972 [D loss: -0.590590] [G loss: -1.478811]\n",
      "2973 [D loss: -0.506580] [G loss: -1.328103]\n",
      "2974 [D loss: -0.704124] [G loss: -1.559672]\n",
      "2975 [D loss: -0.677615] [G loss: -1.454115]\n",
      "2976 [D loss: -0.466800] [G loss: -1.370830]\n",
      "2977 [D loss: -0.082604] [G loss: -2.028237]\n",
      "2978 [D loss: -0.463974] [G loss: -1.296134]\n",
      "2979 [D loss: -0.790727] [G loss: -0.874409]\n",
      "2980 [D loss: -1.097010] [G loss: -1.189600]\n",
      "2981 [D loss: -0.005917] [G loss: -2.062791]\n",
      "2982 [D loss: -0.151954] [G loss: -1.646266]\n",
      "2983 [D loss: -0.387013] [G loss: -1.262791]\n",
      "2984 [D loss: -0.576708] [G loss: -1.611378]\n",
      "2985 [D loss: -0.568962] [G loss: -1.593981]\n",
      "2986 [D loss: -0.068945] [G loss: -1.463562]\n",
      "2987 [D loss: -0.005702] [G loss: -1.467307]\n",
      "2988 [D loss: -1.101824] [G loss: -1.444193]\n",
      "2989 [D loss: -0.023116] [G loss: -1.693723]\n",
      "2990 [D loss: -0.757622] [G loss: -1.754958]\n",
      "2991 [D loss: -0.310395] [G loss: -1.811286]\n",
      "2992 [D loss: -0.223981] [G loss: -2.135082]\n",
      "2993 [D loss: -0.943357] [G loss: -1.712732]\n",
      "2994 [D loss: -0.605464] [G loss: -1.755510]\n",
      "2995 [D loss: -0.435214] [G loss: -1.840193]\n",
      "2996 [D loss: 0.056878] [G loss: -1.599971]\n",
      "2997 [D loss: -0.558152] [G loss: -2.018631]\n",
      "2998 [D loss: -1.213507] [G loss: -1.791776]\n",
      "2999 [D loss: -0.645713] [G loss: -2.253866]\n"
     ]
    }
   ],
   "source": [
    "# epochs=30000\n",
    "epochs=3000\n",
    "train(G, critic_model, generator_model, \n",
    "      n_critic, latent_dim,\n",
    "      epochs=epochs, batch_size=32, sample_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
