{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wasserstein GAN - Improved\n",
    "\n",
    "ref.:\n",
    "GULRAJANI, Ishaan et al.  \n",
    "Improved training of wasserstein gans.  \n",
    "In: Advances in neural information processing systems. 2017. p. 5767-5777.\n",
    "\n",
    "![new objective](wgan_gp_objective.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers.merge import _Merge\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import RMSprop\n",
    "from functools import partial\n",
    "\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWeightedAverage(_Merge):\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "    def _merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((32, 1, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(latent_dim, channels):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=latent_dim))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128, kernel_size=4, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(64, kernel_size=4, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv2D(channels, kernel_size=4, padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "    img = model(noise)\n",
    "\n",
    "    return Model(noise, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_critic(img_shape):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=img_shape, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "    validity = model(img)\n",
    "\n",
    "    return Model(img, validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty_loss(y_true, y_pred, averaged_samples):\n",
    "    \"\"\"\n",
    "    Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "    \"\"\"\n",
    "    gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "    # compute the euclidean norm by squaring ...\n",
    "    gradients_sqr = K.square(gradients)\n",
    "    #   ... summing over the rows ...\n",
    "    gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                              axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "    #   ... and sqrt\n",
    "    gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "    # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "    gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "    # return the mean as loss over all the batch samples\n",
    "    return K.mean(gradient_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_loss(y_true, y_pred):\n",
    "    \"\"\" same loss as in WGAN \"\"\"\n",
    "    return K.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(G, latent_dim, epoch):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
    "    gen_imgs = G.predict(noise)\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(G, critic_model, generator_model, \n",
    "          n_critic, latent_dim,\n",
    "          epochs, batch_size, sample_interval=50):\n",
    "\n",
    "    # Load the dataset\n",
    "    (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "    # Rescale -1 to 1\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "    # Adversarial ground truths\n",
    "    valid = -np.ones((batch_size, 1))\n",
    "    fake =  np.ones((batch_size, 1))\n",
    "    dummy = np.zeros((batch_size, 1)) # Dummy gt for gradient penalty\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for _ in range(n_critic):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "            # Sample generator input\n",
    "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "            # Train the critic\n",
    "            d_loss = critic_model.train_on_batch([imgs, noise],\n",
    "                                                 [valid, fake, dummy])\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "\n",
    "        g_loss = generator_model.train_on_batch(noise, valid)\n",
    "\n",
    "        # Plot the progress\n",
    "        print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, d_loss[0], g_loss))\n",
    "\n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % sample_interval == 0 or epoch == epochs - 1:\n",
    "            sample_images(G, latent_dim, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('images'):\n",
    "    os.makedirs('images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following parameter and optimizer set as recommended in paper\n",
    "n_critic = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# create optimizer\n",
    "optimizer = RMSprop(lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 64)        131136    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 1)         1025      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,028,673\n",
      "Trainable params: 1,028,289\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the generator model\n",
    "G = build_generator(latent_dim, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 16)        160       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 32)          4640      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 100,097\n",
      "Trainable params: 99,649\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the critic model\n",
    "C = build_critic(img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# Construct Computational Graph\n",
    "#       for the Critic\n",
    "#-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze generator's layers while training critic\n",
    "G.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image input (real sample)\n",
    "real_img = Input(shape=img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise input\n",
    "z_disc = Input(shape=(latent_dim,))\n",
    "\n",
    "# Generate image based of noise (fake sample)\n",
    "fake_img = G(z_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator determines validity of the real and fake images\n",
    "fake = C(fake_img)\n",
    "valid = C(real_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct weighted average between real and fake images\n",
    "interpolated_img = RandomWeightedAverage()([real_img, fake_img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine validity of weighted sample\n",
    "validity_interpolated = C(interpolated_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Python partial to provide loss function with additional\n",
    "# 'averaged_samples' argument\n",
    "partial_gp_loss = partial(gradient_penalty_loss,\n",
    "                          averaged_samples=interpolated_img)\n",
    "\n",
    "partial_gp_loss.__name__ = 'gradient_penalty' # Keras requires function names\n",
    "\n",
    "critic_model = Model(inputs=[real_img, z_disc],\n",
    "                     outputs=[valid, fake, validity_interpolated])\n",
    "\n",
    "critic_model.compile(loss=[wasserstein_loss,\n",
    "                           wasserstein_loss,\n",
    "                           partial_gp_loss],\n",
    "                     optimizer=optimizer,\n",
    "                     loss_weights=[1, 1, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# Construct Computational Graph\n",
    "#         for Generator\n",
    "#-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the generator we freeze the critic's layers\n",
    "C.trainable = False\n",
    "G.trainable = True  # this is the default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampled noise for input to generator\n",
    "z_gen = Input(shape=(latent_dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images based of noise\n",
    "img = G(z_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator determines validity\n",
    "valid = C(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines generator model\n",
    "generator_model = Model(z_gen, valid)\n",
    "generator_model.compile(loss=wasserstein_loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 25.070177] [G loss: 0.297224]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 15.671294] [G loss: 0.142512]\n",
      "2 [D loss: 15.337543] [G loss: 0.230281]\n",
      "3 [D loss: 12.432339] [G loss: 0.056703]\n",
      "4 [D loss: 10.851179] [G loss: 0.147348]\n",
      "5 [D loss: 8.952878] [G loss: 0.442589]\n",
      "6 [D loss: 7.261176] [G loss: -0.013684]\n",
      "7 [D loss: 6.972205] [G loss: 0.325016]\n",
      "8 [D loss: 5.035443] [G loss: 0.185003]\n",
      "9 [D loss: 5.259032] [G loss: 0.004149]\n",
      "10 [D loss: 4.227239] [G loss: 0.058557]\n",
      "11 [D loss: 3.524243] [G loss: 0.075562]\n",
      "12 [D loss: 3.562151] [G loss: 0.075373]\n",
      "13 [D loss: 1.219465] [G loss: 0.086477]\n",
      "14 [D loss: 1.879110] [G loss: -0.013041]\n",
      "15 [D loss: 1.456353] [G loss: 0.004118]\n",
      "16 [D loss: 1.266511] [G loss: -0.044474]\n",
      "17 [D loss: 0.833396] [G loss: -0.189196]\n",
      "18 [D loss: 0.918401] [G loss: -0.054217]\n",
      "19 [D loss: 0.098476] [G loss: -0.063081]\n",
      "20 [D loss: 0.282075] [G loss: -0.028315]\n",
      "21 [D loss: 0.161475] [G loss: -0.138403]\n",
      "22 [D loss: -0.262619] [G loss: -0.491620]\n",
      "23 [D loss: -0.266399] [G loss: -0.451789]\n",
      "24 [D loss: -0.206631] [G loss: -0.407562]\n",
      "25 [D loss: -0.554200] [G loss: -0.513265]\n",
      "26 [D loss: -0.001416] [G loss: -0.477432]\n",
      "27 [D loss: -0.386792] [G loss: -0.564311]\n",
      "28 [D loss: -0.389767] [G loss: -0.592039]\n",
      "29 [D loss: -0.535728] [G loss: -0.624239]\n",
      "30 [D loss: -0.732063] [G loss: -0.725365]\n",
      "31 [D loss: -0.777627] [G loss: -0.560101]\n",
      "32 [D loss: -0.686989] [G loss: -0.901833]\n",
      "33 [D loss: -1.106806] [G loss: -0.875886]\n",
      "34 [D loss: -1.073751] [G loss: -0.779835]\n",
      "35 [D loss: -1.119061] [G loss: -0.848128]\n",
      "36 [D loss: -1.001735] [G loss: -0.944344]\n",
      "37 [D loss: -1.090034] [G loss: -1.240726]\n",
      "38 [D loss: -1.390110] [G loss: -0.889298]\n",
      "39 [D loss: -1.142889] [G loss: -1.267280]\n",
      "40 [D loss: -1.069970] [G loss: -1.202893]\n",
      "41 [D loss: -1.696560] [G loss: -1.232475]\n",
      "42 [D loss: -0.817946] [G loss: -1.487631]\n",
      "43 [D loss: -1.109610] [G loss: -1.487747]\n",
      "44 [D loss: -0.902185] [G loss: -1.863937]\n",
      "45 [D loss: -1.042187] [G loss: -1.668095]\n",
      "46 [D loss: -1.067435] [G loss: -1.604807]\n",
      "47 [D loss: -0.986146] [G loss: -1.412992]\n",
      "48 [D loss: -1.279378] [G loss: -1.488737]\n",
      "49 [D loss: -0.766385] [G loss: -1.876685]\n",
      "50 [D loss: -0.876564] [G loss: -1.764488]\n",
      "51 [D loss: -0.981649] [G loss: -2.069877]\n",
      "52 [D loss: -1.021934] [G loss: -2.036800]\n",
      "53 [D loss: -1.408833] [G loss: -1.929405]\n",
      "54 [D loss: -1.061918] [G loss: -2.361048]\n",
      "55 [D loss: -0.741099] [G loss: -2.426511]\n",
      "56 [D loss: -1.278593] [G loss: -2.325201]\n",
      "57 [D loss: -0.558210] [G loss: -2.457148]\n",
      "58 [D loss: -0.520872] [G loss: -2.451554]\n",
      "59 [D loss: -0.920217] [G loss: -2.504654]\n",
      "60 [D loss: -0.946360] [G loss: -2.753081]\n",
      "61 [D loss: -0.320158] [G loss: -2.819373]\n",
      "62 [D loss: -0.816941] [G loss: -2.799486]\n",
      "63 [D loss: -0.747646] [G loss: -2.940184]\n",
      "64 [D loss: -0.399154] [G loss: -3.141222]\n",
      "65 [D loss: -0.812684] [G loss: -2.965510]\n",
      "66 [D loss: -0.385940] [G loss: -3.010318]\n",
      "67 [D loss: -0.498866] [G loss: -2.789148]\n",
      "68 [D loss: -0.406339] [G loss: -3.174393]\n",
      "69 [D loss: -0.295409] [G loss: -2.879430]\n",
      "70 [D loss: -0.076857] [G loss: -3.123940]\n",
      "71 [D loss: -0.517318] [G loss: -2.924675]\n",
      "72 [D loss: 0.073025] [G loss: -3.369738]\n",
      "73 [D loss: -0.100644] [G loss: -3.298262]\n",
      "74 [D loss: -0.760617] [G loss: -3.505100]\n",
      "75 [D loss: -0.331788] [G loss: -3.401697]\n",
      "76 [D loss: -0.837633] [G loss: -2.992279]\n",
      "77 [D loss: -0.716409] [G loss: -3.102402]\n",
      "78 [D loss: -0.534149] [G loss: -3.440134]\n",
      "79 [D loss: -0.454453] [G loss: -3.476396]\n",
      "80 [D loss: -0.441445] [G loss: -3.328845]\n",
      "81 [D loss: -0.537797] [G loss: -3.044704]\n",
      "82 [D loss: -0.417576] [G loss: -3.221294]\n",
      "83 [D loss: -0.676270] [G loss: -3.551393]\n",
      "84 [D loss: -0.466822] [G loss: -3.284372]\n",
      "85 [D loss: -0.227881] [G loss: -3.160799]\n",
      "86 [D loss: -0.497008] [G loss: -3.105584]\n",
      "87 [D loss: -0.294864] [G loss: -3.375686]\n",
      "88 [D loss: -0.251434] [G loss: -3.162066]\n",
      "89 [D loss: -0.656951] [G loss: -3.040349]\n",
      "90 [D loss: 0.006204] [G loss: -3.363696]\n",
      "91 [D loss: -0.522213] [G loss: -3.286977]\n",
      "92 [D loss: -0.470905] [G loss: -2.729548]\n",
      "93 [D loss: -0.570134] [G loss: -2.917490]\n",
      "94 [D loss: -0.287203] [G loss: -3.037196]\n",
      "95 [D loss: -0.472581] [G loss: -2.889572]\n",
      "96 [D loss: -0.670858] [G loss: -2.870620]\n",
      "97 [D loss: -0.573126] [G loss: -2.891487]\n",
      "98 [D loss: -0.297690] [G loss: -2.794147]\n",
      "99 [D loss: -0.080689] [G loss: -2.799009]\n",
      "100 [D loss: -0.179960] [G loss: -2.837023]\n",
      "101 [D loss: -0.234624] [G loss: -2.406060]\n",
      "102 [D loss: -0.317829] [G loss: -2.348829]\n",
      "103 [D loss: -0.458314] [G loss: -2.453546]\n",
      "104 [D loss: -0.457783] [G loss: -2.248569]\n",
      "105 [D loss: -0.483339] [G loss: -2.520058]\n",
      "106 [D loss: -0.578985] [G loss: -2.404913]\n",
      "107 [D loss: -0.516754] [G loss: -2.378740]\n",
      "108 [D loss: -0.264944] [G loss: -2.346348]\n",
      "109 [D loss: -0.205971] [G loss: -2.278308]\n",
      "110 [D loss: -0.187744] [G loss: -2.469878]\n",
      "111 [D loss: -0.394332] [G loss: -2.024429]\n",
      "112 [D loss: -0.768547] [G loss: -1.979805]\n",
      "113 [D loss: -0.224861] [G loss: -1.985562]\n",
      "114 [D loss: -0.404910] [G loss: -1.862610]\n",
      "115 [D loss: -0.300151] [G loss: -1.775937]\n",
      "116 [D loss: -0.570693] [G loss: -2.111527]\n",
      "117 [D loss: -0.502947] [G loss: -1.839322]\n",
      "118 [D loss: -0.823071] [G loss: -1.443584]\n",
      "119 [D loss: -0.553467] [G loss: -1.597898]\n",
      "120 [D loss: -0.547142] [G loss: -1.742411]\n",
      "121 [D loss: -0.654853] [G loss: -1.700079]\n",
      "122 [D loss: -0.503456] [G loss: -1.666211]\n",
      "123 [D loss: -0.646529] [G loss: -1.463672]\n",
      "124 [D loss: -0.361847] [G loss: -1.402239]\n",
      "125 [D loss: -0.857639] [G loss: -1.618027]\n",
      "126 [D loss: -0.765662] [G loss: -1.279636]\n",
      "127 [D loss: -0.571169] [G loss: -1.288763]\n",
      "128 [D loss: -0.867947] [G loss: -1.475717]\n",
      "129 [D loss: -0.232170] [G loss: -1.358266]\n",
      "130 [D loss: -0.226698] [G loss: -1.325804]\n",
      "131 [D loss: -0.394060] [G loss: -1.381486]\n",
      "132 [D loss: -0.695428] [G loss: -1.152539]\n",
      "133 [D loss: -0.368854] [G loss: -1.596550]\n",
      "134 [D loss: -0.286806] [G loss: -1.215351]\n",
      "135 [D loss: -0.698393] [G loss: -1.068768]\n",
      "136 [D loss: 0.052721] [G loss: -1.044202]\n",
      "137 [D loss: -0.288236] [G loss: -1.110865]\n",
      "138 [D loss: -0.717860] [G loss: -1.341444]\n",
      "139 [D loss: -0.527562] [G loss: -1.278962]\n",
      "140 [D loss: -0.061323] [G loss: -1.339512]\n",
      "141 [D loss: -0.307274] [G loss: -1.198379]\n",
      "142 [D loss: -0.631974] [G loss: -1.411343]\n",
      "143 [D loss: -0.716174] [G loss: -1.154721]\n",
      "144 [D loss: -0.798963] [G loss: -1.162143]\n",
      "145 [D loss: -0.738029] [G loss: -1.194636]\n",
      "146 [D loss: -0.553283] [G loss: -1.526354]\n",
      "147 [D loss: -0.788101] [G loss: -1.333618]\n",
      "148 [D loss: -0.328646] [G loss: -1.276018]\n",
      "149 [D loss: -0.057140] [G loss: -1.564731]\n",
      "150 [D loss: -0.458648] [G loss: -1.436762]\n",
      "151 [D loss: -0.334786] [G loss: -1.328761]\n",
      "152 [D loss: -0.559190] [G loss: -1.224076]\n",
      "153 [D loss: -0.702486] [G loss: -1.284237]\n",
      "154 [D loss: -0.289296] [G loss: -1.004591]\n",
      "155 [D loss: -0.627247] [G loss: -1.317678]\n",
      "156 [D loss: -0.670819] [G loss: -1.526887]\n",
      "157 [D loss: -0.402191] [G loss: -1.479605]\n",
      "158 [D loss: -0.393368] [G loss: -1.306712]\n",
      "159 [D loss: -0.416412] [G loss: -1.634306]\n",
      "160 [D loss: -0.485812] [G loss: -1.819059]\n",
      "161 [D loss: -1.011636] [G loss: -1.858888]\n",
      "162 [D loss: -0.138104] [G loss: -1.585411]\n",
      "163 [D loss: -0.374079] [G loss: -1.690820]\n",
      "164 [D loss: -0.673381] [G loss: -1.790972]\n",
      "165 [D loss: -0.434256] [G loss: -1.750434]\n",
      "166 [D loss: -0.479558] [G loss: -1.918026]\n",
      "167 [D loss: -0.145071] [G loss: -1.819326]\n",
      "168 [D loss: -0.409559] [G loss: -1.901745]\n",
      "169 [D loss: -0.426218] [G loss: -1.786627]\n",
      "170 [D loss: -0.533897] [G loss: -1.853959]\n",
      "171 [D loss: -0.521429] [G loss: -2.007041]\n",
      "172 [D loss: -0.662314] [G loss: -1.810603]\n",
      "173 [D loss: -0.623940] [G loss: -2.000686]\n",
      "174 [D loss: -0.448554] [G loss: -1.901991]\n",
      "175 [D loss: -0.184319] [G loss: -2.183244]\n",
      "176 [D loss: -0.716309] [G loss: -1.895703]\n",
      "177 [D loss: -0.782236] [G loss: -2.251605]\n",
      "178 [D loss: 0.063282] [G loss: -2.250019]\n",
      "179 [D loss: -0.091538] [G loss: -2.013003]\n",
      "180 [D loss: -0.857296] [G loss: -2.087917]\n",
      "181 [D loss: -0.508678] [G loss: -2.169069]\n",
      "182 [D loss: -0.120650] [G loss: -2.243894]\n",
      "183 [D loss: -0.530328] [G loss: -2.273810]\n",
      "184 [D loss: -0.326380] [G loss: -2.340534]\n",
      "185 [D loss: -0.270462] [G loss: -2.345347]\n",
      "186 [D loss: -0.454020] [G loss: -2.349087]\n",
      "187 [D loss: -0.335371] [G loss: -2.275207]\n",
      "188 [D loss: -0.425433] [G loss: -2.650778]\n",
      "189 [D loss: -0.436790] [G loss: -2.351426]\n",
      "190 [D loss: -0.495283] [G loss: -2.552995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191 [D loss: -1.064137] [G loss: -2.537212]\n",
      "192 [D loss: -0.683361] [G loss: -2.275415]\n",
      "193 [D loss: -0.389910] [G loss: -2.385437]\n",
      "194 [D loss: -0.748865] [G loss: -2.279803]\n",
      "195 [D loss: -0.814931] [G loss: -2.298585]\n",
      "196 [D loss: -0.349325] [G loss: -2.511345]\n",
      "197 [D loss: -0.440804] [G loss: -2.617544]\n",
      "198 [D loss: -0.169316] [G loss: -2.125013]\n",
      "199 [D loss: -0.246127] [G loss: -2.563831]\n",
      "200 [D loss: -0.301772] [G loss: -2.467508]\n",
      "201 [D loss: -0.017004] [G loss: -2.208903]\n",
      "202 [D loss: -0.580021] [G loss: -2.741154]\n",
      "203 [D loss: -0.481001] [G loss: -2.209312]\n",
      "204 [D loss: -0.145891] [G loss: -2.632309]\n",
      "205 [D loss: -0.099586] [G loss: -2.275704]\n",
      "206 [D loss: -0.447367] [G loss: -2.638948]\n",
      "207 [D loss: -0.253114] [G loss: -2.339246]\n",
      "208 [D loss: 0.010422] [G loss: -2.480601]\n",
      "209 [D loss: -0.445769] [G loss: -2.459414]\n",
      "210 [D loss: -0.323106] [G loss: -2.534847]\n",
      "211 [D loss: -0.299835] [G loss: -2.672637]\n",
      "212 [D loss: -0.196829] [G loss: -2.628883]\n",
      "213 [D loss: -0.516484] [G loss: -2.755405]\n",
      "214 [D loss: -0.312235] [G loss: -2.353329]\n",
      "215 [D loss: -0.379911] [G loss: -2.637338]\n",
      "216 [D loss: -0.388904] [G loss: -2.518440]\n",
      "217 [D loss: -0.365346] [G loss: -2.408177]\n",
      "218 [D loss: -0.509035] [G loss: -2.530439]\n",
      "219 [D loss: -0.646523] [G loss: -3.026291]\n",
      "220 [D loss: -0.335135] [G loss: -2.687396]\n",
      "221 [D loss: 0.162689] [G loss: -2.794325]\n",
      "222 [D loss: -0.667179] [G loss: -2.891233]\n",
      "223 [D loss: -0.558202] [G loss: -2.448424]\n",
      "224 [D loss: -0.603871] [G loss: -2.797119]\n",
      "225 [D loss: -0.402640] [G loss: -2.541808]\n",
      "226 [D loss: -0.512524] [G loss: -2.633786]\n",
      "227 [D loss: -0.814924] [G loss: -2.712926]\n",
      "228 [D loss: -0.391670] [G loss: -2.910740]\n",
      "229 [D loss: 0.074299] [G loss: -2.816163]\n",
      "230 [D loss: -0.086976] [G loss: -2.577815]\n",
      "231 [D loss: -0.154412] [G loss: -2.697284]\n",
      "232 [D loss: -0.467240] [G loss: -2.421170]\n",
      "233 [D loss: -0.816361] [G loss: -2.400041]\n",
      "234 [D loss: -0.419281] [G loss: -2.651496]\n",
      "235 [D loss: -0.641996] [G loss: -2.392639]\n",
      "236 [D loss: -0.177209] [G loss: -2.406008]\n",
      "237 [D loss: -0.015112] [G loss: -2.795547]\n",
      "238 [D loss: 0.347550] [G loss: -2.782946]\n",
      "239 [D loss: -0.325163] [G loss: -2.720174]\n",
      "240 [D loss: -0.134486] [G loss: -2.568514]\n",
      "241 [D loss: -0.211495] [G loss: -2.324424]\n",
      "242 [D loss: -0.280538] [G loss: -2.668126]\n",
      "243 [D loss: -0.275861] [G loss: -2.362526]\n",
      "244 [D loss: -0.578141] [G loss: -2.524907]\n",
      "245 [D loss: -0.176826] [G loss: -2.627517]\n",
      "246 [D loss: -0.528135] [G loss: -2.782127]\n",
      "247 [D loss: -0.499706] [G loss: -2.562051]\n",
      "248 [D loss: -0.181750] [G loss: -2.601048]\n",
      "249 [D loss: -0.290674] [G loss: -2.632999]\n",
      "250 [D loss: -0.483667] [G loss: -2.575986]\n",
      "251 [D loss: -0.491698] [G loss: -2.295223]\n",
      "252 [D loss: -0.458506] [G loss: -2.454367]\n",
      "253 [D loss: -0.852619] [G loss: -2.203713]\n",
      "254 [D loss: -0.596235] [G loss: -2.496985]\n",
      "255 [D loss: -0.389389] [G loss: -2.522251]\n",
      "256 [D loss: -0.266166] [G loss: -2.618252]\n",
      "257 [D loss: -0.439125] [G loss: -2.411325]\n",
      "258 [D loss: -0.067914] [G loss: -2.397615]\n",
      "259 [D loss: -0.051954] [G loss: -2.621198]\n",
      "260 [D loss: -0.302614] [G loss: -2.551078]\n",
      "261 [D loss: -0.572335] [G loss: -2.428612]\n",
      "262 [D loss: -0.281748] [G loss: -2.463456]\n",
      "263 [D loss: -0.259922] [G loss: -2.532809]\n",
      "264 [D loss: 0.029494] [G loss: -2.515253]\n",
      "265 [D loss: -0.267557] [G loss: -2.434808]\n",
      "266 [D loss: -0.214953] [G loss: -2.613297]\n",
      "267 [D loss: -0.026991] [G loss: -2.984880]\n",
      "268 [D loss: -0.320121] [G loss: -2.452468]\n",
      "269 [D loss: -0.261370] [G loss: -2.481351]\n",
      "270 [D loss: -0.111971] [G loss: -2.514941]\n",
      "271 [D loss: -0.510981] [G loss: -2.779427]\n",
      "272 [D loss: -0.637898] [G loss: -2.867948]\n",
      "273 [D loss: -0.371503] [G loss: -2.341904]\n",
      "274 [D loss: -0.351050] [G loss: -2.776648]\n",
      "275 [D loss: -0.437356] [G loss: -2.658059]\n",
      "276 [D loss: -0.166677] [G loss: -3.068024]\n",
      "277 [D loss: -0.220823] [G loss: -2.854202]\n",
      "278 [D loss: -0.066758] [G loss: -2.666360]\n",
      "279 [D loss: -0.293018] [G loss: -2.952440]\n",
      "280 [D loss: -0.517417] [G loss: -2.690261]\n",
      "281 [D loss: -0.706879] [G loss: -2.425940]\n",
      "282 [D loss: -0.173058] [G loss: -2.391267]\n",
      "283 [D loss: -0.514803] [G loss: -2.768528]\n",
      "284 [D loss: -0.594342] [G loss: -2.810609]\n",
      "285 [D loss: -0.465619] [G loss: -2.494701]\n",
      "286 [D loss: -0.260243] [G loss: -2.537942]\n",
      "287 [D loss: -0.204181] [G loss: -2.536787]\n",
      "288 [D loss: 0.044186] [G loss: -2.484985]\n",
      "289 [D loss: -0.137538] [G loss: -2.453605]\n",
      "290 [D loss: -0.164763] [G loss: -2.588247]\n",
      "291 [D loss: -0.186079] [G loss: -2.365888]\n",
      "292 [D loss: -0.476786] [G loss: -2.488451]\n",
      "293 [D loss: -0.636217] [G loss: -2.558306]\n",
      "294 [D loss: -0.431369] [G loss: -2.562963]\n",
      "295 [D loss: -0.079625] [G loss: -2.479422]\n",
      "296 [D loss: -0.422525] [G loss: -2.309701]\n",
      "297 [D loss: -0.359982] [G loss: -2.441264]\n",
      "298 [D loss: -0.140592] [G loss: -2.407581]\n",
      "299 [D loss: -0.637132] [G loss: -2.496150]\n",
      "300 [D loss: -0.208556] [G loss: -2.355052]\n",
      "301 [D loss: 0.207780] [G loss: -2.465309]\n",
      "302 [D loss: -0.449433] [G loss: -2.347197]\n",
      "303 [D loss: -0.529387] [G loss: -2.680422]\n",
      "304 [D loss: -0.174876] [G loss: -2.800395]\n",
      "305 [D loss: -0.549443] [G loss: -2.695447]\n",
      "306 [D loss: -0.151882] [G loss: -2.634629]\n",
      "307 [D loss: -0.129674] [G loss: -2.411424]\n",
      "308 [D loss: -0.120551] [G loss: -2.436934]\n",
      "309 [D loss: -0.290061] [G loss: -2.237526]\n",
      "310 [D loss: -0.279445] [G loss: -2.640412]\n",
      "311 [D loss: -0.636075] [G loss: -2.545136]\n",
      "312 [D loss: -0.560919] [G loss: -2.299874]\n",
      "313 [D loss: -0.210407] [G loss: -2.211755]\n",
      "314 [D loss: -0.345700] [G loss: -2.619282]\n",
      "315 [D loss: -0.445524] [G loss: -2.251350]\n",
      "316 [D loss: -0.233877] [G loss: -2.252349]\n",
      "317 [D loss: -0.740414] [G loss: -2.306063]\n",
      "318 [D loss: 0.046839] [G loss: -1.950159]\n",
      "319 [D loss: -0.202077] [G loss: -2.431832]\n",
      "320 [D loss: 0.055448] [G loss: -2.429212]\n",
      "321 [D loss: -0.143956] [G loss: -2.356472]\n",
      "322 [D loss: 0.082091] [G loss: -2.221801]\n",
      "323 [D loss: -0.359641] [G loss: -2.532777]\n",
      "324 [D loss: -0.177808] [G loss: -2.266760]\n",
      "325 [D loss: -0.673618] [G loss: -2.394892]\n",
      "326 [D loss: 0.009377] [G loss: -2.186224]\n",
      "327 [D loss: -0.752402] [G loss: -2.501909]\n",
      "328 [D loss: -0.324843] [G loss: -2.436224]\n",
      "329 [D loss: -0.489367] [G loss: -2.410019]\n",
      "330 [D loss: 0.175787] [G loss: -2.008576]\n",
      "331 [D loss: -0.199077] [G loss: -2.471766]\n",
      "332 [D loss: -0.472989] [G loss: -2.192517]\n",
      "333 [D loss: -0.404040] [G loss: -2.508022]\n",
      "334 [D loss: 0.009155] [G loss: -2.426872]\n",
      "335 [D loss: -0.597510] [G loss: -2.287604]\n",
      "336 [D loss: -0.523226] [G loss: -2.479235]\n",
      "337 [D loss: -0.434259] [G loss: -2.192521]\n",
      "338 [D loss: -0.609482] [G loss: -2.082816]\n",
      "339 [D loss: -0.062543] [G loss: -2.452321]\n",
      "340 [D loss: -0.668449] [G loss: -2.235391]\n",
      "341 [D loss: -0.288798] [G loss: -2.647621]\n",
      "342 [D loss: -0.887442] [G loss: -2.150807]\n",
      "343 [D loss: -0.730696] [G loss: -2.244064]\n",
      "344 [D loss: -0.573219] [G loss: -2.375361]\n",
      "345 [D loss: -0.439862] [G loss: -2.209304]\n",
      "346 [D loss: -0.604786] [G loss: -2.376744]\n",
      "347 [D loss: -0.787948] [G loss: -2.451829]\n",
      "348 [D loss: -0.190740] [G loss: -2.773957]\n",
      "349 [D loss: -0.189306] [G loss: -2.517705]\n",
      "350 [D loss: -0.667923] [G loss: -2.456203]\n",
      "351 [D loss: -0.491056] [G loss: -2.793021]\n",
      "352 [D loss: -0.319406] [G loss: -2.844542]\n",
      "353 [D loss: -0.200405] [G loss: -2.083082]\n",
      "354 [D loss: -0.169106] [G loss: -2.566899]\n",
      "355 [D loss: -0.606699] [G loss: -2.455972]\n",
      "356 [D loss: -0.185588] [G loss: -2.816846]\n",
      "357 [D loss: -0.499451] [G loss: -2.668290]\n",
      "358 [D loss: -0.313206] [G loss: -2.923193]\n",
      "359 [D loss: -0.464300] [G loss: -2.493393]\n",
      "360 [D loss: -0.733250] [G loss: -2.432061]\n",
      "361 [D loss: -0.129094] [G loss: -2.440331]\n",
      "362 [D loss: -0.582158] [G loss: -2.447736]\n",
      "363 [D loss: 0.076015] [G loss: -2.521101]\n",
      "364 [D loss: -0.174851] [G loss: -1.966697]\n",
      "365 [D loss: -0.495178] [G loss: -2.412304]\n",
      "366 [D loss: -0.484644] [G loss: -2.247888]\n",
      "367 [D loss: -0.461326] [G loss: -2.333626]\n",
      "368 [D loss: -0.808783] [G loss: -2.685942]\n",
      "369 [D loss: -0.476777] [G loss: -2.570402]\n",
      "370 [D loss: 0.191143] [G loss: -2.257330]\n",
      "371 [D loss: -0.107227] [G loss: -2.653156]\n",
      "372 [D loss: -0.013545] [G loss: -2.597404]\n",
      "373 [D loss: -0.267520] [G loss: -2.628840]\n",
      "374 [D loss: 0.022912] [G loss: -2.626578]\n",
      "375 [D loss: -0.213177] [G loss: -2.954788]\n",
      "376 [D loss: -0.401645] [G loss: -2.709180]\n",
      "377 [D loss: -0.113205] [G loss: -2.703230]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378 [D loss: -0.116739] [G loss: -2.648970]\n",
      "379 [D loss: 0.010750] [G loss: -2.589654]\n",
      "380 [D loss: -0.441586] [G loss: -2.460656]\n",
      "381 [D loss: -0.206813] [G loss: -2.441152]\n",
      "382 [D loss: 0.281966] [G loss: -2.474371]\n",
      "383 [D loss: -0.133488] [G loss: -2.311149]\n",
      "384 [D loss: -0.292996] [G loss: -2.403698]\n",
      "385 [D loss: -0.067222] [G loss: -2.365760]\n",
      "386 [D loss: -0.745061] [G loss: -2.979283]\n",
      "387 [D loss: -0.386559] [G loss: -2.123550]\n",
      "388 [D loss: -0.137915] [G loss: -2.242775]\n",
      "389 [D loss: 0.084619] [G loss: -2.664877]\n",
      "390 [D loss: -0.497101] [G loss: -2.509724]\n",
      "391 [D loss: -0.264946] [G loss: -2.902325]\n",
      "392 [D loss: -0.116550] [G loss: -2.418616]\n",
      "393 [D loss: -0.295383] [G loss: -2.787032]\n",
      "394 [D loss: -0.089682] [G loss: -2.645774]\n",
      "395 [D loss: -0.830465] [G loss: -2.395820]\n",
      "396 [D loss: -0.566966] [G loss: -2.665959]\n",
      "397 [D loss: -0.239933] [G loss: -2.603440]\n",
      "398 [D loss: -0.456670] [G loss: -2.405239]\n",
      "399 [D loss: -0.190577] [G loss: -2.686404]\n",
      "400 [D loss: -0.003322] [G loss: -2.455379]\n",
      "401 [D loss: -0.075840] [G loss: -2.762372]\n",
      "402 [D loss: -0.206128] [G loss: -2.386058]\n",
      "403 [D loss: -0.853710] [G loss: -2.378311]\n",
      "404 [D loss: -0.712518] [G loss: -2.421793]\n",
      "405 [D loss: -0.374451] [G loss: -2.639544]\n",
      "406 [D loss: -0.684705] [G loss: -2.153640]\n",
      "407 [D loss: -0.412314] [G loss: -2.563233]\n",
      "408 [D loss: -0.268434] [G loss: -2.653087]\n",
      "409 [D loss: -0.573119] [G loss: -2.408574]\n",
      "410 [D loss: -0.572019] [G loss: -2.175737]\n",
      "411 [D loss: -0.157346] [G loss: -2.353704]\n",
      "412 [D loss: -0.307131] [G loss: -2.345829]\n",
      "413 [D loss: -0.473763] [G loss: -2.485163]\n",
      "414 [D loss: -0.779488] [G loss: -2.631677]\n",
      "415 [D loss: 0.268739] [G loss: -2.549807]\n",
      "416 [D loss: -0.254608] [G loss: -2.681949]\n",
      "417 [D loss: -0.558169] [G loss: -2.124967]\n",
      "418 [D loss: -0.087704] [G loss: -2.337255]\n",
      "419 [D loss: -0.508292] [G loss: -2.296106]\n",
      "420 [D loss: -0.621380] [G loss: -2.381128]\n",
      "421 [D loss: -0.251295] [G loss: -2.431620]\n",
      "422 [D loss: -0.355559] [G loss: -2.291874]\n",
      "423 [D loss: -0.179096] [G loss: -2.468351]\n",
      "424 [D loss: -0.554157] [G loss: -2.711102]\n",
      "425 [D loss: -0.535445] [G loss: -2.723848]\n",
      "426 [D loss: 0.046725] [G loss: -2.469429]\n",
      "427 [D loss: -0.365869] [G loss: -2.162361]\n",
      "428 [D loss: 0.178482] [G loss: -2.321298]\n",
      "429 [D loss: -0.057934] [G loss: -2.634436]\n",
      "430 [D loss: 0.102681] [G loss: -2.419369]\n",
      "431 [D loss: -0.748099] [G loss: -2.380884]\n",
      "432 [D loss: -0.165086] [G loss: -2.398851]\n",
      "433 [D loss: -0.279540] [G loss: -2.424742]\n",
      "434 [D loss: -0.392615] [G loss: -2.224432]\n",
      "435 [D loss: -0.159476] [G loss: -2.366092]\n",
      "436 [D loss: -0.806162] [G loss: -2.319288]\n",
      "437 [D loss: -0.369369] [G loss: -2.296081]\n",
      "438 [D loss: -0.093631] [G loss: -2.340348]\n",
      "439 [D loss: 0.042619] [G loss: -2.346961]\n",
      "440 [D loss: 0.340262] [G loss: -2.347726]\n",
      "441 [D loss: -0.596939] [G loss: -2.327582]\n",
      "442 [D loss: -0.128934] [G loss: -2.221129]\n",
      "443 [D loss: -0.121632] [G loss: -2.410930]\n",
      "444 [D loss: -0.351338] [G loss: -2.218235]\n",
      "445 [D loss: -0.201676] [G loss: -2.384966]\n",
      "446 [D loss: -0.371342] [G loss: -2.112830]\n",
      "447 [D loss: -0.345676] [G loss: -2.336365]\n",
      "448 [D loss: 0.090232] [G loss: -2.325203]\n",
      "449 [D loss: -0.186690] [G loss: -2.413408]\n",
      "450 [D loss: -0.188274] [G loss: -2.290910]\n",
      "451 [D loss: -0.517535] [G loss: -2.268010]\n",
      "452 [D loss: -0.018489] [G loss: -2.537853]\n",
      "453 [D loss: -0.301761] [G loss: -2.544727]\n",
      "454 [D loss: -0.487592] [G loss: -2.317253]\n",
      "455 [D loss: 0.050615] [G loss: -2.219612]\n",
      "456 [D loss: -0.669119] [G loss: -2.145608]\n",
      "457 [D loss: 0.054492] [G loss: -2.352133]\n",
      "458 [D loss: -0.753118] [G loss: -2.411031]\n",
      "459 [D loss: -0.555108] [G loss: -2.203277]\n",
      "460 [D loss: 0.003084] [G loss: -2.641943]\n",
      "461 [D loss: -0.415881] [G loss: -1.987258]\n",
      "462 [D loss: -0.779263] [G loss: -2.272432]\n",
      "463 [D loss: -0.723992] [G loss: -2.308521]\n",
      "464 [D loss: -0.338395] [G loss: -1.954860]\n",
      "465 [D loss: -0.434838] [G loss: -2.193465]\n",
      "466 [D loss: -0.383266] [G loss: -2.209495]\n",
      "467 [D loss: -0.643489] [G loss: -2.129931]\n",
      "468 [D loss: -0.320380] [G loss: -2.705925]\n",
      "469 [D loss: -0.582980] [G loss: -2.232827]\n",
      "470 [D loss: -0.091193] [G loss: -2.014848]\n",
      "471 [D loss: -0.090299] [G loss: -2.233388]\n",
      "472 [D loss: 0.027497] [G loss: -2.118137]\n",
      "473 [D loss: -0.334949] [G loss: -2.162088]\n",
      "474 [D loss: -0.384646] [G loss: -2.483858]\n",
      "475 [D loss: -0.232688] [G loss: -2.379861]\n",
      "476 [D loss: -0.569875] [G loss: -2.168278]\n",
      "477 [D loss: -0.143577] [G loss: -2.541864]\n",
      "478 [D loss: -0.512247] [G loss: -2.397491]\n",
      "479 [D loss: -0.135585] [G loss: -2.546228]\n",
      "480 [D loss: -0.352344] [G loss: -2.201429]\n",
      "481 [D loss: -0.534598] [G loss: -2.457533]\n",
      "482 [D loss: -0.308347] [G loss: -2.180817]\n",
      "483 [D loss: -0.073443] [G loss: -2.229320]\n",
      "484 [D loss: -0.513484] [G loss: -2.465288]\n",
      "485 [D loss: -0.371033] [G loss: -2.366711]\n",
      "486 [D loss: -0.146963] [G loss: -1.883494]\n",
      "487 [D loss: 0.077963] [G loss: -2.707411]\n",
      "488 [D loss: -0.491954] [G loss: -1.885487]\n",
      "489 [D loss: -0.411208] [G loss: -2.146812]\n",
      "490 [D loss: -0.421607] [G loss: -2.357064]\n",
      "491 [D loss: -0.876108] [G loss: -2.360370]\n",
      "492 [D loss: -0.303762] [G loss: -2.347609]\n",
      "493 [D loss: -0.328149] [G loss: -2.602428]\n",
      "494 [D loss: -0.139785] [G loss: -2.406623]\n",
      "495 [D loss: -0.323465] [G loss: -2.489806]\n",
      "496 [D loss: -0.449870] [G loss: -2.361477]\n",
      "497 [D loss: -0.549906] [G loss: -2.418204]\n",
      "498 [D loss: 0.182754] [G loss: -2.773451]\n",
      "499 [D loss: 0.030177] [G loss: -2.559495]\n",
      "500 [D loss: 0.043289] [G loss: -2.447811]\n",
      "501 [D loss: -0.242749] [G loss: -2.721850]\n",
      "502 [D loss: -0.168230] [G loss: -2.537547]\n",
      "503 [D loss: 0.256513] [G loss: -2.309675]\n",
      "504 [D loss: -0.185485] [G loss: -2.606997]\n",
      "505 [D loss: -0.484103] [G loss: -2.834137]\n",
      "506 [D loss: -0.592959] [G loss: -2.091607]\n",
      "507 [D loss: 0.174656] [G loss: -2.708349]\n",
      "508 [D loss: -0.576789] [G loss: -2.198084]\n",
      "509 [D loss: -0.151978] [G loss: -2.515469]\n",
      "510 [D loss: -0.147830] [G loss: -2.572383]\n",
      "511 [D loss: -0.101286] [G loss: -2.363192]\n",
      "512 [D loss: 0.140668] [G loss: -2.445312]\n",
      "513 [D loss: -0.005996] [G loss: -2.594408]\n",
      "514 [D loss: -0.324733] [G loss: -2.355565]\n",
      "515 [D loss: -0.559352] [G loss: -2.311719]\n",
      "516 [D loss: -0.342778] [G loss: -2.248770]\n",
      "517 [D loss: -0.584387] [G loss: -2.399864]\n",
      "518 [D loss: -0.418709] [G loss: -2.305251]\n",
      "519 [D loss: -0.498982] [G loss: -2.410140]\n",
      "520 [D loss: -0.649798] [G loss: -2.657772]\n",
      "521 [D loss: -0.394431] [G loss: -2.389595]\n",
      "522 [D loss: 0.093455] [G loss: -2.274101]\n",
      "523 [D loss: -0.151875] [G loss: -2.321538]\n",
      "524 [D loss: -0.148529] [G loss: -2.236037]\n",
      "525 [D loss: 0.226708] [G loss: -2.546994]\n",
      "526 [D loss: -0.215802] [G loss: -2.329872]\n",
      "527 [D loss: -0.266470] [G loss: -2.752576]\n",
      "528 [D loss: -0.244900] [G loss: -2.314938]\n",
      "529 [D loss: -0.347359] [G loss: -2.093730]\n",
      "530 [D loss: -0.180106] [G loss: -2.376781]\n",
      "531 [D loss: -0.343178] [G loss: -2.552672]\n",
      "532 [D loss: -0.300916] [G loss: -2.302565]\n",
      "533 [D loss: -0.347982] [G loss: -2.425853]\n",
      "534 [D loss: -0.420667] [G loss: -2.283962]\n",
      "535 [D loss: -0.363147] [G loss: -2.390574]\n",
      "536 [D loss: -0.476968] [G loss: -2.375574]\n",
      "537 [D loss: -0.600150] [G loss: -2.203534]\n",
      "538 [D loss: -0.592321] [G loss: -2.413843]\n",
      "539 [D loss: -0.136438] [G loss: -2.281010]\n",
      "540 [D loss: -0.231703] [G loss: -1.813926]\n",
      "541 [D loss: -0.148042] [G loss: -2.328218]\n",
      "542 [D loss: -0.231462] [G loss: -2.142418]\n",
      "543 [D loss: -0.768018] [G loss: -2.161242]\n",
      "544 [D loss: -0.173870] [G loss: -2.215563]\n",
      "545 [D loss: 0.143123] [G loss: -2.123618]\n",
      "546 [D loss: -0.234163] [G loss: -2.297900]\n",
      "547 [D loss: -0.680588] [G loss: -2.004767]\n",
      "548 [D loss: -0.371547] [G loss: -2.096025]\n",
      "549 [D loss: -0.440270] [G loss: -1.867742]\n",
      "550 [D loss: -0.613496] [G loss: -2.199012]\n",
      "551 [D loss: -0.437606] [G loss: -1.993232]\n",
      "552 [D loss: -0.177173] [G loss: -1.848742]\n",
      "553 [D loss: -0.786662] [G loss: -2.490176]\n",
      "554 [D loss: -0.296117] [G loss: -2.152165]\n",
      "555 [D loss: -0.660129] [G loss: -1.887829]\n",
      "556 [D loss: -0.232382] [G loss: -2.111055]\n",
      "557 [D loss: -0.404912] [G loss: -1.755117]\n",
      "558 [D loss: -0.026882] [G loss: -1.789160]\n",
      "559 [D loss: -0.324744] [G loss: -1.867825]\n",
      "560 [D loss: -0.189564] [G loss: -2.327291]\n",
      "561 [D loss: -0.305685] [G loss: -1.870328]\n",
      "562 [D loss: -0.332323] [G loss: -1.935148]\n",
      "563 [D loss: -0.463751] [G loss: -1.873696]\n",
      "564 [D loss: -0.130171] [G loss: -1.814326]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565 [D loss: -0.297545] [G loss: -2.039172]\n",
      "566 [D loss: -0.113412] [G loss: -2.333783]\n",
      "567 [D loss: -0.248245] [G loss: -1.928136]\n",
      "568 [D loss: -0.206390] [G loss: -1.970325]\n",
      "569 [D loss: -0.012758] [G loss: -1.821019]\n",
      "570 [D loss: -0.846002] [G loss: -2.105019]\n",
      "571 [D loss: -0.063511] [G loss: -1.975502]\n",
      "572 [D loss: -0.610505] [G loss: -1.660715]\n",
      "573 [D loss: -0.236640] [G loss: -1.977567]\n",
      "574 [D loss: -0.347666] [G loss: -1.988389]\n",
      "575 [D loss: 0.019874] [G loss: -1.950062]\n",
      "576 [D loss: -0.383859] [G loss: -1.820282]\n",
      "577 [D loss: -0.378650] [G loss: -1.856554]\n",
      "578 [D loss: 0.213223] [G loss: -1.944825]\n",
      "579 [D loss: -0.539018] [G loss: -2.008958]\n",
      "580 [D loss: -0.522736] [G loss: -2.102289]\n",
      "581 [D loss: -0.426613] [G loss: -2.002568]\n",
      "582 [D loss: -0.589562] [G loss: -2.386572]\n",
      "583 [D loss: 0.024214] [G loss: -2.328669]\n",
      "584 [D loss: -0.349392] [G loss: -2.131880]\n",
      "585 [D loss: 0.075837] [G loss: -2.163935]\n",
      "586 [D loss: -0.381199] [G loss: -1.793773]\n",
      "587 [D loss: -0.307189] [G loss: -1.780457]\n",
      "588 [D loss: -0.170137] [G loss: -1.972926]\n",
      "589 [D loss: 0.034932] [G loss: -2.107042]\n",
      "590 [D loss: -0.450399] [G loss: -1.781068]\n",
      "591 [D loss: -0.561779] [G loss: -1.953483]\n",
      "592 [D loss: -0.361570] [G loss: -2.144330]\n",
      "593 [D loss: -0.424587] [G loss: -2.034845]\n",
      "594 [D loss: -0.344022] [G loss: -1.874337]\n",
      "595 [D loss: -0.266775] [G loss: -1.983540]\n",
      "596 [D loss: -0.609142] [G loss: -2.129534]\n",
      "597 [D loss: -0.074352] [G loss: -2.198777]\n",
      "598 [D loss: -0.023144] [G loss: -1.748929]\n",
      "599 [D loss: -0.326569] [G loss: -2.268968]\n",
      "600 [D loss: 0.191470] [G loss: -1.938358]\n",
      "601 [D loss: -0.583656] [G loss: -1.944908]\n",
      "602 [D loss: -0.272180] [G loss: -2.018301]\n",
      "603 [D loss: -0.382443] [G loss: -1.915183]\n",
      "604 [D loss: -0.186430] [G loss: -1.965167]\n",
      "605 [D loss: 0.079487] [G loss: -1.826574]\n",
      "606 [D loss: -0.219464] [G loss: -1.951196]\n",
      "607 [D loss: -0.255939] [G loss: -1.502327]\n",
      "608 [D loss: -0.123950] [G loss: -1.960918]\n",
      "609 [D loss: -0.597806] [G loss: -2.022983]\n",
      "610 [D loss: 0.221239] [G loss: -1.899321]\n",
      "611 [D loss: -0.229716] [G loss: -1.584244]\n",
      "612 [D loss: -0.064183] [G loss: -1.883204]\n",
      "613 [D loss: -0.424013] [G loss: -1.672691]\n",
      "614 [D loss: -0.312706] [G loss: -1.613472]\n",
      "615 [D loss: -0.115201] [G loss: -1.907668]\n",
      "616 [D loss: -0.306433] [G loss: -1.854421]\n",
      "617 [D loss: -0.050328] [G loss: -1.722121]\n",
      "618 [D loss: -0.672457] [G loss: -1.779915]\n",
      "619 [D loss: -0.004072] [G loss: -1.931314]\n",
      "620 [D loss: -0.423566] [G loss: -1.296290]\n",
      "621 [D loss: -0.490865] [G loss: -1.752212]\n",
      "622 [D loss: -0.055193] [G loss: -1.704353]\n",
      "623 [D loss: -0.282788] [G loss: -1.960953]\n",
      "624 [D loss: 0.081683] [G loss: -2.210885]\n",
      "625 [D loss: -0.941437] [G loss: -1.654058]\n",
      "626 [D loss: -0.458496] [G loss: -1.531473]\n",
      "627 [D loss: -0.401040] [G loss: -1.392893]\n",
      "628 [D loss: -0.194421] [G loss: -2.005177]\n",
      "629 [D loss: -0.296591] [G loss: -1.663718]\n",
      "630 [D loss: -0.182135] [G loss: -1.378773]\n",
      "631 [D loss: -0.492057] [G loss: -1.679363]\n",
      "632 [D loss: -0.396252] [G loss: -2.061579]\n",
      "633 [D loss: -0.351735] [G loss: -1.897248]\n",
      "634 [D loss: -0.439729] [G loss: -1.688144]\n",
      "635 [D loss: 0.080313] [G loss: -1.993926]\n",
      "636 [D loss: -0.047020] [G loss: -1.924597]\n",
      "637 [D loss: -0.447029] [G loss: -2.055810]\n",
      "638 [D loss: -0.250409] [G loss: -1.657474]\n",
      "639 [D loss: -0.138258] [G loss: -1.566582]\n",
      "640 [D loss: -0.213650] [G loss: -1.693870]\n",
      "641 [D loss: -0.434357] [G loss: -1.467347]\n",
      "642 [D loss: -0.324230] [G loss: -1.794248]\n",
      "643 [D loss: -0.261264] [G loss: -1.664737]\n",
      "644 [D loss: -0.180331] [G loss: -2.205965]\n",
      "645 [D loss: -0.445680] [G loss: -1.778390]\n",
      "646 [D loss: -0.400152] [G loss: -1.467818]\n",
      "647 [D loss: -0.478419] [G loss: -1.922174]\n",
      "648 [D loss: -0.390013] [G loss: -1.709661]\n",
      "649 [D loss: 0.022564] [G loss: -1.520592]\n",
      "650 [D loss: -0.349591] [G loss: -1.737566]\n",
      "651 [D loss: -0.578643] [G loss: -1.725468]\n",
      "652 [D loss: -0.661615] [G loss: -1.802966]\n",
      "653 [D loss: 0.167222] [G loss: -1.783785]\n",
      "654 [D loss: -0.513032] [G loss: -1.529362]\n",
      "655 [D loss: -0.532999] [G loss: -1.575681]\n",
      "656 [D loss: -0.205624] [G loss: -1.894091]\n",
      "657 [D loss: -0.109453] [G loss: -1.676613]\n",
      "658 [D loss: -0.402241] [G loss: -1.684778]\n",
      "659 [D loss: -0.645566] [G loss: -1.719680]\n",
      "660 [D loss: 0.238876] [G loss: -2.012413]\n",
      "661 [D loss: -0.326277] [G loss: -1.831853]\n",
      "662 [D loss: -0.614222] [G loss: -1.583232]\n",
      "663 [D loss: -0.082187] [G loss: -1.769018]\n",
      "664 [D loss: -0.271219] [G loss: -1.898077]\n",
      "665 [D loss: -0.349581] [G loss: -1.829729]\n",
      "666 [D loss: -0.373711] [G loss: -2.479503]\n",
      "667 [D loss: -0.741093] [G loss: -1.975060]\n",
      "668 [D loss: -0.311647] [G loss: -1.800027]\n",
      "669 [D loss: -0.524607] [G loss: -1.757720]\n",
      "670 [D loss: -0.528400] [G loss: -1.778035]\n",
      "671 [D loss: 0.054641] [G loss: -1.852197]\n",
      "672 [D loss: -0.442983] [G loss: -1.663061]\n",
      "673 [D loss: -0.293846] [G loss: -1.690039]\n",
      "674 [D loss: -0.070044] [G loss: -1.716547]\n",
      "675 [D loss: -0.124251] [G loss: -1.896079]\n",
      "676 [D loss: -0.289984] [G loss: -1.854422]\n",
      "677 [D loss: -0.331093] [G loss: -1.861586]\n",
      "678 [D loss: -0.760567] [G loss: -1.782438]\n",
      "679 [D loss: -0.713700] [G loss: -1.879426]\n",
      "680 [D loss: 0.184381] [G loss: -1.846846]\n",
      "681 [D loss: -0.514891] [G loss: -2.087910]\n",
      "682 [D loss: -0.681416] [G loss: -2.198169]\n",
      "683 [D loss: -0.471749] [G loss: -2.053531]\n",
      "684 [D loss: -0.988408] [G loss: -1.763158]\n",
      "685 [D loss: -0.511226] [G loss: -2.144315]\n",
      "686 [D loss: -0.536981] [G loss: -1.892893]\n",
      "687 [D loss: -0.313330] [G loss: -1.913191]\n",
      "688 [D loss: -0.307466] [G loss: -2.071864]\n",
      "689 [D loss: 0.222958] [G loss: -2.540676]\n",
      "690 [D loss: -0.389651] [G loss: -1.998690]\n",
      "691 [D loss: -0.259158] [G loss: -2.010922]\n",
      "692 [D loss: -0.121693] [G loss: -2.083990]\n",
      "693 [D loss: -0.217802] [G loss: -1.768178]\n",
      "694 [D loss: -0.060528] [G loss: -2.031970]\n",
      "695 [D loss: 0.112599] [G loss: -1.975419]\n",
      "696 [D loss: -0.326410] [G loss: -2.237654]\n",
      "697 [D loss: -0.827212] [G loss: -1.574480]\n",
      "698 [D loss: 0.394382] [G loss: -2.058988]\n",
      "699 [D loss: -0.363241] [G loss: -2.165106]\n",
      "700 [D loss: -0.716991] [G loss: -2.288502]\n",
      "701 [D loss: -0.440599] [G loss: -2.097457]\n",
      "702 [D loss: -0.832565] [G loss: -2.148857]\n",
      "703 [D loss: -0.716641] [G loss: -2.112979]\n",
      "704 [D loss: -0.450790] [G loss: -2.235780]\n",
      "705 [D loss: -0.302432] [G loss: -1.597218]\n",
      "706 [D loss: -0.235651] [G loss: -2.070434]\n",
      "707 [D loss: -0.730663] [G loss: -1.696485]\n",
      "708 [D loss: -0.693271] [G loss: -2.285269]\n",
      "709 [D loss: -0.339262] [G loss: -2.139023]\n",
      "710 [D loss: -0.342782] [G loss: -2.320825]\n",
      "711 [D loss: -0.580589] [G loss: -2.110123]\n",
      "712 [D loss: -0.495892] [G loss: -2.544015]\n",
      "713 [D loss: -0.540277] [G loss: -1.908365]\n",
      "714 [D loss: -0.036432] [G loss: -2.408241]\n",
      "715 [D loss: -0.000346] [G loss: -1.984291]\n",
      "716 [D loss: -0.432453] [G loss: -1.887731]\n",
      "717 [D loss: -0.469015] [G loss: -1.809911]\n",
      "718 [D loss: -0.470624] [G loss: -1.926717]\n",
      "719 [D loss: -0.508732] [G loss: -1.957166]\n",
      "720 [D loss: -0.761117] [G loss: -1.880200]\n",
      "721 [D loss: -0.402630] [G loss: -1.898980]\n",
      "722 [D loss: -0.573163] [G loss: -2.117380]\n",
      "723 [D loss: -0.609656] [G loss: -2.059923]\n",
      "724 [D loss: -0.424195] [G loss: -2.483554]\n",
      "725 [D loss: 0.152628] [G loss: -1.703742]\n",
      "726 [D loss: -0.226892] [G loss: -2.038592]\n",
      "727 [D loss: -0.222870] [G loss: -2.197087]\n",
      "728 [D loss: -0.567695] [G loss: -1.870167]\n",
      "729 [D loss: 0.042743] [G loss: -2.080987]\n",
      "730 [D loss: -0.029458] [G loss: -2.113120]\n",
      "731 [D loss: -0.251817] [G loss: -1.841311]\n",
      "732 [D loss: 0.087261] [G loss: -2.141005]\n",
      "733 [D loss: -0.473503] [G loss: -2.051779]\n",
      "734 [D loss: -0.665467] [G loss: -2.024746]\n",
      "735 [D loss: -0.153295] [G loss: -2.055276]\n",
      "736 [D loss: -0.661340] [G loss: -1.602887]\n",
      "737 [D loss: -0.088544] [G loss: -1.701128]\n",
      "738 [D loss: -0.242968] [G loss: -2.260131]\n",
      "739 [D loss: -0.696295] [G loss: -1.864829]\n",
      "740 [D loss: -0.805316] [G loss: -1.666304]\n",
      "741 [D loss: -0.287697] [G loss: -2.184783]\n",
      "742 [D loss: 0.089986] [G loss: -1.652228]\n",
      "743 [D loss: -0.253897] [G loss: -1.686106]\n",
      "744 [D loss: -0.873147] [G loss: -1.954756]\n",
      "745 [D loss: -0.612839] [G loss: -1.941142]\n",
      "746 [D loss: -0.810987] [G loss: -1.617342]\n",
      "747 [D loss: 0.076809] [G loss: -1.648166]\n",
      "748 [D loss: -0.212278] [G loss: -1.722162]\n",
      "749 [D loss: -0.507094] [G loss: -2.007535]\n",
      "750 [D loss: -0.091830] [G loss: -1.473837]\n",
      "751 [D loss: -0.201481] [G loss: -2.101535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "752 [D loss: -1.178323] [G loss: -1.816783]\n",
      "753 [D loss: -0.934460] [G loss: -1.955165]\n",
      "754 [D loss: -0.054417] [G loss: -1.774947]\n",
      "755 [D loss: -1.108276] [G loss: -1.511630]\n",
      "756 [D loss: -0.133919] [G loss: -1.576594]\n",
      "757 [D loss: -0.886952] [G loss: -2.077778]\n",
      "758 [D loss: -0.239481] [G loss: -1.233009]\n",
      "759 [D loss: -0.465669] [G loss: -1.725977]\n",
      "760 [D loss: -0.373216] [G loss: -1.939457]\n",
      "761 [D loss: -0.955613] [G loss: -1.587133]\n",
      "762 [D loss: -0.172634] [G loss: -1.825816]\n",
      "763 [D loss: -0.171664] [G loss: -2.080106]\n",
      "764 [D loss: -0.312583] [G loss: -1.789814]\n",
      "765 [D loss: -0.087180] [G loss: -1.606876]\n",
      "766 [D loss: -0.622036] [G loss: -1.540080]\n",
      "767 [D loss: -0.182765] [G loss: -2.016765]\n",
      "768 [D loss: 0.049768] [G loss: -1.949809]\n",
      "769 [D loss: -0.102701] [G loss: -1.811347]\n",
      "770 [D loss: -0.862979] [G loss: -1.765056]\n",
      "771 [D loss: -0.153048] [G loss: -2.138021]\n",
      "772 [D loss: -0.980550] [G loss: -1.665675]\n",
      "773 [D loss: -0.211273] [G loss: -1.793531]\n",
      "774 [D loss: -0.725310] [G loss: -1.871071]\n",
      "775 [D loss: -0.605046] [G loss: -1.838801]\n",
      "776 [D loss: -0.487640] [G loss: -1.855689]\n",
      "777 [D loss: -0.092491] [G loss: -1.707359]\n",
      "778 [D loss: -0.077505] [G loss: -1.985008]\n",
      "779 [D loss: -0.397928] [G loss: -2.009441]\n",
      "780 [D loss: -0.468708] [G loss: -1.859712]\n",
      "781 [D loss: -0.410727] [G loss: -1.810103]\n",
      "782 [D loss: -0.361116] [G loss: -1.863340]\n",
      "783 [D loss: -0.329520] [G loss: -1.657387]\n",
      "784 [D loss: -0.669398] [G loss: -2.017718]\n",
      "785 [D loss: -0.439430] [G loss: -1.976480]\n",
      "786 [D loss: -0.156481] [G loss: -1.735795]\n",
      "787 [D loss: -0.483266] [G loss: -1.878818]\n",
      "788 [D loss: -0.647898] [G loss: -2.156613]\n",
      "789 [D loss: -0.432934] [G loss: -1.942900]\n",
      "790 [D loss: -0.631956] [G loss: -1.821055]\n",
      "791 [D loss: -0.129581] [G loss: -2.114766]\n",
      "792 [D loss: -0.540005] [G loss: -2.332489]\n",
      "793 [D loss: 0.029534] [G loss: -2.055682]\n",
      "794 [D loss: -0.332995] [G loss: -2.154494]\n",
      "795 [D loss: -0.264760] [G loss: -1.561331]\n",
      "796 [D loss: -0.395422] [G loss: -1.998855]\n",
      "797 [D loss: -0.141503] [G loss: -1.739901]\n",
      "798 [D loss: -0.363246] [G loss: -2.010946]\n",
      "799 [D loss: -0.147311] [G loss: -1.687124]\n",
      "800 [D loss: -0.224077] [G loss: -2.004551]\n",
      "801 [D loss: 0.001785] [G loss: -1.905746]\n",
      "802 [D loss: 0.172536] [G loss: -2.153552]\n",
      "803 [D loss: -0.382092] [G loss: -2.052182]\n",
      "804 [D loss: -0.238352] [G loss: -1.933372]\n",
      "805 [D loss: -0.818923] [G loss: -1.477032]\n",
      "806 [D loss: -0.431501] [G loss: -2.276451]\n",
      "807 [D loss: -0.765160] [G loss: -1.632561]\n",
      "808 [D loss: 0.185789] [G loss: -2.399297]\n",
      "809 [D loss: -0.313832] [G loss: -1.745643]\n",
      "810 [D loss: -0.114348] [G loss: -2.050771]\n",
      "811 [D loss: -0.585426] [G loss: -1.778215]\n",
      "812 [D loss: -0.362072] [G loss: -1.597920]\n",
      "813 [D loss: -0.556881] [G loss: -2.384878]\n",
      "814 [D loss: -0.609365] [G loss: -2.228810]\n",
      "815 [D loss: 0.210415] [G loss: -1.508409]\n",
      "816 [D loss: -0.011992] [G loss: -2.029037]\n",
      "817 [D loss: -0.476999] [G loss: -1.879073]\n",
      "818 [D loss: -0.541234] [G loss: -1.882999]\n",
      "819 [D loss: 0.006487] [G loss: -1.936823]\n",
      "820 [D loss: -0.159131] [G loss: -1.814545]\n",
      "821 [D loss: -0.489612] [G loss: -2.009562]\n",
      "822 [D loss: -0.890942] [G loss: -2.085776]\n",
      "823 [D loss: -0.358289] [G loss: -2.319200]\n",
      "824 [D loss: -0.286918] [G loss: -2.039331]\n",
      "825 [D loss: -0.487314] [G loss: -1.708594]\n",
      "826 [D loss: -0.467807] [G loss: -1.767682]\n",
      "827 [D loss: -0.734506] [G loss: -1.673507]\n",
      "828 [D loss: -0.467784] [G loss: -1.935886]\n",
      "829 [D loss: -0.529678] [G loss: -2.016685]\n",
      "830 [D loss: -0.845121] [G loss: -1.900612]\n",
      "831 [D loss: -0.060115] [G loss: -1.848313]\n",
      "832 [D loss: -0.207451] [G loss: -1.751549]\n",
      "833 [D loss: -0.334509] [G loss: -2.362559]\n",
      "834 [D loss: -0.067580] [G loss: -1.898876]\n",
      "835 [D loss: -0.517199] [G loss: -1.448738]\n",
      "836 [D loss: -0.666563] [G loss: -1.741751]\n",
      "837 [D loss: -0.199584] [G loss: -1.751438]\n",
      "838 [D loss: -0.476046] [G loss: -1.989716]\n",
      "839 [D loss: -0.670439] [G loss: -1.654525]\n",
      "840 [D loss: -0.343873] [G loss: -1.811658]\n",
      "841 [D loss: -0.220285] [G loss: -1.875852]\n",
      "842 [D loss: -0.635759] [G loss: -1.557120]\n",
      "843 [D loss: -0.083085] [G loss: -1.452204]\n",
      "844 [D loss: -0.704277] [G loss: -1.321595]\n",
      "845 [D loss: -0.507626] [G loss: -1.697037]\n",
      "846 [D loss: -0.627039] [G loss: -1.859341]\n",
      "847 [D loss: -0.990031] [G loss: -1.707777]\n"
     ]
    }
   ],
   "source": [
    "# epochs=30000\n",
    "epochs=3000\n",
    "train(G, critic_model, generator_model, \n",
    "      n_critic, latent_dim,\n",
    "      epochs=epochs, batch_size=32, sample_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
