{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DualGAN\n",
    "\n",
    "ref. YI, Zili et al.  \n",
    "     Dualgan: Unsupervised dual learning for image-to-image translation.  \n",
    "     In: Proceedings of the IEEE international conference on computer vision. 2017. p. 2849-2857.\n",
    "     \n",
    "![Dual GAN architecture](arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(img_dim):\n",
    "\n",
    "    X = Input(shape=(img_dim,))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=img_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(img_dim, activation='tanh'))\n",
    "\n",
    "    X_translated = model(X)\n",
    "\n",
    "    return Model(X, X_translated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(img_dim):\n",
    "\n",
    "    img = Input(shape=(img_dim,))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=img_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    validity = model(img)\n",
    "\n",
    "    return Model(img, validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_generator_input(X, batch_size):\n",
    "    # Sample random batch of images from X\n",
    "    idx = np.random.randint(0, X.shape[0], batch_size)\n",
    "    return X[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_imgs(G_AB, G_BA, epoch, \n",
    "              X_A, X_B, \n",
    "              img_rows, img_cols):\n",
    "    r, c = 4, 4\n",
    "\n",
    "    # Sample generator inputs\n",
    "    imgs_A = sample_generator_input(X_A, c)\n",
    "    imgs_B = sample_generator_input(X_B, c)\n",
    "\n",
    "    # Images translated to their opposite domain\n",
    "    fake_B = G_AB.predict(imgs_A)\n",
    "    fake_A = G_BA.predict(imgs_B)\n",
    "\n",
    "    gen_imgs = np.concatenate([imgs_A, fake_B, imgs_B, fake_A])\n",
    "    gen_imgs = gen_imgs.reshape((r, c, img_rows, img_cols, 1))\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[i, j, :,:,0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(D_A, D_B, G_AB, G_BA, combined,\n",
    "          img_dim, img_rows, img_cols,\n",
    "          epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "    # Load the dataset\n",
    "    (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "    # Rescale -1 to 1\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "    # Domain A and B (rotated)\n",
    "    X_A = X_train[:int(X_train.shape[0]/2)]\n",
    "    X_B = scipy.ndimage.interpolation.rotate(X_train[int(X_train.shape[0]/2):], 90, axes=(1, 2))\n",
    "\n",
    "    X_A = X_A.reshape(X_A.shape[0], img_dim)\n",
    "    X_B = X_B.reshape(X_B.shape[0], img_dim)\n",
    "\n",
    "    clip_value = 0.01\n",
    "    n_critic = 4\n",
    "\n",
    "    # Adversarial ground truths\n",
    "    valid = -np.ones((batch_size, 1))\n",
    "    fake = np.ones((batch_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Train the discriminator for n_critic iterations\n",
    "        for _ in range(n_critic):\n",
    "\n",
    "            # ----------------------\n",
    "            #  Train Discriminators\n",
    "            # ----------------------\n",
    "\n",
    "            # Sample generator inputs\n",
    "            imgs_A = sample_generator_input(X_A, batch_size)\n",
    "            imgs_B = sample_generator_input(X_B, batch_size)\n",
    "\n",
    "            # Translate images to their opposite domain\n",
    "            fake_B = G_AB.predict(imgs_A)\n",
    "            fake_A = G_BA.predict(imgs_B)\n",
    "\n",
    "            # Train the discriminators\n",
    "            D_A_loss_real = D_A.train_on_batch(imgs_A, valid)\n",
    "            D_A_loss_fake = D_A.train_on_batch(fake_A, fake)\n",
    "\n",
    "            D_B_loss_real = D_B.train_on_batch(imgs_B, valid)\n",
    "            D_B_loss_fake = D_B.train_on_batch(fake_B, fake)\n",
    "\n",
    "            D_A_loss = 0.5 * np.add(D_A_loss_real, D_A_loss_fake)\n",
    "            D_B_loss = 0.5 * np.add(D_B_loss_real, D_B_loss_fake)\n",
    "\n",
    "            # Clip discriminator weights\n",
    "            for d in [D_A, D_B]:\n",
    "                for l in d.layers:\n",
    "                    weights = l.get_weights()\n",
    "                    weights = [np.clip(w, -clip_value, clip_value) for w in weights]\n",
    "                    l.set_weights(weights)\n",
    "\n",
    "        # ------------------\n",
    "        #  Train Generators\n",
    "        # ------------------\n",
    "\n",
    "        # Train the generators\n",
    "        g_loss = combined.train_on_batch([imgs_A, imgs_B], [valid, valid, imgs_A, imgs_B])\n",
    "\n",
    "        # Plot the progress\n",
    "        print (\"%d [D1 loss: %f] [D2 loss: %f] [G loss: %f]\" \\\n",
    "            % (epoch, D_A_loss[0], D_B_loss[0], g_loss[0]))\n",
    "\n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % sample_interval == 0:\n",
    "            save_imgs(G_AB, G_BA, epoch, \n",
    "                      X_A, X_B, \n",
    "                      img_rows, img_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('images'):\n",
    "    os.makedirs('images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_dim = img_rows * img_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# create optimizer\n",
    "optimizer = Adam(0.0002, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the discriminators\n",
    "D_A = build_discriminator(img_dim)\n",
    "D_A.compile(loss=wasserstein_loss,\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "D_B = build_discriminator(img_dim)\n",
    "D_B.compile(loss=wasserstein_loss,\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "# Construct Computational\n",
    "#   Graph of Generators\n",
    "#-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the generators\n",
    "G_AB = build_generator(img_dim)\n",
    "G_BA = build_generator(img_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the combined model we will only train the generators\n",
    "D_A.trainable = False\n",
    "D_B.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generator takes images from their respective domains as inputs\n",
    "imgs_A = Input(shape=(img_dim,))\n",
    "imgs_B = Input(shape=(img_dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generators translates the images to the opposite domain\n",
    "fake_B = G_AB(imgs_A)\n",
    "fake_A = G_BA(imgs_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The discriminators determines validity of translated images\n",
    "valid_A = D_A(fake_A)\n",
    "valid_B = D_B(fake_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generators translate the images back to their original domain\n",
    "recov_A = G_BA(fake_B)\n",
    "recov_B = G_AB(fake_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The combined model  (stacked generators and discriminators)\n",
    "combined = Model(inputs=[imgs_A, imgs_B], outputs=[valid_A, valid_B, recov_A, recov_B])\n",
    "combined.compile(loss=[wasserstein_loss, wasserstein_loss, 'mae', 'mae'],\n",
    "                 optimizer=optimizer,\n",
    "                 loss_weights=[1, 1, 100, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D1 loss: 0.000081] [D2 loss: 0.000089] [G loss: 195.662094]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D1 loss: 0.000091] [D2 loss: 0.000095] [G loss: 193.996979]\n",
      "2 [D1 loss: 0.000093] [D2 loss: 0.000096] [G loss: 193.581238]\n",
      "3 [D1 loss: 0.000093] [D2 loss: 0.000095] [G loss: 193.942459]\n",
      "4 [D1 loss: 0.000090] [D2 loss: 0.000091] [G loss: 192.923416]\n",
      "5 [D1 loss: 0.000086] [D2 loss: 0.000086] [G loss: 192.663849]\n",
      "6 [D1 loss: 0.000082] [D2 loss: 0.000081] [G loss: 192.518951]\n",
      "7 [D1 loss: 0.000078] [D2 loss: 0.000076] [G loss: 193.187866]\n",
      "8 [D1 loss: 0.000074] [D2 loss: 0.000071] [G loss: 192.921753]\n",
      "9 [D1 loss: 0.000070] [D2 loss: 0.000067] [G loss: 191.755005]\n",
      "10 [D1 loss: 0.000066] [D2 loss: 0.000063] [G loss: 191.921646]\n",
      "11 [D1 loss: 0.000063] [D2 loss: 0.000060] [G loss: 191.658981]\n",
      "12 [D1 loss: 0.000060] [D2 loss: 0.000057] [G loss: 191.580139]\n",
      "13 [D1 loss: 0.000057] [D2 loss: 0.000055] [G loss: 190.972107]\n",
      "14 [D1 loss: 0.000055] [D2 loss: 0.000053] [G loss: 190.847275]\n",
      "15 [D1 loss: 0.000053] [D2 loss: 0.000051] [G loss: 189.069412]\n",
      "16 [D1 loss: 0.000051] [D2 loss: 0.000050] [G loss: 189.301651]\n",
      "17 [D1 loss: 0.000050] [D2 loss: 0.000049] [G loss: 190.000824]\n",
      "18 [D1 loss: 0.000049] [D2 loss: 0.000048] [G loss: 189.505219]\n",
      "19 [D1 loss: 0.000048] [D2 loss: 0.000048] [G loss: 188.344604]\n",
      "20 [D1 loss: 0.000048] [D2 loss: 0.000047] [G loss: 187.973816]\n",
      "21 [D1 loss: 0.000047] [D2 loss: 0.000047] [G loss: 186.500153]\n",
      "22 [D1 loss: 0.000047] [D2 loss: 0.000047] [G loss: 186.173676]\n",
      "23 [D1 loss: 0.000047] [D2 loss: 0.000046] [G loss: 185.330444]\n",
      "24 [D1 loss: 0.000046] [D2 loss: 0.000046] [G loss: 184.968933]\n",
      "25 [D1 loss: 0.000046] [D2 loss: 0.000046] [G loss: 184.333176]\n",
      "26 [D1 loss: 0.000046] [D2 loss: 0.000046] [G loss: 183.436295]\n",
      "27 [D1 loss: 0.000046] [D2 loss: 0.000046] [G loss: 182.842896]\n",
      "28 [D1 loss: 0.000046] [D2 loss: 0.000046] [G loss: 183.293457]\n",
      "29 [D1 loss: 0.000046] [D2 loss: 0.000046] [G loss: 180.824524]\n",
      "30 [D1 loss: 0.000045] [D2 loss: 0.000046] [G loss: 180.355652]\n",
      "31 [D1 loss: 0.000045] [D2 loss: 0.000045] [G loss: 178.583893]\n",
      "32 [D1 loss: 0.000045] [D2 loss: 0.000045] [G loss: 179.749146]\n",
      "33 [D1 loss: 0.000045] [D2 loss: 0.000045] [G loss: 179.257904]\n",
      "34 [D1 loss: 0.000045] [D2 loss: 0.000045] [G loss: 176.915100]\n",
      "35 [D1 loss: 0.000045] [D2 loss: 0.000045] [G loss: 177.949585]\n",
      "36 [D1 loss: 0.000045] [D2 loss: 0.000045] [G loss: 175.905792]\n",
      "37 [D1 loss: 0.000045] [D2 loss: 0.000045] [G loss: 173.872696]\n",
      "38 [D1 loss: 0.000045] [D2 loss: 0.000045] [G loss: 173.215683]\n",
      "39 [D1 loss: 0.000045] [D2 loss: 0.000045] [G loss: 171.750000]\n",
      "40 [D1 loss: 0.000045] [D2 loss: 0.000045] [G loss: 168.147919]\n",
      "41 [D1 loss: 0.000045] [D2 loss: 0.000045] [G loss: 170.505402]\n",
      "42 [D1 loss: 0.000044] [D2 loss: 0.000045] [G loss: 165.636993]\n",
      "43 [D1 loss: 0.000045] [D2 loss: 0.000044] [G loss: 169.655487]\n",
      "44 [D1 loss: 0.000044] [D2 loss: 0.000044] [G loss: 167.938400]\n",
      "45 [D1 loss: 0.000044] [D2 loss: 0.000044] [G loss: 165.934998]\n",
      "46 [D1 loss: 0.000044] [D2 loss: 0.000044] [G loss: 161.443604]\n",
      "47 [D1 loss: 0.000044] [D2 loss: 0.000044] [G loss: 163.472168]\n",
      "48 [D1 loss: 0.000044] [D2 loss: 0.000044] [G loss: 163.257599]\n",
      "49 [D1 loss: 0.000044] [D2 loss: 0.000044] [G loss: 156.752228]\n",
      "50 [D1 loss: 0.000044] [D2 loss: 0.000044] [G loss: 154.077179]\n",
      "51 [D1 loss: 0.000044] [D2 loss: 0.000044] [G loss: 154.544495]\n",
      "52 [D1 loss: 0.000044] [D2 loss: 0.000044] [G loss: 154.117035]\n",
      "53 [D1 loss: 0.000044] [D2 loss: 0.000044] [G loss: 148.722778]\n",
      "54 [D1 loss: 0.000044] [D2 loss: 0.000044] [G loss: 153.065430]\n",
      "55 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 145.794067]\n",
      "56 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 146.959976]\n",
      "57 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 143.445038]\n",
      "58 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 140.508423]\n",
      "59 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 142.891739]\n",
      "60 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 137.796951]\n",
      "61 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 136.057434]\n",
      "62 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 127.205887]\n",
      "63 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 126.168732]\n",
      "64 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 126.804970]\n",
      "65 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 124.047318]\n",
      "66 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 125.561516]\n",
      "67 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 121.681198]\n",
      "68 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 133.649338]\n",
      "69 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 118.479111]\n",
      "70 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 113.748375]\n",
      "71 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 119.272438]\n",
      "72 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 114.965813]\n",
      "73 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 117.073929]\n",
      "74 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 109.258934]\n",
      "75 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 113.373016]\n",
      "76 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 104.380844]\n",
      "77 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 103.137070]\n",
      "78 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 101.849701]\n",
      "79 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 108.603424]\n",
      "80 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 102.205948]\n",
      "81 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 102.261292]\n",
      "82 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 107.659988]\n",
      "83 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 103.271416]\n",
      "84 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 98.160095]\n",
      "85 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 95.203934]\n",
      "86 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 108.862534]\n",
      "87 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 94.833168]\n",
      "88 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 99.164474]\n",
      "89 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 105.568085]\n",
      "90 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 96.022919]\n",
      "91 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 100.645149]\n",
      "92 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 93.645279]\n",
      "93 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 109.169502]\n",
      "94 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 99.116211]\n",
      "95 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 88.655289]\n",
      "96 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 93.620529]\n",
      "97 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 94.959778]\n",
      "98 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 92.434250]\n",
      "99 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 88.463470]\n",
      "100 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 94.009933]\n",
      "101 [D1 loss: 0.000040] [D2 loss: 0.000041] [G loss: 96.711029]\n",
      "102 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 88.264511]\n",
      "103 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 92.243515]\n",
      "104 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 90.019714]\n",
      "105 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 86.582901]\n",
      "106 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 87.840088]\n",
      "107 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 93.293480]\n",
      "108 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 89.076363]\n",
      "109 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 90.985703]\n",
      "110 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 81.963432]\n",
      "111 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 84.194260]\n",
      "112 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 85.584335]\n",
      "113 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 80.021378]\n",
      "114 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 88.575562]\n",
      "115 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 95.571213]\n",
      "116 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 76.523407]\n",
      "117 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 86.338806]\n",
      "118 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 80.366821]\n",
      "119 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 77.768585]\n",
      "120 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 84.681389]\n",
      "121 [D1 loss: 0.000040] [D2 loss: 0.000039] [G loss: 87.450256]\n",
      "122 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 80.171555]\n",
      "123 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 87.912109]\n",
      "124 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 80.104309]\n",
      "125 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 85.911034]\n",
      "126 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 81.384521]\n",
      "127 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 89.099609]\n",
      "128 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 81.760208]\n",
      "129 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 79.926865]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 80.510185]\n",
      "131 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 89.073471]\n",
      "132 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 89.895767]\n",
      "133 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 81.892517]\n",
      "134 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 78.199097]\n",
      "135 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 76.666214]\n",
      "136 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 72.417206]\n",
      "137 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 84.458305]\n",
      "138 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 78.730972]\n",
      "139 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 77.482674]\n",
      "140 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 78.629974]\n",
      "141 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 76.981247]\n",
      "142 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 77.052887]\n",
      "143 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 81.799057]\n",
      "144 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 78.787079]\n",
      "145 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 75.710815]\n",
      "146 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 81.798271]\n",
      "147 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 76.955811]\n",
      "148 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 69.457535]\n",
      "149 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 74.366089]\n",
      "150 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 85.234985]\n",
      "151 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 78.738060]\n",
      "152 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 73.414658]\n",
      "153 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 72.253860]\n",
      "154 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 79.614937]\n",
      "155 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 75.050034]\n",
      "156 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 73.289749]\n",
      "157 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 76.461899]\n",
      "158 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 73.043365]\n",
      "159 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 74.100471]\n",
      "160 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 91.973068]\n",
      "161 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 76.717606]\n",
      "162 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 78.749695]\n",
      "163 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 73.786148]\n",
      "164 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 76.089600]\n",
      "165 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 77.954674]\n",
      "166 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 80.203888]\n",
      "167 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 70.676514]\n",
      "168 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 75.000717]\n",
      "169 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 80.577309]\n",
      "170 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 77.195900]\n",
      "171 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 73.589203]\n",
      "172 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 71.397141]\n",
      "173 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 74.179131]\n",
      "174 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 74.581726]\n",
      "175 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 97.438049]\n",
      "176 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 80.931763]\n",
      "177 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 79.814865]\n",
      "178 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 70.191925]\n",
      "179 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 80.632660]\n",
      "180 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 66.951279]\n",
      "181 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 64.265450]\n",
      "182 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 72.979691]\n",
      "183 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 77.328201]\n",
      "184 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 71.590134]\n",
      "185 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 69.926346]\n",
      "186 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 73.128448]\n",
      "187 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 69.129318]\n",
      "188 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 79.848816]\n",
      "189 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 78.418091]\n",
      "190 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 65.967575]\n",
      "191 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 66.072868]\n",
      "192 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 67.285988]\n",
      "193 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 72.976257]\n",
      "194 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 79.327087]\n",
      "195 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 69.875412]\n",
      "196 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 64.552994]\n",
      "197 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 81.739464]\n",
      "198 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 70.350029]\n",
      "199 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 67.734055]\n",
      "200 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 71.734100]\n",
      "201 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 69.514099]\n",
      "202 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 75.144333]\n",
      "203 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 70.904358]\n",
      "204 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 72.988205]\n",
      "205 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 63.925957]\n",
      "206 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 65.501854]\n",
      "207 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 71.555710]\n",
      "208 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 72.388748]\n",
      "209 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 75.424561]\n",
      "210 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 75.683975]\n",
      "211 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 67.694061]\n",
      "212 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 74.623032]\n",
      "213 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 68.546722]\n",
      "214 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 67.265137]\n",
      "215 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 70.982773]\n",
      "216 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 71.790947]\n",
      "217 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 63.600990]\n",
      "218 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 80.424622]\n",
      "219 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 67.042381]\n",
      "220 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 66.056534]\n",
      "221 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 66.134415]\n",
      "222 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 71.951401]\n",
      "223 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 67.085129]\n",
      "224 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 66.874146]\n",
      "225 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 74.981140]\n",
      "226 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 70.417610]\n",
      "227 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 68.971931]\n",
      "228 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 82.027954]\n",
      "229 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 73.096695]\n",
      "230 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 65.749176]\n",
      "231 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 66.591644]\n",
      "232 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 76.823647]\n",
      "233 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 59.827793]\n",
      "234 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 67.504517]\n",
      "235 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 64.915932]\n",
      "236 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 66.930145]\n",
      "237 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 69.690994]\n",
      "238 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 65.366516]\n",
      "239 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 64.690994]\n",
      "240 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 62.532227]\n",
      "241 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 73.274979]\n",
      "242 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 71.901375]\n",
      "243 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 66.765228]\n",
      "244 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 61.458641]\n",
      "245 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 64.551399]\n",
      "246 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 64.892204]\n",
      "247 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 67.839722]\n",
      "248 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 59.580669]\n",
      "249 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 67.650818]\n",
      "250 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 59.816696]\n",
      "251 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 74.749672]\n",
      "252 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 59.689522]\n",
      "253 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 69.755508]\n",
      "254 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 64.732574]\n",
      "255 [D1 loss: 0.000036] [D2 loss: 0.000035] [G loss: 64.753677]\n",
      "256 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 69.346016]\n",
      "257 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 65.516853]\n",
      "258 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 61.656128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 70.328476]\n",
      "260 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 66.227875]\n",
      "261 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 70.187859]\n",
      "262 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 63.088196]\n",
      "263 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 67.885345]\n",
      "264 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 70.195198]\n",
      "265 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 69.089096]\n",
      "266 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 63.891193]\n",
      "267 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 66.149811]\n",
      "268 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 60.983379]\n",
      "269 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 64.666458]\n",
      "270 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 61.078197]\n",
      "271 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 61.028534]\n",
      "272 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 61.702236]\n",
      "273 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 68.798882]\n",
      "274 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 58.955429]\n",
      "275 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 62.466362]\n",
      "276 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 71.257927]\n",
      "277 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 59.253525]\n",
      "278 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 62.234283]\n",
      "279 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 61.090965]\n",
      "280 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 59.639046]\n",
      "281 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 60.631104]\n",
      "282 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 55.803097]\n",
      "283 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 66.376328]\n",
      "284 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 62.440224]\n",
      "285 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 61.048874]\n",
      "286 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 62.373680]\n",
      "287 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 71.528358]\n",
      "288 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 60.371700]\n",
      "289 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 65.047516]\n",
      "290 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 67.765350]\n",
      "291 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 74.201698]\n",
      "292 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 67.870392]\n",
      "293 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 59.580734]\n",
      "294 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 61.430450]\n",
      "295 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 59.058681]\n",
      "296 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 61.056740]\n",
      "297 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 62.112122]\n",
      "298 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 65.882713]\n",
      "299 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 69.976456]\n",
      "300 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 62.083000]\n",
      "301 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 74.004578]\n",
      "302 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 62.563065]\n",
      "303 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 60.917091]\n",
      "304 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 80.746460]\n",
      "305 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 58.966232]\n",
      "306 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 57.421898]\n",
      "307 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 60.608452]\n",
      "308 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 66.729034]\n",
      "309 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 67.944626]\n",
      "310 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 62.906612]\n",
      "311 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 67.152527]\n",
      "312 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 60.971497]\n",
      "313 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 64.227615]\n",
      "314 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 68.049545]\n",
      "315 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 56.844795]\n",
      "316 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 68.312759]\n",
      "317 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 64.459496]\n",
      "318 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 61.556290]\n",
      "319 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 58.640209]\n",
      "320 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 60.290573]\n",
      "321 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 54.243950]\n",
      "322 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 64.160591]\n",
      "323 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 56.087372]\n",
      "324 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 57.714249]\n",
      "325 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 57.551926]\n",
      "326 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 59.887932]\n",
      "327 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 55.739971]\n",
      "328 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 61.416061]\n",
      "329 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 64.861084]\n",
      "330 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 62.895466]\n",
      "331 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 56.647499]\n",
      "332 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 55.789860]\n",
      "333 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 68.753418]\n",
      "334 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 59.771786]\n",
      "335 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.755005]\n",
      "336 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 60.364552]\n",
      "337 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.425438]\n",
      "338 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 59.084885]\n",
      "339 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 62.060760]\n",
      "340 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 69.493774]\n",
      "341 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 67.018921]\n",
      "342 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 64.334679]\n",
      "343 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 65.545135]\n",
      "344 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 69.348343]\n",
      "345 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 61.822189]\n",
      "346 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 61.005539]\n",
      "347 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 61.708618]\n",
      "348 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.282204]\n",
      "349 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 62.828712]\n",
      "350 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 61.540249]\n",
      "351 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 62.333488]\n",
      "352 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 68.006310]\n",
      "353 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 62.580566]\n",
      "354 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.988293]\n",
      "355 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.940414]\n",
      "356 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.870590]\n",
      "357 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 60.017761]\n",
      "358 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 61.391899]\n",
      "359 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 61.091732]\n",
      "360 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.458473]\n",
      "361 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 59.049377]\n",
      "362 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 67.141998]\n",
      "363 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.449524]\n",
      "364 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 68.399803]\n",
      "365 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.989265]\n",
      "366 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.825974]\n",
      "367 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 65.305298]\n",
      "368 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 61.803509]\n",
      "369 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.023331]\n",
      "370 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 60.406246]\n",
      "371 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 64.752670]\n",
      "372 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.900764]\n",
      "373 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 71.381165]\n",
      "374 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.057693]\n",
      "375 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 60.663200]\n",
      "376 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.952164]\n",
      "377 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.298065]\n",
      "378 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.964279]\n",
      "379 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 59.958549]\n",
      "380 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.995022]\n",
      "381 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.398796]\n",
      "382 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 66.656715]\n",
      "383 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.926659]\n",
      "384 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.392666]\n",
      "385 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.358006]\n",
      "386 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.764511]\n",
      "387 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 64.318237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 61.279408]\n",
      "389 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 60.632568]\n",
      "390 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 62.445675]\n",
      "391 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.372147]\n",
      "392 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.960342]\n",
      "393 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.985840]\n",
      "394 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 59.687622]\n",
      "395 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.543030]\n",
      "396 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.292110]\n",
      "397 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 59.515930]\n",
      "398 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.482376]\n",
      "399 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.070473]\n",
      "400 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.415787]\n",
      "401 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 60.995506]\n",
      "402 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.708080]\n",
      "403 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.853668]\n",
      "404 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.158829]\n",
      "405 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.408241]\n",
      "406 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.808426]\n",
      "407 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.705559]\n",
      "408 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.421600]\n",
      "409 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.976200]\n",
      "410 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 60.489037]\n",
      "411 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 59.223366]\n",
      "412 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.842831]\n",
      "413 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.996994]\n",
      "414 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 63.282269]\n",
      "415 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.480621]\n",
      "416 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.299557]\n",
      "417 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 64.702187]\n",
      "418 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.548904]\n",
      "419 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.167431]\n",
      "420 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.089592]\n",
      "421 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 63.870811]\n",
      "422 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 62.137806]\n",
      "423 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.199547]\n",
      "424 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.164200]\n",
      "425 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 63.436638]\n",
      "426 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 62.776085]\n",
      "427 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.031021]\n",
      "428 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.542126]\n",
      "429 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.056473]\n",
      "430 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 61.162636]\n",
      "431 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.978348]\n",
      "432 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.310535]\n",
      "433 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.009453]\n",
      "434 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.205086]\n",
      "435 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 63.755836]\n",
      "436 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.996567]\n",
      "437 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 59.736599]\n",
      "438 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.690956]\n",
      "439 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.296951]\n",
      "440 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 60.647949]\n",
      "441 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.878410]\n",
      "442 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.679638]\n",
      "443 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.563446]\n",
      "444 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.542419]\n",
      "445 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.882729]\n",
      "446 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 61.041775]\n",
      "447 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.260666]\n",
      "448 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.888405]\n",
      "449 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.410889]\n",
      "450 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 60.537125]\n",
      "451 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.170700]\n",
      "452 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.417152]\n",
      "453 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 60.190689]\n",
      "454 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.689281]\n",
      "455 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 62.471169]\n",
      "456 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.179531]\n",
      "457 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 59.380424]\n",
      "458 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.147835]\n",
      "459 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.489693]\n",
      "460 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.403816]\n",
      "461 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.676239]\n",
      "462 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.583321]\n",
      "463 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.577843]\n",
      "464 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.275215]\n",
      "465 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.805092]\n",
      "466 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.379288]\n",
      "467 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.751160]\n",
      "468 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 59.400108]\n",
      "469 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.395786]\n",
      "470 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 65.397827]\n",
      "471 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.137543]\n",
      "472 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 64.084244]\n",
      "473 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.827019]\n",
      "474 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.219368]\n",
      "475 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.042534]\n",
      "476 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.701927]\n",
      "477 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.209175]\n",
      "478 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.386086]\n",
      "479 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 59.647041]\n",
      "480 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.439850]\n",
      "481 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.661026]\n",
      "482 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.762669]\n",
      "483 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.726776]\n",
      "484 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.813778]\n",
      "485 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.375061]\n",
      "486 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.754509]\n",
      "487 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.410484]\n",
      "488 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.322788]\n",
      "489 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.603161]\n",
      "490 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.275688]\n",
      "491 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.178467]\n",
      "492 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.458664]\n",
      "493 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.745987]\n",
      "494 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 61.737473]\n",
      "495 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.135948]\n",
      "496 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.630791]\n",
      "497 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.730854]\n",
      "498 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.496017]\n",
      "499 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 60.748020]\n",
      "500 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 69.108055]\n",
      "501 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.153210]\n",
      "502 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.076122]\n",
      "503 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.525738]\n",
      "504 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.615562]\n",
      "505 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.809929]\n",
      "506 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.972115]\n",
      "507 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.990677]\n",
      "508 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.283768]\n",
      "509 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.126583]\n",
      "510 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.180836]\n",
      "511 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 69.951180]\n",
      "512 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.135918]\n",
      "513 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.817253]\n",
      "514 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.567749]\n",
      "515 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.092628]\n",
      "516 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.296303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.645638]\n",
      "518 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.222000]\n",
      "519 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.744083]\n",
      "520 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.652451]\n",
      "521 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.726921]\n",
      "522 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.476452]\n",
      "523 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.477840]\n",
      "524 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.105511]\n",
      "525 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.841286]\n",
      "526 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.822906]\n",
      "527 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.345154]\n",
      "528 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.007698]\n",
      "529 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 68.414795]\n",
      "530 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.173775]\n",
      "531 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.613022]\n",
      "532 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.274418]\n",
      "533 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.276199]\n",
      "534 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.552132]\n",
      "535 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.991135]\n",
      "536 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.433746]\n",
      "537 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.322815]\n",
      "538 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.316135]\n",
      "539 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.713234]\n",
      "540 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.558205]\n",
      "541 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.851875]\n",
      "542 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.776031]\n",
      "543 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.579216]\n",
      "544 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.615166]\n",
      "545 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.022240]\n",
      "546 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.023876]\n",
      "547 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.432095]\n",
      "548 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.011215]\n",
      "549 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.633175]\n",
      "550 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.291702]\n",
      "551 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.185989]\n",
      "552 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.064842]\n",
      "553 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.654373]\n",
      "554 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.920410]\n",
      "555 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.648430]\n",
      "556 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.615211]\n",
      "557 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.821236]\n",
      "558 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.291748]\n",
      "559 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.660492]\n",
      "560 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.052406]\n",
      "561 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.579590]\n",
      "562 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.169315]\n",
      "563 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.338428]\n",
      "564 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.568848]\n",
      "565 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.082024]\n",
      "566 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.274952]\n",
      "567 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.463181]\n",
      "568 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 64.192459]\n",
      "569 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.608173]\n",
      "570 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.815533]\n",
      "571 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.641075]\n",
      "572 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.561729]\n",
      "573 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.646194]\n",
      "574 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.690369]\n",
      "575 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.302032]\n",
      "576 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.306229]\n",
      "577 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.927994]\n",
      "578 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.731102]\n",
      "579 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.329773]\n",
      "580 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.139824]\n",
      "581 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.341591]\n",
      "582 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.328331]\n",
      "583 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.181671]\n",
      "584 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.024803]\n",
      "585 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.821205]\n",
      "586 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.938160]\n",
      "587 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.192955]\n",
      "588 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.454056]\n",
      "589 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 61.388107]\n",
      "590 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.289398]\n",
      "591 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.401505]\n",
      "592 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.620522]\n",
      "593 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.849655]\n",
      "594 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.047157]\n",
      "595 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.242401]\n",
      "596 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.988720]\n",
      "597 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.640213]\n",
      "598 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.252434]\n",
      "599 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.413185]\n",
      "600 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.664467]\n",
      "601 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.292580]\n",
      "602 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.782570]\n",
      "603 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 66.013916]\n",
      "604 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.684086]\n",
      "605 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.635269]\n",
      "606 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.132137]\n",
      "607 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 62.805222]\n",
      "608 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.204769]\n",
      "609 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.854591]\n",
      "610 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.215977]\n",
      "611 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.286995]\n",
      "612 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.518448]\n",
      "613 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.788177]\n",
      "614 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.902096]\n",
      "615 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.192245]\n",
      "616 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.083614]\n",
      "617 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.566528]\n",
      "618 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.033524]\n",
      "619 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 61.585716]\n",
      "620 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.209091]\n",
      "621 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.766487]\n",
      "622 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.356354]\n",
      "623 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.972820]\n",
      "624 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.758472]\n",
      "625 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.579021]\n",
      "626 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.909451]\n",
      "627 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.726151]\n",
      "628 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.107452]\n",
      "629 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.806519]\n",
      "630 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.400307]\n",
      "631 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.195240]\n",
      "632 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.441719]\n",
      "633 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.083572]\n",
      "634 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.545948]\n",
      "635 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.882637]\n",
      "636 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.971764]\n",
      "637 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.365051]\n",
      "638 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.237717]\n",
      "639 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.356518]\n",
      "640 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.389404]\n",
      "641 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.958397]\n",
      "642 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.661400]\n",
      "643 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.169365]\n",
      "644 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.943977]\n",
      "645 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.341034]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.174416]\n",
      "647 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.661171]\n",
      "648 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.280159]\n",
      "649 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.817726]\n",
      "650 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.418648]\n",
      "651 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.404572]\n",
      "652 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.667812]\n",
      "653 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.306431]\n",
      "654 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.122131]\n",
      "655 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.298565]\n",
      "656 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.309757]\n",
      "657 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.414677]\n",
      "658 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.951183]\n",
      "659 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.247345]\n",
      "660 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.736629]\n",
      "661 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.950447]\n",
      "662 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.247551]\n",
      "663 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.709709]\n",
      "664 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.181778]\n",
      "665 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.230087]\n",
      "666 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.005840]\n",
      "667 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 59.721363]\n",
      "668 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.148674]\n",
      "669 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.547119]\n",
      "670 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.564415]\n",
      "671 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.653500]\n",
      "672 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.591118]\n",
      "673 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.456657]\n",
      "674 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.023323]\n",
      "675 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.931530]\n",
      "676 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.036499]\n",
      "677 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.578156]\n",
      "678 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.110878]\n",
      "679 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.366493]\n",
      "680 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.846504]\n",
      "681 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.386726]\n",
      "682 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.878433]\n",
      "683 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.101387]\n",
      "684 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 60.017288]\n",
      "685 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.594269]\n",
      "686 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.147270]\n",
      "687 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.046875]\n",
      "688 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.801510]\n",
      "689 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.035744]\n",
      "690 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.397453]\n",
      "691 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.029564]\n",
      "692 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.197296]\n",
      "693 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.471195]\n",
      "694 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.684692]\n",
      "695 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.581829]\n",
      "696 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.693089]\n",
      "697 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.343788]\n",
      "698 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.829880]\n",
      "699 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.331757]\n",
      "700 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.756676]\n",
      "701 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.027107]\n",
      "702 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.437267]\n",
      "703 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.039856]\n",
      "704 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.476677]\n",
      "705 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.352112]\n",
      "706 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.485840]\n",
      "707 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.204109]\n",
      "708 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.887497]\n",
      "709 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.140129]\n",
      "710 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.477516]\n",
      "711 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.545525]\n",
      "712 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.622719]\n",
      "713 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.739555]\n",
      "714 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.056583]\n",
      "715 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.372124]\n",
      "716 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.619720]\n",
      "717 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.025673]\n",
      "718 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.001640]\n",
      "719 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.778015]\n",
      "720 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.017151]\n",
      "721 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.065910]\n",
      "722 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.318901]\n",
      "723 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.136711]\n",
      "724 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.276012]\n",
      "725 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.300320]\n",
      "726 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.745544]\n",
      "727 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.608025]\n",
      "728 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.864082]\n",
      "729 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.752121]\n",
      "730 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.252346]\n",
      "731 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.024994]\n",
      "732 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.730362]\n",
      "733 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.247200]\n",
      "734 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.685936]\n",
      "735 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.912979]\n",
      "736 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.397110]\n",
      "737 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.377068]\n",
      "738 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.237320]\n",
      "739 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.924858]\n",
      "740 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.657158]\n",
      "741 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.494766]\n",
      "742 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.348747]\n",
      "743 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.592659]\n",
      "744 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.922928]\n",
      "745 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.596210]\n",
      "746 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.214653]\n",
      "747 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.192589]\n",
      "748 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.671139]\n",
      "749 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.097652]\n",
      "750 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.511925]\n",
      "751 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.459599]\n",
      "752 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.828114]\n",
      "753 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.571606]\n",
      "754 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.824459]\n",
      "755 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.612427]\n",
      "756 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.049126]\n",
      "757 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.364841]\n",
      "758 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.905704]\n",
      "759 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.443035]\n",
      "760 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.374779]\n",
      "761 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.222122]\n",
      "762 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.783951]\n",
      "763 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.983253]\n",
      "764 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.326828]\n",
      "765 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.253292]\n",
      "766 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.142895]\n",
      "767 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.157814]\n",
      "768 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.639004]\n",
      "769 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.841393]\n",
      "770 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.430138]\n",
      "771 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.318649]\n",
      "772 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.396347]\n",
      "773 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.631744]\n",
      "774 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.347935]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "775 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.812828]\n",
      "776 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.262585]\n",
      "777 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.993286]\n",
      "778 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.531708]\n",
      "779 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.089298]\n",
      "780 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 63.590919]\n",
      "781 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.571827]\n",
      "782 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.145275]\n",
      "783 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.643845]\n",
      "784 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.570885]\n",
      "785 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.614891]\n",
      "786 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.495144]\n",
      "787 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.587967]\n",
      "788 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.581047]\n",
      "789 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.498848]\n",
      "790 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.130035]\n",
      "791 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.112732]\n",
      "792 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.001328]\n",
      "793 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.074539]\n",
      "794 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.291908]\n",
      "795 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.174618]\n",
      "796 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.203690]\n",
      "797 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.755577]\n",
      "798 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.212471]\n",
      "799 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.832603]\n",
      "800 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.854691]\n",
      "801 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.574280]\n",
      "802 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.275146]\n",
      "803 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.534309]\n",
      "804 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.817474]\n",
      "805 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.903725]\n",
      "806 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.881943]\n",
      "807 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.131721]\n",
      "808 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.388145]\n",
      "809 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.556496]\n",
      "810 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.616806]\n",
      "811 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.358330]\n",
      "812 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.814186]\n",
      "813 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.247829]\n",
      "814 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.861111]\n",
      "815 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.856857]\n",
      "816 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.963402]\n",
      "817 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.257629]\n",
      "818 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.205345]\n",
      "819 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.977730]\n",
      "820 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.411041]\n",
      "821 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.337440]\n",
      "822 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.107208]\n",
      "823 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.285385]\n",
      "824 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.912426]\n",
      "825 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.696548]\n",
      "826 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.705673]\n",
      "827 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.892395]\n",
      "828 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.011459]\n",
      "829 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.950874]\n",
      "830 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.231838]\n",
      "831 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.351780]\n",
      "832 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.645264]\n",
      "833 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.418579]\n",
      "834 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.594440]\n",
      "835 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.967670]\n",
      "836 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.504971]\n",
      "837 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.677074]\n",
      "838 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.030251]\n",
      "839 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.720352]\n",
      "840 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.604702]\n",
      "841 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.977882]\n",
      "842 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.493294]\n",
      "843 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.631218]\n",
      "844 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.708687]\n",
      "845 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.582237]\n",
      "846 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.558186]\n",
      "847 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.520645]\n",
      "848 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.001698]\n",
      "849 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.691818]\n",
      "850 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.383286]\n",
      "851 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.849567]\n",
      "852 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.347385]\n",
      "853 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.745140]\n",
      "854 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.582081]\n",
      "855 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.088043]\n",
      "856 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.369137]\n",
      "857 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.413834]\n",
      "858 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.066711]\n",
      "859 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.679039]\n",
      "860 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.895813]\n",
      "861 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.729679]\n",
      "862 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.902008]\n",
      "863 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.756954]\n",
      "864 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.901550]\n",
      "865 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.288445]\n",
      "866 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.071419]\n",
      "867 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 46.516373]\n",
      "868 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.054634]\n",
      "869 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.734650]\n",
      "870 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.124435]\n",
      "871 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.596313]\n",
      "872 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.844128]\n",
      "873 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.118137]\n",
      "874 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.594574]\n",
      "875 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.488571]\n",
      "876 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.826347]\n",
      "877 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.980064]\n",
      "878 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.287895]\n",
      "879 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.522194]\n",
      "880 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.969852]\n",
      "881 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.207474]\n",
      "882 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.639694]\n",
      "883 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.524456]\n",
      "884 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.722427]\n",
      "885 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.789036]\n",
      "886 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.247780]\n",
      "887 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.334061]\n",
      "888 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.717937]\n",
      "889 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.385765]\n",
      "890 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.323784]\n",
      "891 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.983994]\n",
      "892 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.517746]\n",
      "893 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.344688]\n",
      "894 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.091484]\n",
      "895 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.491360]\n",
      "896 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.025299]\n",
      "897 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.705544]\n",
      "898 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.068199]\n",
      "899 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.295425]\n",
      "900 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.411049]\n",
      "901 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.283245]\n",
      "902 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.100403]\n",
      "903 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.351719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "904 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.371761]\n",
      "905 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.426594]\n",
      "906 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.714951]\n",
      "907 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.648346]\n",
      "908 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.086891]\n",
      "909 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.524925]\n",
      "910 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.283348]\n",
      "911 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.563061]\n",
      "912 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.568443]\n",
      "913 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.520458]\n",
      "914 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.567207]\n",
      "915 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.299412]\n",
      "916 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.525635]\n",
      "917 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.949081]\n",
      "918 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.134941]\n",
      "919 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.185246]\n",
      "920 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.323250]\n",
      "921 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.990135]\n",
      "922 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.841820]\n",
      "923 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.817760]\n",
      "924 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.535561]\n",
      "925 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 46.903786]\n",
      "926 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.240318]\n",
      "927 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.617325]\n",
      "928 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.427628]\n",
      "929 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.786655]\n",
      "930 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 48.386230]\n",
      "931 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.039604]\n",
      "932 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.018883]\n",
      "933 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.578243]\n",
      "934 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.158478]\n",
      "935 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.470455]\n",
      "936 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.432026]\n",
      "937 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.473820]\n",
      "938 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.894798]\n",
      "939 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.777069]\n",
      "940 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.818512]\n",
      "941 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.096573]\n",
      "942 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.697037]\n",
      "943 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.158012]\n",
      "944 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.628059]\n",
      "945 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.470234]\n",
      "946 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.011345]\n",
      "947 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.248028]\n",
      "948 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.502655]\n",
      "949 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.559052]\n",
      "950 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.231178]\n",
      "951 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.768814]\n",
      "952 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.052589]\n",
      "953 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.002678]\n",
      "954 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.812920]\n",
      "955 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.505508]\n",
      "956 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.528156]\n",
      "957 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 48.837654]\n",
      "958 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.880989]\n",
      "959 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.875664]\n",
      "960 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.002647]\n",
      "961 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.092316]\n",
      "962 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.719971]\n",
      "963 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.650978]\n",
      "964 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.820358]\n",
      "965 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.944405]\n",
      "966 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.374657]\n",
      "967 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.207005]\n",
      "968 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.722591]\n",
      "969 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.087013]\n",
      "970 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.376953]\n",
      "971 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.660439]\n",
      "972 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.000404]\n",
      "973 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.743359]\n",
      "974 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.651909]\n",
      "975 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.465988]\n",
      "976 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.036247]\n",
      "977 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.119667]\n",
      "978 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.646149]\n",
      "979 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.305981]\n",
      "980 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.872696]\n",
      "981 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.488689]\n",
      "982 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.043785]\n",
      "983 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.532578]\n",
      "984 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.884949]\n",
      "985 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 49.600506]\n",
      "986 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.001106]\n",
      "987 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.155907]\n",
      "988 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.576004]\n",
      "989 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 49.448547]\n",
      "990 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 54.605026]\n",
      "991 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 51.702454]\n",
      "992 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.499809]\n",
      "993 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.384342]\n",
      "994 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.013302]\n",
      "995 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.479630]\n",
      "996 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.655785]\n",
      "997 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.587406]\n",
      "998 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.760170]\n",
      "999 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.644791]\n",
      "1000 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.304901]\n",
      "1001 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.070473]\n",
      "1002 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.839638]\n",
      "1003 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.120178]\n",
      "1004 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.501362]\n",
      "1005 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.683960]\n",
      "1006 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.771378]\n",
      "1007 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.468399]\n",
      "1008 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.701508]\n",
      "1009 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.871277]\n",
      "1010 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.357674]\n",
      "1011 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.559944]\n",
      "1012 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.190723]\n",
      "1013 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.642353]\n",
      "1014 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.756901]\n",
      "1015 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.505264]\n",
      "1016 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 52.191391]\n",
      "1017 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.248123]\n",
      "1018 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.171844]\n",
      "1019 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.699200]\n",
      "1020 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.968239]\n",
      "1021 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.136959]\n",
      "1022 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.816582]\n",
      "1023 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 52.745216]\n",
      "1024 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 48.659821]\n",
      "1025 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.243317]\n",
      "1026 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.791824]\n",
      "1027 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.452278]\n",
      "1028 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.540310]\n",
      "1029 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.367752]\n",
      "1030 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.417259]\n",
      "1031 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.431355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1032 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.109581]\n",
      "1033 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.866104]\n",
      "1034 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.742210]\n",
      "1035 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.994095]\n",
      "1036 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.699951]\n",
      "1037 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.606190]\n",
      "1038 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.798225]\n",
      "1039 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 47.725311]\n",
      "1040 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.819519]\n",
      "1041 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.395397]\n",
      "1042 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.488533]\n",
      "1043 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 46.454384]\n",
      "1044 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.632385]\n",
      "1045 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.175690]\n",
      "1046 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.746441]\n",
      "1047 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 46.454510]\n",
      "1048 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.273895]\n",
      "1049 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.872494]\n",
      "1050 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.707787]\n",
      "1051 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 47.836617]\n",
      "1052 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.515549]\n",
      "1053 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.227562]\n",
      "1054 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.101036]\n",
      "1055 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.855736]\n",
      "1056 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 47.864578]\n",
      "1057 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 48.285843]\n",
      "1058 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.152519]\n",
      "1059 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 51.567101]\n",
      "1060 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.076698]\n",
      "1061 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.174957]\n",
      "1062 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.925575]\n",
      "1063 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.442886]\n",
      "1064 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.677322]\n",
      "1065 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.613491]\n",
      "1066 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.782471]\n",
      "1067 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.765388]\n",
      "1068 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.359909]\n",
      "1069 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.652691]\n",
      "1070 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.219841]\n",
      "1071 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.057751]\n",
      "1072 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.838005]\n",
      "1073 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.234200]\n",
      "1074 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.314682]\n",
      "1075 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 50.561996]\n",
      "1076 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.536243]\n",
      "1077 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.980061]\n",
      "1078 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 53.628536]\n",
      "1079 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.012939]\n",
      "1080 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.771271]\n",
      "1081 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 47.326416]\n",
      "1082 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 45.308212]\n",
      "1083 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 47.429222]\n",
      "1084 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.434261]\n",
      "1085 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.402000]\n",
      "1086 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.606823]\n",
      "1087 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.139858]\n",
      "1088 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 52.485657]\n",
      "1089 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.256699]\n",
      "1090 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 45.064728]\n",
      "1091 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 53.608921]\n",
      "1092 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.649239]\n",
      "1093 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.547043]\n",
      "1094 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.727440]\n",
      "1095 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.617790]\n",
      "1096 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.858429]\n",
      "1097 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.038246]\n",
      "1098 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.668816]\n",
      "1099 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.185005]\n",
      "1100 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.976601]\n",
      "1101 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.050484]\n",
      "1102 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.742737]\n",
      "1103 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.968433]\n",
      "1104 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.276154]\n",
      "1105 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.014610]\n",
      "1106 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 49.488190]\n",
      "1107 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 47.888268]\n",
      "1108 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.763405]\n",
      "1109 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 60.170387]\n",
      "1110 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.404350]\n",
      "1111 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.155457]\n",
      "1112 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.978443]\n",
      "1113 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.663834]\n",
      "1114 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 46.659828]\n",
      "1115 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.874771]\n",
      "1116 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.881962]\n",
      "1117 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.454674]\n",
      "1118 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 47.550980]\n",
      "1119 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.832310]\n",
      "1120 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 44.410828]\n",
      "1121 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.836884]\n",
      "1122 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.698013]\n",
      "1123 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.065651]\n",
      "1124 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 48.175888]\n",
      "1125 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.472061]\n",
      "1126 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.561558]\n",
      "1127 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.951248]\n",
      "1128 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.876816]\n",
      "1129 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.902206]\n",
      "1130 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.655762]\n",
      "1131 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.648468]\n",
      "1132 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.720551]\n",
      "1133 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.561619]\n",
      "1134 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.848015]\n",
      "1135 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.898018]\n",
      "1136 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.617329]\n",
      "1137 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 47.457642]\n",
      "1138 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 47.205166]\n",
      "1139 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.839951]\n",
      "1140 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 44.058899]\n",
      "1141 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.140068]\n",
      "1142 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.903694]\n",
      "1143 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.322212]\n",
      "1144 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.114716]\n",
      "1145 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 46.399662]\n",
      "1146 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.199356]\n",
      "1147 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.723938]\n",
      "1148 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.328484]\n",
      "1149 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.973186]\n",
      "1150 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.592682]\n",
      "1151 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 43.520336]\n",
      "1152 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 55.568237]\n",
      "1153 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 47.675625]\n",
      "1154 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.290985]\n",
      "1155 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.110657]\n",
      "1156 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 43.206535]\n",
      "1157 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.410858]\n",
      "1158 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.184647]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1159 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.317703]\n",
      "1160 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.137169]\n",
      "1161 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 49.479607]\n",
      "1162 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.935310]\n",
      "1163 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.015831]\n",
      "1164 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.116837]\n",
      "1165 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.371574]\n",
      "1166 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.414032]\n",
      "1167 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.190308]\n",
      "1168 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.419037]\n",
      "1169 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 47.101730]\n",
      "1170 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.756294]\n",
      "1171 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.521317]\n",
      "1172 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.139534]\n",
      "1173 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.926872]\n",
      "1174 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.414177]\n",
      "1175 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.407326]\n",
      "1176 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.607269]\n",
      "1177 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.468288]\n",
      "1178 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 50.593719]\n",
      "1179 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.921867]\n",
      "1180 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.486553]\n",
      "1181 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.639927]\n",
      "1182 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.484108]\n",
      "1183 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.567955]\n",
      "1184 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.853951]\n",
      "1185 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.577591]\n",
      "1186 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.772301]\n",
      "1187 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 41.393417]\n",
      "1188 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 43.783180]\n",
      "1189 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 50.172070]\n",
      "1190 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.108925]\n",
      "1191 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.610695]\n",
      "1192 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.716438]\n",
      "1193 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 52.068378]\n",
      "1194 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 50.093269]\n",
      "1195 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.814972]\n",
      "1196 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.206665]\n",
      "1197 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.151749]\n",
      "1198 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.573673]\n",
      "1199 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.484535]\n",
      "1200 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.296959]\n",
      "1201 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.082424]\n",
      "1202 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.639263]\n",
      "1203 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.755375]\n",
      "1204 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.777245]\n",
      "1205 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.170837]\n",
      "1206 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 41.451035]\n",
      "1207 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.602055]\n",
      "1208 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.322254]\n",
      "1209 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.392937]\n",
      "1210 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.127029]\n",
      "1211 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 49.902985]\n",
      "1212 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.721333]\n",
      "1213 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.480888]\n",
      "1214 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.819084]\n",
      "1215 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.373875]\n",
      "1216 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.287422]\n",
      "1217 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.068634]\n",
      "1218 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.816605]\n",
      "1219 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.825226]\n",
      "1220 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.912781]\n",
      "1221 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 48.226456]\n",
      "1222 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.188774]\n",
      "1223 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.838154]\n",
      "1224 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.246506]\n",
      "1225 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.068718]\n",
      "1226 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.264107]\n",
      "1227 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.817745]\n",
      "1228 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.513924]\n",
      "1229 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.267418]\n",
      "1230 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 44.271435]\n",
      "1231 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 48.319237]\n",
      "1232 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.910168]\n",
      "1233 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.156967]\n",
      "1234 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.552345]\n",
      "1235 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.289154]\n",
      "1236 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 49.037292]\n",
      "1237 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.768829]\n",
      "1238 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 46.836411]\n",
      "1239 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.745636]\n",
      "1240 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.739414]\n",
      "1241 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 47.261627]\n",
      "1242 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.101006]\n",
      "1243 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 45.321915]\n",
      "1244 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.843269]\n",
      "1245 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.993462]\n",
      "1246 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.286751]\n",
      "1247 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.758430]\n",
      "1248 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.683464]\n",
      "1249 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.067169]\n",
      "1250 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.070259]\n",
      "1251 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.819183]\n",
      "1252 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 47.018364]\n",
      "1253 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.773243]\n",
      "1254 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.174652]\n",
      "1255 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.467049]\n",
      "1256 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.239429]\n",
      "1257 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 42.450172]\n",
      "1258 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 53.422276]\n",
      "1259 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.750038]\n",
      "1260 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.267395]\n",
      "1261 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.337189]\n",
      "1262 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.813087]\n",
      "1263 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.597694]\n",
      "1264 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.786987]\n",
      "1265 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.875969]\n",
      "1266 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.911690]\n",
      "1267 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 44.690796]\n",
      "1268 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.921944]\n",
      "1269 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.462479]\n",
      "1270 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.347332]\n",
      "1271 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 42.856163]\n",
      "1272 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.365524]\n",
      "1273 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.808876]\n",
      "1274 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.112198]\n",
      "1275 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 45.242050]\n",
      "1276 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 51.945000]\n",
      "1277 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.081406]\n",
      "1278 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.349625]\n",
      "1279 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 49.264931]\n",
      "1280 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.441566]\n",
      "1281 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 46.972534]\n",
      "1282 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.424313]\n",
      "1283 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.146217]\n",
      "1284 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 50.431778]\n",
      "1285 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 46.041832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.499760]\n",
      "1287 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.083450]\n",
      "1288 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 50.276840]\n",
      "1289 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 44.485619]\n",
      "1290 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 45.410225]\n",
      "1291 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.610054]\n",
      "1292 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.359489]\n",
      "1293 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.603546]\n",
      "1294 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.760635]\n",
      "1295 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.911972]\n",
      "1296 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.548923]\n",
      "1297 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.011684]\n",
      "1298 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.395035]\n",
      "1299 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 42.209057]\n",
      "1300 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.568718]\n",
      "1301 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.632248]\n",
      "1302 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 43.758408]\n",
      "1303 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 46.193050]\n",
      "1304 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.618279]\n",
      "1305 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.531174]\n",
      "1306 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.116470]\n",
      "1307 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.409714]\n",
      "1308 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 45.234482]\n",
      "1309 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.012745]\n",
      "1310 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 43.927261]\n",
      "1311 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.617523]\n",
      "1312 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.094482]\n",
      "1313 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.150841]\n",
      "1314 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 41.524132]\n",
      "1315 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 46.613152]\n",
      "1316 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.908707]\n",
      "1317 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.013054]\n",
      "1318 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.069313]\n",
      "1319 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.357674]\n",
      "1320 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.439728]\n",
      "1321 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.361290]\n",
      "1322 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.566166]\n",
      "1323 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.336712]\n",
      "1324 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.233528]\n",
      "1325 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.697033]\n",
      "1326 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 43.826965]\n",
      "1327 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 43.049019]\n",
      "1328 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.511711]\n",
      "1329 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.999374]\n",
      "1330 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 41.681465]\n",
      "1331 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.187744]\n",
      "1332 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 43.176876]\n",
      "1333 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.029251]\n",
      "1334 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 40.962563]\n",
      "1335 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 44.217838]\n",
      "1336 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.324142]\n",
      "1337 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 43.099716]\n",
      "1338 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 41.344486]\n",
      "1339 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 42.210472]\n",
      "1340 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.717979]\n",
      "1341 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.645813]\n",
      "1342 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.471821]\n",
      "1343 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.656090]\n",
      "1344 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 46.360302]\n",
      "1345 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.555141]\n",
      "1346 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 43.723869]\n",
      "1347 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 41.741585]\n",
      "1348 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 44.121346]\n",
      "1349 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.458862]\n",
      "1350 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.978485]\n",
      "1351 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.577827]\n",
      "1352 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.152561]\n",
      "1353 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.975861]\n",
      "1354 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.899796]\n",
      "1355 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.039139]\n",
      "1356 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.829826]\n",
      "1357 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.085007]\n",
      "1358 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.000244]\n",
      "1359 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.115585]\n",
      "1360 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 43.324951]\n",
      "1361 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 42.666695]\n",
      "1362 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.383553]\n",
      "1363 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.469498]\n",
      "1364 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 44.303623]\n",
      "1365 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 44.112549]\n",
      "1366 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 44.427052]\n",
      "1367 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 41.150047]\n",
      "1368 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 46.163269]\n",
      "1369 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 48.313507]\n",
      "1370 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.946098]\n",
      "1371 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.174347]\n",
      "1372 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.730125]\n",
      "1373 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 41.459885]\n",
      "1374 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.202583]\n",
      "1375 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.309822]\n",
      "1376 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.270576]\n",
      "1377 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.326340]\n",
      "1378 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 43.976959]\n",
      "1379 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 44.563141]\n",
      "1380 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 47.969841]\n",
      "1381 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 49.617531]\n",
      "1382 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 42.568115]\n",
      "1383 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.961624]\n",
      "1384 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.602016]\n",
      "1385 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.381958]\n",
      "1386 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.908226]\n",
      "1387 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 41.763962]\n",
      "1388 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.178822]\n",
      "1389 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.912132]\n",
      "1390 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.211655]\n",
      "1391 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 39.740269]\n",
      "1392 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.941597]\n",
      "1393 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.726822]\n",
      "1394 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.024612]\n",
      "1395 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.747543]\n",
      "1396 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.808777]\n",
      "1397 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 39.628151]\n",
      "1398 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 43.334145]\n",
      "1399 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 44.773880]\n",
      "1400 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.951927]\n",
      "1401 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 45.710632]\n",
      "1402 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 45.372887]\n",
      "1403 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.141609]\n",
      "1404 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 43.025978]\n",
      "1405 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.816528]\n",
      "1406 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.343948]\n",
      "1407 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.590725]\n",
      "1408 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.792717]\n",
      "1409 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.450104]\n",
      "1410 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.079643]\n",
      "1411 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 41.811226]\n",
      "1412 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 45.926052]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1413 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 44.166367]\n",
      "1414 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 46.228210]\n",
      "1415 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.789387]\n",
      "1416 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.997589]\n",
      "1417 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.618301]\n",
      "1418 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.659981]\n",
      "1419 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.297180]\n",
      "1420 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.759132]\n",
      "1421 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.102978]\n",
      "1422 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.106361]\n",
      "1423 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.641457]\n",
      "1424 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.325310]\n",
      "1425 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.167019]\n",
      "1426 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.267841]\n",
      "1427 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 45.213257]\n",
      "1428 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 42.831715]\n",
      "1429 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 47.085136]\n",
      "1430 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.375626]\n",
      "1431 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 49.619522]\n",
      "1432 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.817726]\n",
      "1433 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.680641]\n",
      "1434 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 43.456844]\n",
      "1435 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.206810]\n",
      "1436 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.853180]\n",
      "1437 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 44.952152]\n",
      "1438 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 42.026634]\n",
      "1439 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.614269]\n",
      "1440 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.741997]\n",
      "1441 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.654793]\n",
      "1442 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.424026]\n",
      "1443 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.574699]\n",
      "1444 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.865189]\n",
      "1445 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.613007]\n",
      "1446 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 42.022182]\n",
      "1447 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 42.205742]\n",
      "1448 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.851013]\n",
      "1449 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.194366]\n",
      "1450 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 42.273720]\n",
      "1451 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.499847]\n",
      "1452 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 44.582779]\n",
      "1453 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.827194]\n",
      "1454 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.286465]\n",
      "1455 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.665066]\n",
      "1456 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.044518]\n",
      "1457 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.553848]\n",
      "1458 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.949570]\n",
      "1459 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 41.026638]\n",
      "1460 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.448437]\n",
      "1461 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.155846]\n",
      "1462 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 50.570904]\n",
      "1463 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.583183]\n",
      "1464 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 42.457932]\n",
      "1465 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 44.374756]\n",
      "1466 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 47.182907]\n",
      "1467 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 48.897846]\n",
      "1468 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.300858]\n",
      "1469 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.771732]\n",
      "1470 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.969963]\n",
      "1471 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.670513]\n",
      "1472 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.193275]\n",
      "1473 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.234360]\n",
      "1474 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.126457]\n",
      "1475 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 44.250336]\n",
      "1476 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.175087]\n",
      "1477 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 40.214039]\n",
      "1478 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 39.351936]\n",
      "1479 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.808338]\n",
      "1480 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 43.951542]\n",
      "1481 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 46.251072]\n",
      "1482 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.322739]\n",
      "1483 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 44.137180]\n",
      "1484 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.845749]\n",
      "1485 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.963860]\n",
      "1486 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.483925]\n",
      "1487 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.475250]\n",
      "1488 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.059444]\n",
      "1489 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.517502]\n",
      "1490 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 39.847679]\n",
      "1491 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.317093]\n",
      "1492 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.879707]\n",
      "1493 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.214622]\n",
      "1494 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 43.657360]\n",
      "1495 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 48.298958]\n",
      "1496 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.872116]\n",
      "1497 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 44.212540]\n",
      "1498 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 45.217506]\n",
      "1499 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 43.854889]\n",
      "1500 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 44.167358]\n",
      "1501 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 42.289951]\n",
      "1502 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.148998]\n",
      "1503 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.350212]\n",
      "1504 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.181705]\n",
      "1505 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.109291]\n",
      "1506 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.916950]\n",
      "1507 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.077419]\n",
      "1508 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 42.835011]\n",
      "1509 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 42.563080]\n",
      "1510 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.945641]\n",
      "1511 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.629440]\n",
      "1512 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.917999]\n",
      "1513 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.179852]\n",
      "1514 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 46.037540]\n",
      "1515 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 50.138603]\n",
      "1516 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 45.124489]\n",
      "1517 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.756538]\n",
      "1518 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.177147]\n",
      "1519 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.265442]\n",
      "1520 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.337807]\n",
      "1521 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.872589]\n",
      "1522 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.705379]\n",
      "1523 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.189415]\n",
      "1524 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.664089]\n",
      "1525 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.893127]\n",
      "1526 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.072723]\n",
      "1527 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.965202]\n",
      "1528 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.555553]\n",
      "1529 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.171829]\n",
      "1530 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 52.149834]\n",
      "1531 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 41.562988]\n",
      "1532 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 43.190025]\n",
      "1533 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.445969]\n",
      "1534 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 43.765491]\n",
      "1535 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.635525]\n",
      "1536 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.559502]\n",
      "1537 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.294556]\n",
      "1538 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 39.773430]\n",
      "1539 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.249306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1540 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.829750]\n",
      "1541 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 42.549973]\n",
      "1542 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 44.168243]\n",
      "1543 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.401176]\n",
      "1544 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.824863]\n",
      "1545 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.368816]\n",
      "1546 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 40.621487]\n",
      "1547 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 47.795319]\n",
      "1548 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.461281]\n",
      "1549 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 42.265648]\n",
      "1550 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 44.586288]\n",
      "1551 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.946854]\n",
      "1552 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.251400]\n",
      "1553 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.507256]\n",
      "1554 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.561157]\n",
      "1555 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.737106]\n",
      "1556 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.813915]\n",
      "1557 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 39.835983]\n",
      "1558 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.042015]\n",
      "1559 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.359558]\n",
      "1560 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 43.554436]\n",
      "1561 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.411514]\n",
      "1562 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 44.659412]\n",
      "1563 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.441132]\n",
      "1564 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.454407]\n",
      "1565 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 42.775002]\n",
      "1566 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.784210]\n",
      "1567 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.510040]\n",
      "1568 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 50.510658]\n",
      "1569 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.591309]\n",
      "1570 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 39.671841]\n",
      "1571 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.956638]\n",
      "1572 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.894039]\n",
      "1573 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 40.269463]\n",
      "1574 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 42.901684]\n",
      "1575 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.270409]\n",
      "1576 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 40.509392]\n",
      "1577 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 39.419384]\n",
      "1578 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.607864]\n",
      "1579 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 39.574478]\n",
      "1580 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 44.955803]\n",
      "1581 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 43.770645]\n",
      "1582 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 40.521881]\n",
      "1583 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 45.290230]\n",
      "1584 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.001167]\n",
      "1585 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.227127]\n",
      "1586 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.880638]\n",
      "1587 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.328506]\n",
      "1588 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.898548]\n",
      "1589 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.260925]\n",
      "1590 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.558510]\n",
      "1591 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.642189]\n",
      "1592 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 43.725113]\n",
      "1593 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 41.587044]\n",
      "1594 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 43.544212]\n",
      "1595 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.021645]\n",
      "1596 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.075150]\n",
      "1597 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.384686]\n",
      "1598 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.157776]\n",
      "1599 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.709393]\n",
      "1600 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.110657]\n",
      "1601 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 39.849495]\n",
      "1602 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.638088]\n",
      "1603 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.067879]\n",
      "1604 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.825466]\n",
      "1605 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.220047]\n",
      "1606 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 41.571690]\n",
      "1607 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 45.919415]\n",
      "1608 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.282944]\n",
      "1609 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 44.936615]\n",
      "1610 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.059258]\n",
      "1611 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 43.642647]\n",
      "1612 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 41.994083]\n",
      "1613 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 44.742653]\n",
      "1614 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 41.298691]\n",
      "1615 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.649750]\n",
      "1616 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.791023]\n",
      "1617 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.350266]\n",
      "1618 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.147964]\n",
      "1619 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.312645]\n",
      "1620 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.563454]\n",
      "1621 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.838650]\n",
      "1622 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.022350]\n",
      "1623 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.383820]\n",
      "1624 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.563469]\n",
      "1625 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.529076]\n",
      "1626 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.681190]\n",
      "1627 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.059998]\n",
      "1628 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 41.732674]\n",
      "1629 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 46.091675]\n",
      "1630 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.388908]\n",
      "1631 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 40.197647]\n",
      "1632 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.777245]\n",
      "1633 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.648170]\n",
      "1634 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 43.040199]\n",
      "1635 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.779022]\n",
      "1636 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 42.408035]\n",
      "1637 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 39.890034]\n",
      "1638 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.213669]\n",
      "1639 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.693924]\n",
      "1640 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 43.159935]\n",
      "1641 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.222431]\n",
      "1642 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.312729]\n",
      "1643 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 43.732834]\n",
      "1644 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 43.124016]\n",
      "1645 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 46.165230]\n",
      "1646 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.942276]\n",
      "1647 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.288948]\n",
      "1648 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.289284]\n",
      "1649 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.856018]\n",
      "1650 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.817780]\n",
      "1651 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.479862]\n",
      "1652 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 43.226791]\n",
      "1653 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.401627]\n",
      "1654 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 41.854073]\n",
      "1655 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.182549]\n",
      "1656 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 38.974678]\n",
      "1657 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.079933]\n",
      "1658 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.700790]\n",
      "1659 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 42.281204]\n",
      "1660 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 39.800583]\n",
      "1661 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 42.465843]\n",
      "1662 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 41.060070]\n",
      "1663 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.061226]\n",
      "1664 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.109711]\n",
      "1665 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.003300]\n",
      "1666 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.270782]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1667 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 43.559555]\n",
      "1668 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 42.197517]\n",
      "1669 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.391106]\n",
      "1670 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 41.502930]\n",
      "1671 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.996590]\n",
      "1672 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.241615]\n",
      "1673 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 42.945618]\n",
      "1674 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.588741]\n",
      "1675 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 41.069603]\n",
      "1676 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.793938]\n",
      "1677 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 41.985825]\n",
      "1678 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 45.848518]\n",
      "1679 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.428421]\n",
      "1680 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.836441]\n",
      "1681 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 44.780403]\n",
      "1682 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.026901]\n",
      "1683 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 37.284420]\n",
      "1684 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 39.628609]\n",
      "1685 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 40.573143]\n",
      "1686 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.775639]\n",
      "1687 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.092987]\n",
      "1688 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 38.977158]\n",
      "1689 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.741623]\n",
      "1690 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 39.617695]\n",
      "1691 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.364693]\n",
      "1692 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 43.143440]\n",
      "1693 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 40.949448]\n",
      "1694 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 42.462624]\n",
      "1695 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.608536]\n",
      "1696 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 41.170948]\n",
      "1697 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 38.030014]\n",
      "1698 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.085411]\n",
      "1699 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 40.206657]\n",
      "1700 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.766720]\n",
      "1701 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.821846]\n",
      "1702 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 42.382397]\n",
      "1703 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.738346]\n",
      "1704 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.583084]\n",
      "1705 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.568089]\n",
      "1706 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.960869]\n",
      "1707 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 42.720154]\n",
      "1708 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.158691]\n",
      "1709 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.665260]\n",
      "1710 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 46.998772]\n",
      "1711 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.745003]\n",
      "1712 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.094505]\n",
      "1713 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.068153]\n",
      "1714 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.313992]\n",
      "1715 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 36.270138]\n",
      "1716 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 49.369980]\n",
      "1717 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 40.389931]\n",
      "1718 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 38.780849]\n",
      "1719 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.132076]\n",
      "1720 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.821846]\n",
      "1721 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.039543]\n",
      "1722 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.629879]\n",
      "1723 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.125023]\n",
      "1724 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 41.413921]\n",
      "1725 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.320324]\n",
      "1726 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 40.027679]\n",
      "1727 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.818085]\n",
      "1728 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.575371]\n",
      "1729 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.127098]\n",
      "1730 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.832790]\n",
      "1731 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 38.953461]\n",
      "1732 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 38.901421]\n",
      "1733 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 41.692619]\n",
      "1734 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 41.506882]\n",
      "1735 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.754417]\n",
      "1736 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.413544]\n",
      "1737 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.105652]\n",
      "1738 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.176003]\n",
      "1739 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 39.878727]\n",
      "1740 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.309128]\n",
      "1741 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.562103]\n",
      "1742 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 43.919552]\n",
      "1743 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 40.650314]\n"
     ]
    }
   ],
   "source": [
    "# epochs=30000\n",
    "epochs=5000\n",
    "train(D_A, D_B, G_AB, G_BA, combined,\n",
    "      img_dim, img_rows, img_cols,\n",
    "      epochs=epochs, batch_size=32, sample_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
