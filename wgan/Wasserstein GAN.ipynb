{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wasserstein GAN\n",
    "\n",
    "ref.:\n",
    "ARJOVSKY, Martin; CHINTALA, Soumith; BOTTOU, LÃ©on.  \n",
    "Wasserstein GAN.  \n",
    "arXiv preprint arXiv:1701.07875, 2017.\n",
    "\n",
    "![Comparing equations](equations.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import RMSprop\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(latent_dim, channels):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=latent_dim))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128, kernel_size=4, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(64, kernel_size=4, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv2D(channels, kernel_size=4, padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "    img = model(noise)\n",
    "\n",
    "    return Model(noise, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_critic(img_shape):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=img_shape, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "    validity = model(img)\n",
    "\n",
    "    return Model(img, validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(G, latent_dim, epoch):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
    "    gen_imgs = G.predict(noise)\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(G, critic, combined, n_critic, latent_dim, clip_value, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "    # Load the dataset\n",
    "    (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "    # Rescale -1 to 1\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "    # Adversarial ground truths\n",
    "    valid = -np.ones((batch_size, 1))\n",
    "    fake = np.ones((batch_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for _ in range(n_critic):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # Sample noise as generator input\n",
    "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = G.predict(noise)\n",
    "\n",
    "            # Train the critic\n",
    "            d_loss_real = critic.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = critic.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
    "\n",
    "            # Clip critic weights\n",
    "            for l in critic.layers:\n",
    "                weights = l.get_weights()\n",
    "                weights = [np.clip(w, -clip_value, clip_value) for w in weights]\n",
    "                l.set_weights(weights)\n",
    "\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "\n",
    "        g_loss = combined.train_on_batch(noise, valid)\n",
    "\n",
    "        # Plot the progress\n",
    "        print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, 1 - d_loss[0], 1 - g_loss[0]))\n",
    "\n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % sample_interval == 0 or epoch == epochs - 1:\n",
    "            sample_images(G, latent_dim, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('images'):\n",
    "    os.makedirs('images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following parameter and optimizer set as recommended in paper\n",
    "n_critic = 5\n",
    "clip_value = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# create optimizer\n",
    "optimizer = RMSprop(lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 16)        160       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 32)          4640      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 100,097\n",
      "Trainable params: 99,649\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build and compile the critic\n",
    "critic = build_critic(img_shape)\n",
    "critic.compile(loss=wasserstein_loss,\n",
    "               optimizer=optimizer,\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 64)        131136    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 1)         1025      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,028,673\n",
      "Trainable params: 1,028,289\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the generator\n",
    "G = build_generator(latent_dim, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generator takes noise as input and generated imgs\n",
    "z = Input(shape=(latent_dim,))\n",
    "img = G(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the combined model we will only train the generator\n",
    "critic.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The critic takes generated images as input and determines validity\n",
    "valid = critic(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The combined model  (stacked generator and critic)\n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss=wasserstein_loss,\n",
    "                 optimizer=optimizer,\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.999930] [G loss: 1.000140]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\henri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 0.999940] [G loss: 1.000139]\n",
      "2 [D loss: 0.999936] [G loss: 1.000137]\n",
      "3 [D loss: 0.999940] [G loss: 1.000130]\n",
      "4 [D loss: 0.999935] [G loss: 1.000134]\n",
      "5 [D loss: 0.999939] [G loss: 1.000140]\n",
      "6 [D loss: 0.999938] [G loss: 1.000126]\n",
      "7 [D loss: 0.999937] [G loss: 1.000127]\n",
      "8 [D loss: 0.999940] [G loss: 1.000131]\n",
      "9 [D loss: 0.999938] [G loss: 1.000130]\n",
      "10 [D loss: 0.999937] [G loss: 1.000122]\n",
      "11 [D loss: 0.999937] [G loss: 1.000124]\n",
      "12 [D loss: 0.999943] [G loss: 1.000124]\n",
      "13 [D loss: 0.999942] [G loss: 1.000119]\n",
      "14 [D loss: 0.999940] [G loss: 1.000117]\n",
      "15 [D loss: 0.999946] [G loss: 1.000115]\n",
      "16 [D loss: 0.999948] [G loss: 1.000118]\n",
      "17 [D loss: 0.999946] [G loss: 1.000111]\n",
      "18 [D loss: 0.999950] [G loss: 1.000109]\n",
      "19 [D loss: 0.999948] [G loss: 1.000109]\n",
      "20 [D loss: 0.999948] [G loss: 1.000102]\n",
      "21 [D loss: 0.999957] [G loss: 1.000100]\n",
      "22 [D loss: 0.999955] [G loss: 1.000095]\n",
      "23 [D loss: 0.999953] [G loss: 1.000095]\n",
      "24 [D loss: 0.999959] [G loss: 1.000092]\n",
      "25 [D loss: 0.999956] [G loss: 1.000090]\n",
      "26 [D loss: 0.999960] [G loss: 1.000078]\n",
      "27 [D loss: 0.999957] [G loss: 1.000080]\n",
      "28 [D loss: 0.999960] [G loss: 1.000080]\n",
      "29 [D loss: 0.999965] [G loss: 1.000079]\n",
      "30 [D loss: 0.999960] [G loss: 1.000075]\n",
      "31 [D loss: 0.999967] [G loss: 1.000073]\n",
      "32 [D loss: 0.999966] [G loss: 1.000072]\n",
      "33 [D loss: 0.999962] [G loss: 1.000081]\n",
      "34 [D loss: 0.999966] [G loss: 1.000081]\n",
      "35 [D loss: 0.999975] [G loss: 1.000077]\n",
      "36 [D loss: 0.999964] [G loss: 1.000081]\n",
      "37 [D loss: 0.999975] [G loss: 1.000069]\n",
      "38 [D loss: 0.999960] [G loss: 1.000065]\n",
      "39 [D loss: 0.999963] [G loss: 1.000065]\n",
      "40 [D loss: 0.999964] [G loss: 1.000062]\n",
      "41 [D loss: 0.999971] [G loss: 1.000063]\n",
      "42 [D loss: 0.999964] [G loss: 1.000065]\n",
      "43 [D loss: 0.999966] [G loss: 1.000070]\n",
      "44 [D loss: 0.999968] [G loss: 1.000066]\n",
      "45 [D loss: 0.999971] [G loss: 1.000072]\n",
      "46 [D loss: 0.999970] [G loss: 1.000068]\n",
      "47 [D loss: 0.999962] [G loss: 1.000075]\n",
      "48 [D loss: 0.999973] [G loss: 1.000057]\n",
      "49 [D loss: 0.999972] [G loss: 1.000068]\n",
      "50 [D loss: 0.999974] [G loss: 1.000069]\n",
      "51 [D loss: 0.999968] [G loss: 1.000063]\n",
      "52 [D loss: 0.999975] [G loss: 1.000064]\n",
      "53 [D loss: 0.999974] [G loss: 1.000060]\n",
      "54 [D loss: 0.999979] [G loss: 1.000070]\n",
      "55 [D loss: 0.999983] [G loss: 1.000074]\n",
      "56 [D loss: 0.999972] [G loss: 1.000067]\n",
      "57 [D loss: 0.999979] [G loss: 1.000059]\n",
      "58 [D loss: 0.999979] [G loss: 1.000059]\n",
      "59 [D loss: 0.999987] [G loss: 1.000070]\n",
      "60 [D loss: 0.999979] [G loss: 1.000078]\n",
      "61 [D loss: 0.999986] [G loss: 1.000082]\n",
      "62 [D loss: 0.999991] [G loss: 1.000063]\n",
      "63 [D loss: 0.999989] [G loss: 1.000089]\n",
      "64 [D loss: 0.999991] [G loss: 1.000079]\n",
      "65 [D loss: 0.999981] [G loss: 1.000082]\n",
      "66 [D loss: 0.999985] [G loss: 1.000092]\n",
      "67 [D loss: 0.999976] [G loss: 1.000091]\n",
      "68 [D loss: 0.999987] [G loss: 1.000101]\n",
      "69 [D loss: 0.999989] [G loss: 1.000098]\n",
      "70 [D loss: 0.999978] [G loss: 1.000106]\n",
      "71 [D loss: 0.999990] [G loss: 1.000076]\n",
      "72 [D loss: 0.999986] [G loss: 1.000084]\n",
      "73 [D loss: 1.000001] [G loss: 1.000095]\n",
      "74 [D loss: 1.000004] [G loss: 1.000099]\n",
      "75 [D loss: 0.999979] [G loss: 1.000128]\n",
      "76 [D loss: 0.999991] [G loss: 1.000113]\n",
      "77 [D loss: 0.999983] [G loss: 1.000119]\n",
      "78 [D loss: 0.999986] [G loss: 1.000125]\n",
      "79 [D loss: 0.999961] [G loss: 1.000153]\n",
      "80 [D loss: 0.999983] [G loss: 1.000126]\n",
      "81 [D loss: 0.999973] [G loss: 1.000152]\n",
      "82 [D loss: 0.999983] [G loss: 1.000137]\n",
      "83 [D loss: 0.999967] [G loss: 1.000137]\n",
      "84 [D loss: 0.999987] [G loss: 1.000136]\n",
      "85 [D loss: 0.999978] [G loss: 1.000130]\n",
      "86 [D loss: 0.999968] [G loss: 1.000130]\n",
      "87 [D loss: 0.999963] [G loss: 1.000139]\n",
      "88 [D loss: 0.999959] [G loss: 1.000146]\n",
      "89 [D loss: 0.999977] [G loss: 1.000159]\n",
      "90 [D loss: 0.999980] [G loss: 1.000147]\n",
      "91 [D loss: 0.999958] [G loss: 1.000172]\n",
      "92 [D loss: 0.999970] [G loss: 1.000151]\n",
      "93 [D loss: 0.999968] [G loss: 1.000148]\n",
      "94 [D loss: 0.999958] [G loss: 1.000161]\n",
      "95 [D loss: 0.999973] [G loss: 1.000150]\n",
      "96 [D loss: 0.999974] [G loss: 1.000154]\n",
      "97 [D loss: 0.999959] [G loss: 1.000145]\n",
      "98 [D loss: 0.999958] [G loss: 1.000151]\n",
      "99 [D loss: 0.999962] [G loss: 1.000148]\n",
      "100 [D loss: 0.999963] [G loss: 1.000127]\n",
      "101 [D loss: 0.999956] [G loss: 1.000134]\n",
      "102 [D loss: 0.999967] [G loss: 1.000123]\n",
      "103 [D loss: 0.999969] [G loss: 1.000129]\n",
      "104 [D loss: 0.999969] [G loss: 1.000109]\n",
      "105 [D loss: 0.999971] [G loss: 1.000128]\n",
      "106 [D loss: 0.999968] [G loss: 1.000115]\n",
      "107 [D loss: 0.999960] [G loss: 1.000117]\n",
      "108 [D loss: 0.999962] [G loss: 1.000105]\n",
      "109 [D loss: 0.999963] [G loss: 1.000103]\n",
      "110 [D loss: 0.999967] [G loss: 1.000120]\n",
      "111 [D loss: 0.999961] [G loss: 1.000091]\n",
      "112 [D loss: 0.999970] [G loss: 1.000102]\n",
      "113 [D loss: 0.999970] [G loss: 1.000110]\n",
      "114 [D loss: 0.999979] [G loss: 1.000097]\n",
      "115 [D loss: 0.999968] [G loss: 1.000096]\n",
      "116 [D loss: 0.999972] [G loss: 1.000094]\n",
      "117 [D loss: 0.999970] [G loss: 1.000075]\n",
      "118 [D loss: 0.999963] [G loss: 1.000102]\n",
      "119 [D loss: 0.999956] [G loss: 1.000084]\n",
      "120 [D loss: 0.999973] [G loss: 1.000089]\n",
      "121 [D loss: 0.999974] [G loss: 1.000090]\n",
      "122 [D loss: 0.999979] [G loss: 1.000093]\n",
      "123 [D loss: 0.999966] [G loss: 1.000086]\n",
      "124 [D loss: 0.999966] [G loss: 1.000079]\n",
      "125 [D loss: 0.999974] [G loss: 1.000083]\n",
      "126 [D loss: 0.999972] [G loss: 1.000092]\n",
      "127 [D loss: 0.999972] [G loss: 1.000087]\n",
      "128 [D loss: 0.999974] [G loss: 1.000088]\n",
      "129 [D loss: 0.999972] [G loss: 1.000065]\n",
      "130 [D loss: 0.999959] [G loss: 1.000089]\n",
      "131 [D loss: 0.999971] [G loss: 1.000095]\n",
      "132 [D loss: 0.999972] [G loss: 1.000089]\n",
      "133 [D loss: 0.999969] [G loss: 1.000086]\n",
      "134 [D loss: 0.999966] [G loss: 1.000080]\n",
      "135 [D loss: 0.999966] [G loss: 1.000091]\n",
      "136 [D loss: 0.999961] [G loss: 1.000083]\n",
      "137 [D loss: 0.999969] [G loss: 1.000086]\n",
      "138 [D loss: 0.999972] [G loss: 1.000074]\n",
      "139 [D loss: 0.999971] [G loss: 1.000080]\n",
      "140 [D loss: 0.999971] [G loss: 1.000080]\n",
      "141 [D loss: 0.999964] [G loss: 1.000087]\n",
      "142 [D loss: 0.999971] [G loss: 1.000072]\n",
      "143 [D loss: 0.999967] [G loss: 1.000071]\n",
      "144 [D loss: 0.999979] [G loss: 1.000064]\n",
      "145 [D loss: 0.999970] [G loss: 1.000070]\n",
      "146 [D loss: 0.999964] [G loss: 1.000082]\n",
      "147 [D loss: 0.999971] [G loss: 1.000078]\n",
      "148 [D loss: 0.999962] [G loss: 1.000079]\n",
      "149 [D loss: 0.999983] [G loss: 1.000063]\n",
      "150 [D loss: 0.999978] [G loss: 1.000070]\n",
      "151 [D loss: 0.999965] [G loss: 1.000064]\n",
      "152 [D loss: 0.999967] [G loss: 1.000073]\n",
      "153 [D loss: 0.999968] [G loss: 1.000087]\n",
      "154 [D loss: 0.999978] [G loss: 1.000081]\n",
      "155 [D loss: 0.999970] [G loss: 1.000061]\n",
      "156 [D loss: 0.999969] [G loss: 1.000073]\n",
      "157 [D loss: 0.999981] [G loss: 1.000073]\n",
      "158 [D loss: 0.999970] [G loss: 1.000090]\n",
      "159 [D loss: 0.999960] [G loss: 1.000084]\n",
      "160 [D loss: 0.999960] [G loss: 1.000082]\n",
      "161 [D loss: 0.999972] [G loss: 1.000070]\n",
      "162 [D loss: 0.999967] [G loss: 1.000060]\n",
      "163 [D loss: 0.999968] [G loss: 1.000077]\n",
      "164 [D loss: 0.999968] [G loss: 1.000083]\n",
      "165 [D loss: 0.999963] [G loss: 1.000081]\n",
      "166 [D loss: 0.999968] [G loss: 1.000076]\n",
      "167 [D loss: 0.999972] [G loss: 1.000057]\n",
      "168 [D loss: 0.999970] [G loss: 1.000070]\n",
      "169 [D loss: 0.999968] [G loss: 1.000065]\n",
      "170 [D loss: 0.999968] [G loss: 1.000062]\n",
      "171 [D loss: 0.999977] [G loss: 1.000082]\n",
      "172 [D loss: 0.999968] [G loss: 1.000058]\n",
      "173 [D loss: 0.999967] [G loss: 1.000069]\n",
      "174 [D loss: 0.999970] [G loss: 1.000075]\n",
      "175 [D loss: 0.999965] [G loss: 1.000073]\n",
      "176 [D loss: 0.999966] [G loss: 1.000076]\n",
      "177 [D loss: 0.999967] [G loss: 1.000077]\n",
      "178 [D loss: 0.999973] [G loss: 1.000071]\n",
      "179 [D loss: 0.999973] [G loss: 1.000067]\n",
      "180 [D loss: 0.999970] [G loss: 1.000075]\n",
      "181 [D loss: 0.999964] [G loss: 1.000066]\n",
      "182 [D loss: 0.999973] [G loss: 1.000071]\n",
      "183 [D loss: 0.999966] [G loss: 1.000067]\n",
      "184 [D loss: 0.999978] [G loss: 1.000078]\n",
      "185 [D loss: 0.999969] [G loss: 1.000081]\n",
      "186 [D loss: 0.999966] [G loss: 1.000065]\n",
      "187 [D loss: 0.999970] [G loss: 1.000073]\n",
      "188 [D loss: 0.999962] [G loss: 1.000073]\n",
      "189 [D loss: 0.999982] [G loss: 1.000073]\n",
      "190 [D loss: 0.999972] [G loss: 1.000066]\n",
      "191 [D loss: 0.999966] [G loss: 1.000072]\n",
      "192 [D loss: 0.999970] [G loss: 1.000073]\n",
      "193 [D loss: 0.999972] [G loss: 1.000082]\n",
      "194 [D loss: 0.999967] [G loss: 1.000079]\n",
      "195 [D loss: 0.999969] [G loss: 1.000064]\n",
      "196 [D loss: 0.999970] [G loss: 1.000058]\n",
      "197 [D loss: 0.999970] [G loss: 1.000063]\n",
      "198 [D loss: 0.999964] [G loss: 1.000070]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199 [D loss: 0.999968] [G loss: 1.000077]\n",
      "200 [D loss: 0.999974] [G loss: 1.000064]\n",
      "201 [D loss: 0.999961] [G loss: 1.000057]\n",
      "202 [D loss: 0.999962] [G loss: 1.000066]\n",
      "203 [D loss: 0.999967] [G loss: 1.000068]\n",
      "204 [D loss: 0.999968] [G loss: 1.000070]\n",
      "205 [D loss: 0.999969] [G loss: 1.000070]\n",
      "206 [D loss: 0.999976] [G loss: 1.000064]\n",
      "207 [D loss: 0.999969] [G loss: 1.000070]\n",
      "208 [D loss: 0.999962] [G loss: 1.000071]\n",
      "209 [D loss: 0.999971] [G loss: 1.000052]\n",
      "210 [D loss: 0.999974] [G loss: 1.000057]\n",
      "211 [D loss: 0.999968] [G loss: 1.000056]\n",
      "212 [D loss: 0.999968] [G loss: 1.000077]\n",
      "213 [D loss: 0.999960] [G loss: 1.000064]\n",
      "214 [D loss: 0.999967] [G loss: 1.000068]\n",
      "215 [D loss: 0.999969] [G loss: 1.000076]\n",
      "216 [D loss: 0.999971] [G loss: 1.000065]\n",
      "217 [D loss: 0.999970] [G loss: 1.000060]\n",
      "218 [D loss: 0.999971] [G loss: 1.000071]\n",
      "219 [D loss: 0.999978] [G loss: 1.000061]\n",
      "220 [D loss: 0.999958] [G loss: 1.000079]\n",
      "221 [D loss: 0.999974] [G loss: 1.000091]\n",
      "222 [D loss: 0.999979] [G loss: 1.000065]\n",
      "223 [D loss: 0.999968] [G loss: 1.000069]\n",
      "224 [D loss: 0.999962] [G loss: 1.000071]\n",
      "225 [D loss: 0.999965] [G loss: 1.000062]\n",
      "226 [D loss: 0.999981] [G loss: 1.000071]\n",
      "227 [D loss: 0.999963] [G loss: 1.000079]\n",
      "228 [D loss: 0.999974] [G loss: 1.000096]\n",
      "229 [D loss: 0.999986] [G loss: 1.000079]\n",
      "230 [D loss: 0.999973] [G loss: 1.000066]\n",
      "231 [D loss: 0.999974] [G loss: 1.000067]\n",
      "232 [D loss: 0.999974] [G loss: 1.000075]\n",
      "233 [D loss: 0.999969] [G loss: 1.000063]\n",
      "234 [D loss: 0.999968] [G loss: 1.000060]\n",
      "235 [D loss: 0.999976] [G loss: 1.000068]\n",
      "236 [D loss: 0.999974] [G loss: 1.000094]\n",
      "237 [D loss: 0.999976] [G loss: 1.000060]\n",
      "238 [D loss: 0.999978] [G loss: 1.000071]\n",
      "239 [D loss: 0.999969] [G loss: 1.000062]\n",
      "240 [D loss: 0.999976] [G loss: 1.000068]\n",
      "241 [D loss: 0.999973] [G loss: 1.000073]\n",
      "242 [D loss: 0.999980] [G loss: 1.000056]\n",
      "243 [D loss: 0.999972] [G loss: 1.000071]\n",
      "244 [D loss: 0.999965] [G loss: 1.000064]\n",
      "245 [D loss: 0.999964] [G loss: 1.000085]\n",
      "246 [D loss: 0.999963] [G loss: 1.000087]\n",
      "247 [D loss: 0.999965] [G loss: 1.000085]\n",
      "248 [D loss: 0.999972] [G loss: 1.000047]\n",
      "249 [D loss: 0.999966] [G loss: 1.000072]\n",
      "250 [D loss: 0.999976] [G loss: 1.000066]\n",
      "251 [D loss: 0.999968] [G loss: 1.000072]\n",
      "252 [D loss: 0.999964] [G loss: 1.000061]\n",
      "253 [D loss: 0.999973] [G loss: 1.000076]\n",
      "254 [D loss: 0.999973] [G loss: 1.000071]\n",
      "255 [D loss: 0.999967] [G loss: 1.000072]\n",
      "256 [D loss: 0.999968] [G loss: 1.000064]\n",
      "257 [D loss: 0.999967] [G loss: 1.000061]\n",
      "258 [D loss: 0.999977] [G loss: 1.000066]\n",
      "259 [D loss: 0.999967] [G loss: 1.000072]\n",
      "260 [D loss: 0.999973] [G loss: 1.000069]\n",
      "261 [D loss: 0.999956] [G loss: 1.000068]\n",
      "262 [D loss: 0.999967] [G loss: 1.000065]\n",
      "263 [D loss: 0.999971] [G loss: 1.000068]\n",
      "264 [D loss: 0.999962] [G loss: 1.000073]\n",
      "265 [D loss: 0.999968] [G loss: 1.000069]\n",
      "266 [D loss: 0.999964] [G loss: 1.000069]\n",
      "267 [D loss: 0.999968] [G loss: 1.000074]\n",
      "268 [D loss: 0.999974] [G loss: 1.000086]\n",
      "269 [D loss: 0.999974] [G loss: 1.000076]\n",
      "270 [D loss: 0.999969] [G loss: 1.000055]\n",
      "271 [D loss: 0.999978] [G loss: 1.000062]\n",
      "272 [D loss: 0.999971] [G loss: 1.000070]\n",
      "273 [D loss: 0.999971] [G loss: 1.000079]\n",
      "274 [D loss: 0.999965] [G loss: 1.000069]\n",
      "275 [D loss: 0.999977] [G loss: 1.000068]\n",
      "276 [D loss: 0.999968] [G loss: 1.000068]\n",
      "277 [D loss: 0.999970] [G loss: 1.000076]\n",
      "278 [D loss: 0.999979] [G loss: 1.000065]\n",
      "279 [D loss: 0.999974] [G loss: 1.000062]\n",
      "280 [D loss: 0.999971] [G loss: 1.000067]\n",
      "281 [D loss: 0.999968] [G loss: 1.000066]\n",
      "282 [D loss: 0.999968] [G loss: 1.000066]\n",
      "283 [D loss: 0.999970] [G loss: 1.000066]\n",
      "284 [D loss: 0.999970] [G loss: 1.000063]\n",
      "285 [D loss: 0.999973] [G loss: 1.000068]\n",
      "286 [D loss: 0.999965] [G loss: 1.000061]\n",
      "287 [D loss: 0.999969] [G loss: 1.000065]\n",
      "288 [D loss: 0.999968] [G loss: 1.000075]\n",
      "289 [D loss: 0.999972] [G loss: 1.000066]\n",
      "290 [D loss: 0.999971] [G loss: 1.000081]\n",
      "291 [D loss: 0.999967] [G loss: 1.000067]\n",
      "292 [D loss: 0.999971] [G loss: 1.000063]\n",
      "293 [D loss: 0.999969] [G loss: 1.000066]\n",
      "294 [D loss: 0.999977] [G loss: 1.000072]\n",
      "295 [D loss: 0.999964] [G loss: 1.000048]\n",
      "296 [D loss: 0.999970] [G loss: 1.000074]\n",
      "297 [D loss: 0.999972] [G loss: 1.000079]\n",
      "298 [D loss: 0.999970] [G loss: 1.000061]\n",
      "299 [D loss: 0.999969] [G loss: 1.000082]\n",
      "300 [D loss: 0.999974] [G loss: 1.000053]\n",
      "301 [D loss: 0.999960] [G loss: 1.000066]\n",
      "302 [D loss: 0.999973] [G loss: 1.000043]\n",
      "303 [D loss: 0.999972] [G loss: 1.000084]\n",
      "304 [D loss: 0.999968] [G loss: 1.000068]\n",
      "305 [D loss: 0.999971] [G loss: 1.000063]\n",
      "306 [D loss: 0.999976] [G loss: 1.000071]\n",
      "307 [D loss: 0.999961] [G loss: 1.000066]\n",
      "308 [D loss: 0.999965] [G loss: 1.000074]\n",
      "309 [D loss: 0.999971] [G loss: 1.000046]\n",
      "310 [D loss: 0.999962] [G loss: 1.000059]\n",
      "311 [D loss: 0.999972] [G loss: 1.000048]\n",
      "312 [D loss: 0.999960] [G loss: 1.000072]\n",
      "313 [D loss: 0.999974] [G loss: 1.000051]\n",
      "314 [D loss: 0.999969] [G loss: 1.000041]\n",
      "315 [D loss: 0.999971] [G loss: 1.000073]\n",
      "316 [D loss: 0.999963] [G loss: 1.000070]\n",
      "317 [D loss: 0.999978] [G loss: 1.000056]\n",
      "318 [D loss: 0.999969] [G loss: 1.000050]\n",
      "319 [D loss: 0.999966] [G loss: 1.000073]\n",
      "320 [D loss: 0.999965] [G loss: 1.000059]\n",
      "321 [D loss: 0.999970] [G loss: 1.000063]\n",
      "322 [D loss: 0.999963] [G loss: 1.000063]\n",
      "323 [D loss: 0.999969] [G loss: 1.000054]\n",
      "324 [D loss: 0.999969] [G loss: 1.000082]\n",
      "325 [D loss: 0.999971] [G loss: 1.000065]\n",
      "326 [D loss: 0.999978] [G loss: 1.000057]\n",
      "327 [D loss: 0.999979] [G loss: 1.000067]\n",
      "328 [D loss: 0.999973] [G loss: 1.000063]\n",
      "329 [D loss: 0.999974] [G loss: 1.000067]\n",
      "330 [D loss: 0.999967] [G loss: 1.000074]\n",
      "331 [D loss: 0.999976] [G loss: 1.000063]\n",
      "332 [D loss: 0.999965] [G loss: 1.000055]\n",
      "333 [D loss: 0.999973] [G loss: 1.000068]\n",
      "334 [D loss: 0.999972] [G loss: 1.000064]\n",
      "335 [D loss: 0.999967] [G loss: 1.000056]\n",
      "336 [D loss: 0.999971] [G loss: 1.000065]\n",
      "337 [D loss: 0.999972] [G loss: 1.000055]\n",
      "338 [D loss: 0.999970] [G loss: 1.000077]\n",
      "339 [D loss: 0.999968] [G loss: 1.000071]\n",
      "340 [D loss: 0.999970] [G loss: 1.000064]\n",
      "341 [D loss: 0.999990] [G loss: 1.000069]\n",
      "342 [D loss: 0.999970] [G loss: 1.000054]\n",
      "343 [D loss: 0.999970] [G loss: 1.000064]\n",
      "344 [D loss: 0.999979] [G loss: 1.000068]\n",
      "345 [D loss: 0.999972] [G loss: 1.000072]\n",
      "346 [D loss: 0.999967] [G loss: 1.000059]\n",
      "347 [D loss: 0.999974] [G loss: 1.000056]\n",
      "348 [D loss: 0.999974] [G loss: 1.000065]\n",
      "349 [D loss: 0.999964] [G loss: 1.000070]\n",
      "350 [D loss: 0.999967] [G loss: 1.000076]\n",
      "351 [D loss: 0.999968] [G loss: 1.000059]\n",
      "352 [D loss: 0.999976] [G loss: 1.000055]\n",
      "353 [D loss: 0.999967] [G loss: 1.000065]\n",
      "354 [D loss: 0.999974] [G loss: 1.000057]\n",
      "355 [D loss: 0.999969] [G loss: 1.000061]\n",
      "356 [D loss: 0.999975] [G loss: 1.000061]\n",
      "357 [D loss: 0.999973] [G loss: 1.000068]\n",
      "358 [D loss: 0.999970] [G loss: 1.000074]\n",
      "359 [D loss: 0.999970] [G loss: 1.000068]\n",
      "360 [D loss: 0.999970] [G loss: 1.000062]\n",
      "361 [D loss: 0.999959] [G loss: 1.000063]\n",
      "362 [D loss: 0.999959] [G loss: 1.000053]\n",
      "363 [D loss: 0.999972] [G loss: 1.000060]\n",
      "364 [D loss: 0.999969] [G loss: 1.000051]\n",
      "365 [D loss: 0.999970] [G loss: 1.000077]\n",
      "366 [D loss: 0.999959] [G loss: 1.000069]\n",
      "367 [D loss: 0.999961] [G loss: 1.000067]\n",
      "368 [D loss: 0.999964] [G loss: 1.000069]\n",
      "369 [D loss: 0.999970] [G loss: 1.000064]\n",
      "370 [D loss: 0.999970] [G loss: 1.000071]\n",
      "371 [D loss: 0.999973] [G loss: 1.000078]\n",
      "372 [D loss: 0.999972] [G loss: 1.000066]\n",
      "373 [D loss: 0.999968] [G loss: 1.000074]\n",
      "374 [D loss: 0.999963] [G loss: 1.000065]\n",
      "375 [D loss: 0.999972] [G loss: 1.000068]\n",
      "376 [D loss: 0.999959] [G loss: 1.000068]\n",
      "377 [D loss: 0.999969] [G loss: 1.000060]\n",
      "378 [D loss: 0.999968] [G loss: 1.000060]\n",
      "379 [D loss: 0.999975] [G loss: 1.000065]\n",
      "380 [D loss: 0.999978] [G loss: 1.000060]\n",
      "381 [D loss: 0.999971] [G loss: 1.000060]\n",
      "382 [D loss: 0.999960] [G loss: 1.000061]\n",
      "383 [D loss: 0.999976] [G loss: 1.000066]\n",
      "384 [D loss: 0.999973] [G loss: 1.000067]\n",
      "385 [D loss: 0.999963] [G loss: 1.000071]\n",
      "386 [D loss: 0.999970] [G loss: 1.000052]\n",
      "387 [D loss: 0.999979] [G loss: 1.000072]\n",
      "388 [D loss: 0.999974] [G loss: 1.000071]\n",
      "389 [D loss: 0.999967] [G loss: 1.000070]\n",
      "390 [D loss: 0.999976] [G loss: 1.000064]\n",
      "391 [D loss: 0.999966] [G loss: 1.000068]\n",
      "392 [D loss: 0.999967] [G loss: 1.000067]\n",
      "393 [D loss: 0.999974] [G loss: 1.000053]\n",
      "394 [D loss: 0.999977] [G loss: 1.000071]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395 [D loss: 0.999969] [G loss: 1.000058]\n",
      "396 [D loss: 0.999976] [G loss: 1.000054]\n",
      "397 [D loss: 0.999963] [G loss: 1.000064]\n",
      "398 [D loss: 0.999979] [G loss: 1.000071]\n",
      "399 [D loss: 0.999955] [G loss: 1.000065]\n",
      "400 [D loss: 0.999973] [G loss: 1.000071]\n",
      "401 [D loss: 0.999966] [G loss: 1.000064]\n",
      "402 [D loss: 0.999968] [G loss: 1.000068]\n",
      "403 [D loss: 0.999968] [G loss: 1.000063]\n",
      "404 [D loss: 0.999976] [G loss: 1.000062]\n",
      "405 [D loss: 0.999962] [G loss: 1.000075]\n",
      "406 [D loss: 0.999975] [G loss: 1.000065]\n",
      "407 [D loss: 0.999969] [G loss: 1.000061]\n",
      "408 [D loss: 0.999973] [G loss: 1.000062]\n",
      "409 [D loss: 0.999967] [G loss: 1.000068]\n",
      "410 [D loss: 0.999974] [G loss: 1.000065]\n",
      "411 [D loss: 0.999965] [G loss: 1.000054]\n",
      "412 [D loss: 0.999978] [G loss: 1.000064]\n",
      "413 [D loss: 0.999969] [G loss: 1.000068]\n",
      "414 [D loss: 0.999965] [G loss: 1.000062]\n",
      "415 [D loss: 0.999957] [G loss: 1.000066]\n",
      "416 [D loss: 0.999974] [G loss: 1.000054]\n",
      "417 [D loss: 0.999974] [G loss: 1.000070]\n",
      "418 [D loss: 0.999971] [G loss: 1.000060]\n",
      "419 [D loss: 0.999972] [G loss: 1.000059]\n",
      "420 [D loss: 0.999977] [G loss: 1.000064]\n",
      "421 [D loss: 0.999963] [G loss: 1.000061]\n",
      "422 [D loss: 0.999984] [G loss: 1.000063]\n",
      "423 [D loss: 0.999965] [G loss: 1.000063]\n",
      "424 [D loss: 0.999967] [G loss: 1.000064]\n",
      "425 [D loss: 0.999973] [G loss: 1.000054]\n",
      "426 [D loss: 0.999968] [G loss: 1.000047]\n",
      "427 [D loss: 0.999971] [G loss: 1.000043]\n",
      "428 [D loss: 0.999978] [G loss: 1.000062]\n",
      "429 [D loss: 0.999971] [G loss: 1.000067]\n",
      "430 [D loss: 0.999968] [G loss: 1.000069]\n",
      "431 [D loss: 0.999972] [G loss: 1.000076]\n",
      "432 [D loss: 0.999967] [G loss: 1.000075]\n",
      "433 [D loss: 0.999974] [G loss: 1.000068]\n",
      "434 [D loss: 0.999973] [G loss: 1.000064]\n",
      "435 [D loss: 0.999965] [G loss: 1.000088]\n",
      "436 [D loss: 0.999966] [G loss: 1.000068]\n",
      "437 [D loss: 0.999972] [G loss: 1.000068]\n",
      "438 [D loss: 0.999974] [G loss: 1.000061]\n",
      "439 [D loss: 0.999963] [G loss: 1.000067]\n",
      "440 [D loss: 0.999973] [G loss: 1.000065]\n",
      "441 [D loss: 0.999977] [G loss: 1.000056]\n",
      "442 [D loss: 0.999982] [G loss: 1.000070]\n",
      "443 [D loss: 0.999967] [G loss: 1.000064]\n",
      "444 [D loss: 0.999966] [G loss: 1.000064]\n",
      "445 [D loss: 0.999963] [G loss: 1.000065]\n",
      "446 [D loss: 0.999973] [G loss: 1.000066]\n",
      "447 [D loss: 0.999966] [G loss: 1.000052]\n",
      "448 [D loss: 0.999970] [G loss: 1.000067]\n",
      "449 [D loss: 0.999969] [G loss: 1.000048]\n",
      "450 [D loss: 0.999974] [G loss: 1.000055]\n",
      "451 [D loss: 0.999969] [G loss: 1.000057]\n",
      "452 [D loss: 0.999977] [G loss: 1.000057]\n",
      "453 [D loss: 0.999966] [G loss: 1.000051]\n",
      "454 [D loss: 0.999971] [G loss: 1.000074]\n",
      "455 [D loss: 0.999971] [G loss: 1.000049]\n",
      "456 [D loss: 0.999974] [G loss: 1.000054]\n",
      "457 [D loss: 0.999977] [G loss: 1.000071]\n",
      "458 [D loss: 0.999965] [G loss: 1.000054]\n",
      "459 [D loss: 0.999977] [G loss: 1.000056]\n",
      "460 [D loss: 0.999973] [G loss: 1.000058]\n",
      "461 [D loss: 0.999972] [G loss: 1.000067]\n",
      "462 [D loss: 0.999963] [G loss: 1.000065]\n",
      "463 [D loss: 0.999972] [G loss: 1.000062]\n",
      "464 [D loss: 0.999977] [G loss: 1.000059]\n",
      "465 [D loss: 0.999983] [G loss: 1.000041]\n",
      "466 [D loss: 0.999960] [G loss: 1.000070]\n",
      "467 [D loss: 0.999977] [G loss: 1.000047]\n",
      "468 [D loss: 0.999967] [G loss: 1.000046]\n",
      "469 [D loss: 0.999974] [G loss: 1.000055]\n",
      "470 [D loss: 0.999966] [G loss: 1.000068]\n",
      "471 [D loss: 0.999971] [G loss: 1.000041]\n",
      "472 [D loss: 0.999964] [G loss: 1.000040]\n",
      "473 [D loss: 0.999973] [G loss: 1.000054]\n",
      "474 [D loss: 0.999966] [G loss: 1.000063]\n",
      "475 [D loss: 0.999958] [G loss: 1.000062]\n",
      "476 [D loss: 0.999971] [G loss: 1.000048]\n",
      "477 [D loss: 0.999952] [G loss: 1.000069]\n",
      "478 [D loss: 0.999979] [G loss: 1.000040]\n",
      "479 [D loss: 0.999965] [G loss: 1.000055]\n",
      "480 [D loss: 0.999971] [G loss: 1.000062]\n",
      "481 [D loss: 0.999963] [G loss: 1.000075]\n",
      "482 [D loss: 0.999970] [G loss: 1.000060]\n",
      "483 [D loss: 0.999965] [G loss: 1.000050]\n",
      "484 [D loss: 0.999974] [G loss: 1.000068]\n",
      "485 [D loss: 0.999970] [G loss: 1.000064]\n",
      "486 [D loss: 0.999975] [G loss: 1.000059]\n",
      "487 [D loss: 0.999967] [G loss: 1.000066]\n",
      "488 [D loss: 0.999968] [G loss: 1.000057]\n",
      "489 [D loss: 0.999970] [G loss: 1.000065]\n",
      "490 [D loss: 0.999969] [G loss: 1.000052]\n",
      "491 [D loss: 0.999966] [G loss: 1.000066]\n",
      "492 [D loss: 0.999970] [G loss: 1.000066]\n",
      "493 [D loss: 0.999974] [G loss: 1.000055]\n",
      "494 [D loss: 0.999970] [G loss: 1.000058]\n",
      "495 [D loss: 0.999972] [G loss: 1.000072]\n",
      "496 [D loss: 0.999969] [G loss: 1.000060]\n",
      "497 [D loss: 0.999962] [G loss: 1.000063]\n",
      "498 [D loss: 0.999965] [G loss: 1.000073]\n",
      "499 [D loss: 0.999968] [G loss: 1.000054]\n",
      "500 [D loss: 0.999957] [G loss: 1.000063]\n",
      "501 [D loss: 0.999965] [G loss: 1.000071]\n",
      "502 [D loss: 0.999964] [G loss: 1.000065]\n",
      "503 [D loss: 0.999967] [G loss: 1.000063]\n",
      "504 [D loss: 0.999977] [G loss: 1.000066]\n",
      "505 [D loss: 0.999971] [G loss: 1.000066]\n",
      "506 [D loss: 0.999967] [G loss: 1.000072]\n",
      "507 [D loss: 0.999973] [G loss: 1.000056]\n",
      "508 [D loss: 0.999969] [G loss: 1.000071]\n",
      "509 [D loss: 0.999968] [G loss: 1.000077]\n",
      "510 [D loss: 0.999970] [G loss: 1.000083]\n",
      "511 [D loss: 0.999965] [G loss: 1.000066]\n",
      "512 [D loss: 0.999966] [G loss: 1.000062]\n",
      "513 [D loss: 0.999966] [G loss: 1.000058]\n",
      "514 [D loss: 0.999970] [G loss: 1.000063]\n",
      "515 [D loss: 0.999977] [G loss: 1.000073]\n",
      "516 [D loss: 0.999975] [G loss: 1.000066]\n",
      "517 [D loss: 0.999970] [G loss: 1.000075]\n",
      "518 [D loss: 0.999963] [G loss: 1.000069]\n",
      "519 [D loss: 0.999972] [G loss: 1.000063]\n",
      "520 [D loss: 0.999969] [G loss: 1.000058]\n",
      "521 [D loss: 0.999968] [G loss: 1.000074]\n",
      "522 [D loss: 0.999971] [G loss: 1.000057]\n",
      "523 [D loss: 0.999959] [G loss: 1.000061]\n",
      "524 [D loss: 0.999965] [G loss: 1.000052]\n",
      "525 [D loss: 0.999973] [G loss: 1.000063]\n",
      "526 [D loss: 0.999975] [G loss: 1.000063]\n",
      "527 [D loss: 0.999974] [G loss: 1.000059]\n",
      "528 [D loss: 0.999972] [G loss: 1.000066]\n",
      "529 [D loss: 0.999974] [G loss: 1.000070]\n",
      "530 [D loss: 0.999970] [G loss: 1.000058]\n",
      "531 [D loss: 0.999973] [G loss: 1.000047]\n",
      "532 [D loss: 0.999967] [G loss: 1.000076]\n",
      "533 [D loss: 0.999964] [G loss: 1.000052]\n",
      "534 [D loss: 0.999965] [G loss: 1.000060]\n",
      "535 [D loss: 0.999963] [G loss: 1.000057]\n",
      "536 [D loss: 0.999966] [G loss: 1.000064]\n",
      "537 [D loss: 0.999972] [G loss: 1.000059]\n",
      "538 [D loss: 0.999976] [G loss: 1.000065]\n",
      "539 [D loss: 0.999974] [G loss: 1.000068]\n",
      "540 [D loss: 0.999964] [G loss: 1.000058]\n",
      "541 [D loss: 0.999975] [G loss: 1.000070]\n",
      "542 [D loss: 0.999978] [G loss: 1.000057]\n",
      "543 [D loss: 0.999973] [G loss: 1.000063]\n",
      "544 [D loss: 0.999970] [G loss: 1.000072]\n",
      "545 [D loss: 0.999968] [G loss: 1.000062]\n",
      "546 [D loss: 0.999968] [G loss: 1.000059]\n",
      "547 [D loss: 0.999960] [G loss: 1.000072]\n",
      "548 [D loss: 0.999969] [G loss: 1.000072]\n",
      "549 [D loss: 0.999965] [G loss: 1.000073]\n",
      "550 [D loss: 0.999975] [G loss: 1.000067]\n",
      "551 [D loss: 0.999975] [G loss: 1.000053]\n",
      "552 [D loss: 0.999977] [G loss: 1.000069]\n",
      "553 [D loss: 0.999967] [G loss: 1.000054]\n",
      "554 [D loss: 0.999965] [G loss: 1.000063]\n",
      "555 [D loss: 0.999963] [G loss: 1.000072]\n",
      "556 [D loss: 0.999962] [G loss: 1.000055]\n",
      "557 [D loss: 0.999964] [G loss: 1.000066]\n",
      "558 [D loss: 0.999970] [G loss: 1.000058]\n",
      "559 [D loss: 0.999967] [G loss: 1.000065]\n",
      "560 [D loss: 0.999968] [G loss: 1.000066]\n",
      "561 [D loss: 0.999971] [G loss: 1.000060]\n",
      "562 [D loss: 0.999970] [G loss: 1.000059]\n",
      "563 [D loss: 0.999959] [G loss: 1.000072]\n",
      "564 [D loss: 0.999968] [G loss: 1.000068]\n",
      "565 [D loss: 0.999969] [G loss: 1.000079]\n",
      "566 [D loss: 0.999960] [G loss: 1.000059]\n",
      "567 [D loss: 0.999970] [G loss: 1.000063]\n",
      "568 [D loss: 0.999975] [G loss: 1.000071]\n",
      "569 [D loss: 0.999961] [G loss: 1.000065]\n",
      "570 [D loss: 0.999970] [G loss: 1.000060]\n",
      "571 [D loss: 0.999968] [G loss: 1.000068]\n",
      "572 [D loss: 0.999966] [G loss: 1.000065]\n",
      "573 [D loss: 0.999969] [G loss: 1.000065]\n",
      "574 [D loss: 0.999972] [G loss: 1.000073]\n",
      "575 [D loss: 0.999971] [G loss: 1.000063]\n",
      "576 [D loss: 0.999971] [G loss: 1.000072]\n",
      "577 [D loss: 0.999966] [G loss: 1.000062]\n",
      "578 [D loss: 0.999973] [G loss: 1.000072]\n",
      "579 [D loss: 0.999966] [G loss: 1.000064]\n",
      "580 [D loss: 0.999965] [G loss: 1.000055]\n",
      "581 [D loss: 0.999973] [G loss: 1.000063]\n",
      "582 [D loss: 0.999966] [G loss: 1.000053]\n",
      "583 [D loss: 0.999972] [G loss: 1.000048]\n",
      "584 [D loss: 0.999972] [G loss: 1.000064]\n",
      "585 [D loss: 0.999965] [G loss: 1.000052]\n",
      "586 [D loss: 0.999973] [G loss: 1.000047]\n",
      "587 [D loss: 0.999967] [G loss: 1.000060]\n",
      "588 [D loss: 0.999971] [G loss: 1.000056]\n",
      "589 [D loss: 0.999978] [G loss: 1.000052]\n",
      "590 [D loss: 0.999976] [G loss: 1.000057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "591 [D loss: 0.999970] [G loss: 1.000065]\n",
      "592 [D loss: 0.999968] [G loss: 1.000066]\n",
      "593 [D loss: 0.999975] [G loss: 1.000069]\n",
      "594 [D loss: 0.999968] [G loss: 1.000062]\n",
      "595 [D loss: 0.999970] [G loss: 1.000075]\n",
      "596 [D loss: 0.999967] [G loss: 1.000071]\n",
      "597 [D loss: 0.999978] [G loss: 1.000055]\n",
      "598 [D loss: 0.999970] [G loss: 1.000066]\n",
      "599 [D loss: 0.999974] [G loss: 1.000065]\n",
      "600 [D loss: 0.999965] [G loss: 1.000076]\n",
      "601 [D loss: 0.999963] [G loss: 1.000062]\n",
      "602 [D loss: 0.999968] [G loss: 1.000066]\n",
      "603 [D loss: 0.999972] [G loss: 1.000066]\n",
      "604 [D loss: 0.999966] [G loss: 1.000062]\n",
      "605 [D loss: 0.999979] [G loss: 1.000061]\n",
      "606 [D loss: 0.999976] [G loss: 1.000071]\n",
      "607 [D loss: 0.999963] [G loss: 1.000048]\n",
      "608 [D loss: 0.999967] [G loss: 1.000058]\n",
      "609 [D loss: 0.999973] [G loss: 1.000058]\n",
      "610 [D loss: 0.999959] [G loss: 1.000061]\n",
      "611 [D loss: 0.999969] [G loss: 1.000052]\n",
      "612 [D loss: 0.999976] [G loss: 1.000068]\n",
      "613 [D loss: 0.999967] [G loss: 1.000063]\n",
      "614 [D loss: 0.999980] [G loss: 1.000069]\n",
      "615 [D loss: 0.999964] [G loss: 1.000062]\n",
      "616 [D loss: 0.999968] [G loss: 1.000057]\n",
      "617 [D loss: 0.999978] [G loss: 1.000068]\n",
      "618 [D loss: 0.999976] [G loss: 1.000067]\n",
      "619 [D loss: 0.999968] [G loss: 1.000064]\n",
      "620 [D loss: 0.999976] [G loss: 1.000075]\n",
      "621 [D loss: 0.999969] [G loss: 1.000060]\n",
      "622 [D loss: 0.999986] [G loss: 1.000070]\n",
      "623 [D loss: 0.999975] [G loss: 1.000052]\n",
      "624 [D loss: 0.999982] [G loss: 1.000042]\n",
      "625 [D loss: 0.999970] [G loss: 1.000054]\n",
      "626 [D loss: 0.999975] [G loss: 1.000068]\n",
      "627 [D loss: 0.999964] [G loss: 1.000053]\n",
      "628 [D loss: 0.999975] [G loss: 1.000061]\n",
      "629 [D loss: 0.999961] [G loss: 1.000070]\n",
      "630 [D loss: 0.999964] [G loss: 1.000059]\n",
      "631 [D loss: 0.999957] [G loss: 1.000042]\n",
      "632 [D loss: 0.999988] [G loss: 1.000045]\n",
      "633 [D loss: 0.999974] [G loss: 1.000085]\n",
      "634 [D loss: 0.999974] [G loss: 1.000076]\n",
      "635 [D loss: 0.999960] [G loss: 1.000062]\n",
      "636 [D loss: 0.999981] [G loss: 1.000063]\n",
      "637 [D loss: 0.999973] [G loss: 1.000067]\n",
      "638 [D loss: 0.999968] [G loss: 1.000067]\n",
      "639 [D loss: 0.999976] [G loss: 1.000068]\n",
      "640 [D loss: 0.999975] [G loss: 1.000066]\n",
      "641 [D loss: 0.999951] [G loss: 1.000052]\n",
      "642 [D loss: 0.999975] [G loss: 1.000080]\n",
      "643 [D loss: 0.999963] [G loss: 1.000087]\n",
      "644 [D loss: 0.999963] [G loss: 1.000052]\n",
      "645 [D loss: 0.999962] [G loss: 1.000077]\n",
      "646 [D loss: 0.999975] [G loss: 1.000054]\n",
      "647 [D loss: 0.999964] [G loss: 1.000067]\n",
      "648 [D loss: 0.999962] [G loss: 1.000071]\n",
      "649 [D loss: 0.999979] [G loss: 1.000062]\n",
      "650 [D loss: 0.999971] [G loss: 1.000044]\n",
      "651 [D loss: 0.999984] [G loss: 1.000071]\n",
      "652 [D loss: 0.999959] [G loss: 1.000049]\n",
      "653 [D loss: 0.999960] [G loss: 1.000052]\n",
      "654 [D loss: 0.999976] [G loss: 1.000055]\n",
      "655 [D loss: 0.999977] [G loss: 1.000048]\n",
      "656 [D loss: 0.999983] [G loss: 1.000072]\n",
      "657 [D loss: 0.999979] [G loss: 1.000051]\n",
      "658 [D loss: 0.999984] [G loss: 1.000054]\n",
      "659 [D loss: 0.999975] [G loss: 1.000071]\n",
      "660 [D loss: 0.999973] [G loss: 1.000058]\n",
      "661 [D loss: 0.999961] [G loss: 1.000047]\n",
      "662 [D loss: 0.999972] [G loss: 1.000072]\n",
      "663 [D loss: 0.999965] [G loss: 1.000075]\n",
      "664 [D loss: 0.999975] [G loss: 1.000059]\n",
      "665 [D loss: 0.999952] [G loss: 1.000089]\n",
      "666 [D loss: 0.999972] [G loss: 1.000043]\n",
      "667 [D loss: 0.999966] [G loss: 1.000054]\n",
      "668 [D loss: 0.999967] [G loss: 1.000072]\n",
      "669 [D loss: 0.999968] [G loss: 1.000059]\n",
      "670 [D loss: 0.999960] [G loss: 1.000071]\n",
      "671 [D loss: 0.999977] [G loss: 1.000056]\n",
      "672 [D loss: 0.999979] [G loss: 1.000062]\n",
      "673 [D loss: 0.999963] [G loss: 1.000062]\n",
      "674 [D loss: 0.999965] [G loss: 1.000056]\n",
      "675 [D loss: 0.999972] [G loss: 1.000065]\n",
      "676 [D loss: 0.999970] [G loss: 1.000068]\n",
      "677 [D loss: 0.999976] [G loss: 1.000048]\n",
      "678 [D loss: 0.999970] [G loss: 1.000070]\n",
      "679 [D loss: 0.999972] [G loss: 1.000064]\n",
      "680 [D loss: 0.999966] [G loss: 1.000061]\n",
      "681 [D loss: 0.999970] [G loss: 1.000060]\n",
      "682 [D loss: 0.999978] [G loss: 1.000050]\n",
      "683 [D loss: 0.999957] [G loss: 1.000057]\n",
      "684 [D loss: 0.999967] [G loss: 1.000081]\n",
      "685 [D loss: 0.999958] [G loss: 1.000079]\n",
      "686 [D loss: 0.999977] [G loss: 1.000047]\n",
      "687 [D loss: 0.999961] [G loss: 1.000079]\n",
      "688 [D loss: 0.999979] [G loss: 1.000060]\n",
      "689 [D loss: 0.999969] [G loss: 1.000059]\n",
      "690 [D loss: 0.999975] [G loss: 1.000072]\n",
      "691 [D loss: 0.999975] [G loss: 1.000064]\n",
      "692 [D loss: 0.999975] [G loss: 1.000064]\n",
      "693 [D loss: 0.999973] [G loss: 1.000065]\n",
      "694 [D loss: 0.999968] [G loss: 1.000057]\n",
      "695 [D loss: 0.999976] [G loss: 1.000051]\n",
      "696 [D loss: 0.999969] [G loss: 1.000064]\n",
      "697 [D loss: 0.999972] [G loss: 1.000059]\n",
      "698 [D loss: 0.999969] [G loss: 1.000066]\n",
      "699 [D loss: 0.999969] [G loss: 1.000062]\n",
      "700 [D loss: 0.999979] [G loss: 1.000068]\n",
      "701 [D loss: 0.999977] [G loss: 1.000073]\n",
      "702 [D loss: 0.999960] [G loss: 1.000061]\n",
      "703 [D loss: 0.999971] [G loss: 1.000074]\n",
      "704 [D loss: 0.999967] [G loss: 1.000064]\n",
      "705 [D loss: 0.999967] [G loss: 1.000059]\n",
      "706 [D loss: 0.999966] [G loss: 1.000062]\n",
      "707 [D loss: 0.999968] [G loss: 1.000078]\n",
      "708 [D loss: 0.999965] [G loss: 1.000056]\n",
      "709 [D loss: 0.999972] [G loss: 1.000068]\n",
      "710 [D loss: 0.999974] [G loss: 1.000063]\n",
      "711 [D loss: 0.999967] [G loss: 1.000063]\n",
      "712 [D loss: 0.999974] [G loss: 1.000065]\n",
      "713 [D loss: 0.999968] [G loss: 1.000052]\n",
      "714 [D loss: 0.999972] [G loss: 1.000046]\n",
      "715 [D loss: 0.999968] [G loss: 1.000049]\n",
      "716 [D loss: 0.999976] [G loss: 1.000058]\n",
      "717 [D loss: 0.999965] [G loss: 1.000055]\n",
      "718 [D loss: 0.999973] [G loss: 1.000063]\n",
      "719 [D loss: 0.999973] [G loss: 1.000054]\n",
      "720 [D loss: 0.999965] [G loss: 1.000057]\n",
      "721 [D loss: 0.999968] [G loss: 1.000063]\n",
      "722 [D loss: 0.999971] [G loss: 1.000070]\n",
      "723 [D loss: 0.999969] [G loss: 1.000063]\n",
      "724 [D loss: 0.999971] [G loss: 1.000075]\n",
      "725 [D loss: 0.999975] [G loss: 1.000071]\n",
      "726 [D loss: 0.999970] [G loss: 1.000063]\n",
      "727 [D loss: 0.999973] [G loss: 1.000060]\n",
      "728 [D loss: 0.999968] [G loss: 1.000072]\n",
      "729 [D loss: 0.999968] [G loss: 1.000064]\n",
      "730 [D loss: 0.999973] [G loss: 1.000064]\n",
      "731 [D loss: 0.999965] [G loss: 1.000074]\n",
      "732 [D loss: 0.999966] [G loss: 1.000068]\n",
      "733 [D loss: 0.999978] [G loss: 1.000067]\n",
      "734 [D loss: 0.999980] [G loss: 1.000055]\n",
      "735 [D loss: 0.999973] [G loss: 1.000065]\n",
      "736 [D loss: 0.999959] [G loss: 1.000074]\n",
      "737 [D loss: 0.999970] [G loss: 1.000065]\n",
      "738 [D loss: 0.999975] [G loss: 1.000059]\n",
      "739 [D loss: 0.999973] [G loss: 1.000055]\n",
      "740 [D loss: 0.999974] [G loss: 1.000059]\n",
      "741 [D loss: 0.999972] [G loss: 1.000062]\n",
      "742 [D loss: 0.999965] [G loss: 1.000064]\n",
      "743 [D loss: 0.999973] [G loss: 1.000064]\n",
      "744 [D loss: 0.999958] [G loss: 1.000064]\n",
      "745 [D loss: 0.999967] [G loss: 1.000049]\n",
      "746 [D loss: 0.999973] [G loss: 1.000053]\n",
      "747 [D loss: 0.999978] [G loss: 1.000051]\n",
      "748 [D loss: 0.999965] [G loss: 1.000078]\n",
      "749 [D loss: 0.999971] [G loss: 1.000064]\n",
      "750 [D loss: 0.999969] [G loss: 1.000058]\n",
      "751 [D loss: 0.999969] [G loss: 1.000053]\n",
      "752 [D loss: 0.999974] [G loss: 1.000052]\n",
      "753 [D loss: 0.999970] [G loss: 1.000069]\n",
      "754 [D loss: 0.999969] [G loss: 1.000058]\n",
      "755 [D loss: 0.999975] [G loss: 1.000055]\n",
      "756 [D loss: 0.999973] [G loss: 1.000058]\n",
      "757 [D loss: 0.999962] [G loss: 1.000059]\n",
      "758 [D loss: 0.999974] [G loss: 1.000062]\n",
      "759 [D loss: 0.999970] [G loss: 1.000063]\n",
      "760 [D loss: 0.999966] [G loss: 1.000069]\n",
      "761 [D loss: 0.999971] [G loss: 1.000052]\n",
      "762 [D loss: 0.999970] [G loss: 1.000058]\n",
      "763 [D loss: 0.999975] [G loss: 1.000070]\n",
      "764 [D loss: 0.999979] [G loss: 1.000059]\n",
      "765 [D loss: 0.999973] [G loss: 1.000065]\n",
      "766 [D loss: 0.999975] [G loss: 1.000063]\n",
      "767 [D loss: 0.999977] [G loss: 1.000060]\n",
      "768 [D loss: 0.999966] [G loss: 1.000062]\n",
      "769 [D loss: 0.999965] [G loss: 1.000055]\n",
      "770 [D loss: 0.999967] [G loss: 1.000058]\n",
      "771 [D loss: 0.999972] [G loss: 1.000062]\n",
      "772 [D loss: 0.999961] [G loss: 1.000071]\n",
      "773 [D loss: 0.999971] [G loss: 1.000068]\n",
      "774 [D loss: 0.999967] [G loss: 1.000054]\n",
      "775 [D loss: 0.999971] [G loss: 1.000070]\n",
      "776 [D loss: 0.999971] [G loss: 1.000058]\n",
      "777 [D loss: 0.999958] [G loss: 1.000052]\n",
      "778 [D loss: 0.999977] [G loss: 1.000061]\n",
      "779 [D loss: 0.999968] [G loss: 1.000067]\n",
      "780 [D loss: 0.999979] [G loss: 1.000058]\n",
      "781 [D loss: 0.999972] [G loss: 1.000048]\n",
      "782 [D loss: 0.999975] [G loss: 1.000055]\n",
      "783 [D loss: 0.999967] [G loss: 1.000071]\n",
      "784 [D loss: 0.999969] [G loss: 1.000073]\n",
      "785 [D loss: 0.999960] [G loss: 1.000071]\n",
      "786 [D loss: 0.999962] [G loss: 1.000073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787 [D loss: 0.999969] [G loss: 1.000075]\n",
      "788 [D loss: 0.999961] [G loss: 1.000067]\n",
      "789 [D loss: 0.999967] [G loss: 1.000064]\n",
      "790 [D loss: 0.999981] [G loss: 1.000058]\n",
      "791 [D loss: 0.999977] [G loss: 1.000058]\n",
      "792 [D loss: 0.999974] [G loss: 1.000059]\n",
      "793 [D loss: 0.999976] [G loss: 1.000053]\n",
      "794 [D loss: 0.999961] [G loss: 1.000066]\n",
      "795 [D loss: 0.999970] [G loss: 1.000087]\n",
      "796 [D loss: 0.999969] [G loss: 1.000067]\n",
      "797 [D loss: 0.999964] [G loss: 1.000067]\n",
      "798 [D loss: 0.999965] [G loss: 1.000035]\n",
      "799 [D loss: 0.999961] [G loss: 1.000061]\n",
      "800 [D loss: 0.999969] [G loss: 1.000073]\n",
      "801 [D loss: 0.999965] [G loss: 1.000073]\n",
      "802 [D loss: 0.999969] [G loss: 1.000071]\n",
      "803 [D loss: 0.999977] [G loss: 1.000063]\n",
      "804 [D loss: 0.999973] [G loss: 1.000062]\n",
      "805 [D loss: 0.999974] [G loss: 1.000049]\n",
      "806 [D loss: 0.999966] [G loss: 1.000057]\n",
      "807 [D loss: 0.999960] [G loss: 1.000069]\n",
      "808 [D loss: 0.999970] [G loss: 1.000082]\n",
      "809 [D loss: 0.999971] [G loss: 1.000070]\n",
      "810 [D loss: 0.999965] [G loss: 1.000044]\n",
      "811 [D loss: 0.999963] [G loss: 1.000058]\n",
      "812 [D loss: 0.999978] [G loss: 1.000050]\n",
      "813 [D loss: 0.999971] [G loss: 1.000051]\n",
      "814 [D loss: 0.999976] [G loss: 1.000065]\n",
      "815 [D loss: 0.999980] [G loss: 1.000054]\n",
      "816 [D loss: 0.999978] [G loss: 1.000063]\n",
      "817 [D loss: 0.999976] [G loss: 1.000066]\n",
      "818 [D loss: 0.999973] [G loss: 1.000040]\n",
      "819 [D loss: 0.999964] [G loss: 1.000061]\n",
      "820 [D loss: 0.999970] [G loss: 1.000068]\n",
      "821 [D loss: 0.999973] [G loss: 1.000067]\n",
      "822 [D loss: 0.999975] [G loss: 1.000056]\n",
      "823 [D loss: 0.999970] [G loss: 1.000059]\n",
      "824 [D loss: 0.999969] [G loss: 1.000058]\n",
      "825 [D loss: 0.999970] [G loss: 1.000059]\n",
      "826 [D loss: 0.999973] [G loss: 1.000070]\n",
      "827 [D loss: 0.999964] [G loss: 1.000064]\n",
      "828 [D loss: 0.999975] [G loss: 1.000052]\n",
      "829 [D loss: 0.999970] [G loss: 1.000060]\n",
      "830 [D loss: 0.999971] [G loss: 1.000063]\n",
      "831 [D loss: 0.999966] [G loss: 1.000066]\n",
      "832 [D loss: 0.999970] [G loss: 1.000056]\n",
      "833 [D loss: 0.999974] [G loss: 1.000057]\n",
      "834 [D loss: 0.999978] [G loss: 1.000063]\n",
      "835 [D loss: 0.999975] [G loss: 1.000067]\n",
      "836 [D loss: 0.999955] [G loss: 1.000056]\n",
      "837 [D loss: 0.999965] [G loss: 1.000052]\n",
      "838 [D loss: 0.999964] [G loss: 1.000060]\n",
      "839 [D loss: 0.999970] [G loss: 1.000060]\n",
      "840 [D loss: 0.999977] [G loss: 1.000065]\n",
      "841 [D loss: 0.999967] [G loss: 1.000065]\n",
      "842 [D loss: 0.999970] [G loss: 1.000056]\n",
      "843 [D loss: 0.999968] [G loss: 1.000069]\n",
      "844 [D loss: 0.999968] [G loss: 1.000062]\n",
      "845 [D loss: 0.999973] [G loss: 1.000049]\n",
      "846 [D loss: 0.999971] [G loss: 1.000070]\n",
      "847 [D loss: 0.999969] [G loss: 1.000051]\n",
      "848 [D loss: 0.999972] [G loss: 1.000062]\n",
      "849 [D loss: 0.999971] [G loss: 1.000066]\n",
      "850 [D loss: 0.999969] [G loss: 1.000066]\n",
      "851 [D loss: 0.999966] [G loss: 1.000052]\n",
      "852 [D loss: 0.999973] [G loss: 1.000064]\n",
      "853 [D loss: 0.999972] [G loss: 1.000055]\n",
      "854 [D loss: 0.999969] [G loss: 1.000060]\n",
      "855 [D loss: 0.999968] [G loss: 1.000064]\n",
      "856 [D loss: 0.999971] [G loss: 1.000052]\n",
      "857 [D loss: 0.999976] [G loss: 1.000051]\n",
      "858 [D loss: 0.999964] [G loss: 1.000065]\n",
      "859 [D loss: 0.999972] [G loss: 1.000063]\n",
      "860 [D loss: 0.999972] [G loss: 1.000073]\n",
      "861 [D loss: 0.999976] [G loss: 1.000065]\n",
      "862 [D loss: 0.999968] [G loss: 1.000055]\n",
      "863 [D loss: 0.999961] [G loss: 1.000062]\n",
      "864 [D loss: 0.999971] [G loss: 1.000076]\n",
      "865 [D loss: 0.999969] [G loss: 1.000053]\n",
      "866 [D loss: 0.999970] [G loss: 1.000062]\n",
      "867 [D loss: 0.999972] [G loss: 1.000068]\n",
      "868 [D loss: 0.999970] [G loss: 1.000057]\n",
      "869 [D loss: 0.999968] [G loss: 1.000065]\n",
      "870 [D loss: 0.999970] [G loss: 1.000059]\n",
      "871 [D loss: 0.999976] [G loss: 1.000061]\n",
      "872 [D loss: 0.999962] [G loss: 1.000062]\n",
      "873 [D loss: 0.999977] [G loss: 1.000056]\n",
      "874 [D loss: 0.999969] [G loss: 1.000063]\n",
      "875 [D loss: 0.999969] [G loss: 1.000047]\n",
      "876 [D loss: 0.999968] [G loss: 1.000054]\n",
      "877 [D loss: 0.999963] [G loss: 1.000060]\n",
      "878 [D loss: 0.999969] [G loss: 1.000064]\n",
      "879 [D loss: 0.999959] [G loss: 1.000066]\n",
      "880 [D loss: 0.999967] [G loss: 1.000060]\n",
      "881 [D loss: 0.999970] [G loss: 1.000056]\n",
      "882 [D loss: 0.999969] [G loss: 1.000066]\n",
      "883 [D loss: 0.999973] [G loss: 1.000062]\n",
      "884 [D loss: 0.999968] [G loss: 1.000065]\n",
      "885 [D loss: 0.999964] [G loss: 1.000058]\n",
      "886 [D loss: 0.999974] [G loss: 1.000064]\n",
      "887 [D loss: 0.999972] [G loss: 1.000060]\n",
      "888 [D loss: 0.999962] [G loss: 1.000069]\n",
      "889 [D loss: 0.999977] [G loss: 1.000056]\n",
      "890 [D loss: 0.999972] [G loss: 1.000065]\n",
      "891 [D loss: 0.999973] [G loss: 1.000072]\n",
      "892 [D loss: 0.999966] [G loss: 1.000062]\n",
      "893 [D loss: 0.999971] [G loss: 1.000070]\n",
      "894 [D loss: 0.999970] [G loss: 1.000057]\n",
      "895 [D loss: 0.999983] [G loss: 1.000053]\n",
      "896 [D loss: 0.999965] [G loss: 1.000058]\n",
      "897 [D loss: 0.999976] [G loss: 1.000076]\n",
      "898 [D loss: 0.999976] [G loss: 1.000035]\n",
      "899 [D loss: 0.999975] [G loss: 1.000065]\n",
      "900 [D loss: 0.999967] [G loss: 1.000055]\n",
      "901 [D loss: 0.999976] [G loss: 1.000066]\n",
      "902 [D loss: 0.999976] [G loss: 1.000069]\n",
      "903 [D loss: 0.999968] [G loss: 1.000058]\n",
      "904 [D loss: 0.999973] [G loss: 1.000056]\n",
      "905 [D loss: 0.999967] [G loss: 1.000073]\n",
      "906 [D loss: 0.999965] [G loss: 1.000063]\n",
      "907 [D loss: 0.999970] [G loss: 1.000061]\n",
      "908 [D loss: 0.999963] [G loss: 1.000067]\n",
      "909 [D loss: 0.999970] [G loss: 1.000060]\n",
      "910 [D loss: 0.999966] [G loss: 1.000062]\n",
      "911 [D loss: 0.999974] [G loss: 1.000064]\n",
      "912 [D loss: 0.999979] [G loss: 1.000072]\n",
      "913 [D loss: 0.999972] [G loss: 1.000069]\n",
      "914 [D loss: 0.999975] [G loss: 1.000060]\n",
      "915 [D loss: 0.999970] [G loss: 1.000058]\n",
      "916 [D loss: 0.999971] [G loss: 1.000063]\n",
      "917 [D loss: 0.999976] [G loss: 1.000053]\n",
      "918 [D loss: 0.999977] [G loss: 1.000052]\n",
      "919 [D loss: 0.999975] [G loss: 1.000059]\n",
      "920 [D loss: 0.999965] [G loss: 1.000048]\n",
      "921 [D loss: 0.999972] [G loss: 1.000069]\n",
      "922 [D loss: 0.999964] [G loss: 1.000064]\n",
      "923 [D loss: 0.999973] [G loss: 1.000069]\n",
      "924 [D loss: 0.999970] [G loss: 1.000057]\n",
      "925 [D loss: 0.999969] [G loss: 1.000057]\n",
      "926 [D loss: 0.999979] [G loss: 1.000058]\n",
      "927 [D loss: 0.999974] [G loss: 1.000064]\n",
      "928 [D loss: 0.999983] [G loss: 1.000073]\n",
      "929 [D loss: 0.999971] [G loss: 1.000058]\n",
      "930 [D loss: 0.999966] [G loss: 1.000071]\n",
      "931 [D loss: 0.999966] [G loss: 1.000067]\n",
      "932 [D loss: 0.999972] [G loss: 1.000058]\n",
      "933 [D loss: 0.999970] [G loss: 1.000061]\n",
      "934 [D loss: 0.999972] [G loss: 1.000055]\n",
      "935 [D loss: 0.999980] [G loss: 1.000066]\n",
      "936 [D loss: 0.999969] [G loss: 1.000061]\n",
      "937 [D loss: 0.999974] [G loss: 1.000063]\n",
      "938 [D loss: 0.999971] [G loss: 1.000057]\n",
      "939 [D loss: 0.999976] [G loss: 1.000057]\n",
      "940 [D loss: 0.999967] [G loss: 1.000073]\n",
      "941 [D loss: 0.999969] [G loss: 1.000061]\n",
      "942 [D loss: 0.999975] [G loss: 1.000055]\n",
      "943 [D loss: 0.999961] [G loss: 1.000066]\n",
      "944 [D loss: 0.999968] [G loss: 1.000057]\n",
      "945 [D loss: 0.999973] [G loss: 1.000051]\n",
      "946 [D loss: 0.999967] [G loss: 1.000060]\n",
      "947 [D loss: 0.999973] [G loss: 1.000055]\n",
      "948 [D loss: 0.999975] [G loss: 1.000078]\n",
      "949 [D loss: 0.999964] [G loss: 1.000066]\n",
      "950 [D loss: 0.999965] [G loss: 1.000064]\n",
      "951 [D loss: 0.999963] [G loss: 1.000067]\n",
      "952 [D loss: 0.999950] [G loss: 1.000073]\n",
      "953 [D loss: 0.999967] [G loss: 1.000055]\n",
      "954 [D loss: 0.999969] [G loss: 1.000063]\n",
      "955 [D loss: 0.999979] [G loss: 1.000066]\n",
      "956 [D loss: 0.999978] [G loss: 1.000077]\n",
      "957 [D loss: 0.999970] [G loss: 1.000056]\n",
      "958 [D loss: 0.999980] [G loss: 1.000054]\n",
      "959 [D loss: 0.999968] [G loss: 1.000070]\n",
      "960 [D loss: 0.999967] [G loss: 1.000054]\n",
      "961 [D loss: 0.999973] [G loss: 1.000069]\n",
      "962 [D loss: 0.999972] [G loss: 1.000059]\n",
      "963 [D loss: 0.999969] [G loss: 1.000070]\n",
      "964 [D loss: 0.999973] [G loss: 1.000053]\n",
      "965 [D loss: 0.999964] [G loss: 1.000054]\n",
      "966 [D loss: 0.999974] [G loss: 1.000058]\n",
      "967 [D loss: 0.999971] [G loss: 1.000053]\n",
      "968 [D loss: 0.999970] [G loss: 1.000057]\n",
      "969 [D loss: 0.999974] [G loss: 1.000057]\n",
      "970 [D loss: 0.999971] [G loss: 1.000068]\n",
      "971 [D loss: 0.999969] [G loss: 1.000069]\n",
      "972 [D loss: 0.999974] [G loss: 1.000067]\n",
      "973 [D loss: 0.999957] [G loss: 1.000058]\n",
      "974 [D loss: 0.999969] [G loss: 1.000056]\n",
      "975 [D loss: 0.999969] [G loss: 1.000057]\n",
      "976 [D loss: 0.999963] [G loss: 1.000061]\n",
      "977 [D loss: 0.999969] [G loss: 1.000066]\n",
      "978 [D loss: 0.999974] [G loss: 1.000066]\n",
      "979 [D loss: 0.999965] [G loss: 1.000072]\n",
      "980 [D loss: 0.999970] [G loss: 1.000053]\n",
      "981 [D loss: 0.999978] [G loss: 1.000054]\n",
      "982 [D loss: 0.999965] [G loss: 1.000063]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "983 [D loss: 0.999964] [G loss: 1.000068]\n",
      "984 [D loss: 0.999970] [G loss: 1.000059]\n",
      "985 [D loss: 0.999968] [G loss: 1.000051]\n",
      "986 [D loss: 0.999968] [G loss: 1.000058]\n",
      "987 [D loss: 0.999970] [G loss: 1.000063]\n",
      "988 [D loss: 0.999976] [G loss: 1.000059]\n",
      "989 [D loss: 0.999976] [G loss: 1.000060]\n",
      "990 [D loss: 0.999972] [G loss: 1.000065]\n",
      "991 [D loss: 0.999972] [G loss: 1.000069]\n",
      "992 [D loss: 0.999973] [G loss: 1.000059]\n",
      "993 [D loss: 0.999971] [G loss: 1.000066]\n",
      "994 [D loss: 0.999970] [G loss: 1.000064]\n",
      "995 [D loss: 0.999970] [G loss: 1.000057]\n",
      "996 [D loss: 0.999975] [G loss: 1.000062]\n",
      "997 [D loss: 0.999967] [G loss: 1.000068]\n",
      "998 [D loss: 0.999969] [G loss: 1.000062]\n",
      "999 [D loss: 0.999970] [G loss: 1.000058]\n"
     ]
    }
   ],
   "source": [
    "# epochs=4000\n",
    "epochs=1000\n",
    "train(G, critic, combined, \n",
    "      n_critic, latent_dim, clip_value,\n",
    "      epochs=epochs, batch_size=32, sample_interval=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
